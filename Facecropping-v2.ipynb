{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mtcnn\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2, mmcv\n",
    "# confirm mtcnn was installed correctly\n",
    "from facenet_pytorch import MTCNN\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=20, min_face_size=40,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/s3/bucket/kaggle_vijay/train_sample_videos/avibnnhwhp.mp4\n"
     ]
    }
   ],
   "source": [
    "# filename = \"./image-data/frame175.jpg\"\n",
    "\n",
    "# filename = \"test.png\"\n",
    "image_dir = \"/s3/bucket/kaggle_vijay/train_sample_videos/\"\n",
    "image_file = \"avibnnhwhp.mp4\"\n",
    "# filename = \"family2.jpg\"\n",
    "filename = image_dir + image_file\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "video = mmcv.VideoReader(filename)\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "print(type(frames[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 1<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Tracking frame: 2<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Tracking frame: 3<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Tracking frame: 4<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "Tracking frame: 5<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "cropped_faces = []\n",
    "count = 0\n",
    "for k, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(k + 1), end='')\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, p = mtcnn.detect(frame)\n",
    "    for i in range(len(p)):\n",
    "        if p[i] > 0.9:\n",
    "            face_crop = frame.crop(boxes[i])\n",
    "            print(type(face_crop))\n",
    "            cropped_faces.append(face_crop)\n",
    "            count += 1\n",
    "\n",
    "    if count == 10:\n",
    "        break\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cropped_faces[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocv_image = np.array(cropped_faces[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34a85c1198>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ocv_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"cropped.png\", ocv_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"cropped.png\")\n",
    "# convert to RGB, if needed\n",
    "#image = image.convert('RGB')\n",
    "# convert to array\n",
    "rbi = np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f344c5692e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, x = mtcnn.detect(frames_tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_img = frames_tracked[ny:ny+nr, nx:nx+nr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[9][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(MTCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = display.display(frames_tracked[0], display_id=True)\n",
    "i = 1\n",
    "try:\n",
    "    while True:\n",
    "        d.update(frames_tracked[i % len(frames_tracked)])\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"/s3/bucket/demo_update/022920_005659/original/0163.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image from file\n",
    "image = Image.open(image_file)\n",
    "# convert to RGB, if needed\n",
    "image = image.convert('RGB')\n",
    "# convert to array\n",
    "pixels = np.asarray(image)\n",
    "\n",
    "# create the detector, using default weights\n",
    "results, x = mtcnn.detect(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# go over the cropped images\n",
    "for i,result in enumerate(results):\n",
    "    \n",
    "    dpi = 10\n",
    "    x1, y1, width, height = result\n",
    "    print(\"{} {} {} {}\".format(x1, y1, width, height))\n",
    "    # bug fix\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "\n",
    "    face = pixels[y1-20:y2+20, x1-20:x2+20]\n",
    "    image = Image.fromarray(face)\n",
    "#     image = image.resize((400, 400))\n",
    "    face_array = np.asarray(image)\n",
    "    \n",
    "    height, width, depth = face_array.shape\n",
    "\n",
    "    # What size does the figure need to be in inches to fit the image?\n",
    "#     figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "    plt.subplot(len(results), 1, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(face_array)\n",
    "    print(y1,y2,x1,x2,\"height : \",y2-y1,\"width : \",x2-x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package cv2.cv2 in cv2:\n",
      "\n",
      "NAME\n",
      "    cv2.cv2 - Python wrapper for OpenCV.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    cv2\n",
      "    data (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        cv2.error\n",
      "    builtins.object\n",
      "        cv2.Algorithm\n",
      "            cv2.AlignExposures\n",
      "                cv2.AlignMTB\n",
      "            cv2.BackgroundSubtractor\n",
      "                cv2.BackgroundSubtractorKNN\n",
      "                cv2.BackgroundSubtractorMOG2\n",
      "            cv2.BaseCascadeClassifier\n",
      "            cv2.CLAHE\n",
      "            cv2.CalibrateCRF\n",
      "                cv2.CalibrateDebevec\n",
      "                cv2.CalibrateRobertson\n",
      "            cv2.DenseOpticalFlow\n",
      "                cv2.DISOpticalFlow\n",
      "                cv2.FarnebackOpticalFlow\n",
      "                cv2.VariationalRefinement\n",
      "            cv2.DescriptorMatcher\n",
      "                cv2.BFMatcher\n",
      "                cv2.FlannBasedMatcher\n",
      "            cv2.GeneralizedHough\n",
      "                cv2.GeneralizedHoughBallard\n",
      "                cv2.GeneralizedHoughGuil\n",
      "            cv2.LineSegmentDetector\n",
      "            cv2.MergeExposures\n",
      "                cv2.MergeDebevec\n",
      "                cv2.MergeMertens\n",
      "                cv2.MergeRobertson\n",
      "            cv2.SparseOpticalFlow\n",
      "                cv2.SparsePyrLKOpticalFlow\n",
      "            cv2.StereoMatcher\n",
      "                cv2.StereoBM\n",
      "                cv2.StereoSGBM\n",
      "            cv2.Tonemap\n",
      "                cv2.TonemapDrago\n",
      "                cv2.TonemapMantiuk\n",
      "                cv2.TonemapReinhard\n",
      "            cv2.dnn_Layer\n",
      "            cv2.ml_StatModel\n",
      "                cv2.ml_ANN_MLP\n",
      "                cv2.ml_DTrees\n",
      "                    cv2.ml_Boost\n",
      "                    cv2.ml_RTrees\n",
      "                cv2.ml_EM\n",
      "                cv2.ml_KNearest\n",
      "                cv2.ml_LogisticRegression\n",
      "                cv2.ml_NormalBayesClassifier\n",
      "                cv2.ml_SVM\n",
      "                cv2.ml_SVMSGD\n",
      "        cv2.AsyncArray\n",
      "        cv2.BOWImgDescriptorExtractor\n",
      "        cv2.BOWTrainer\n",
      "            cv2.BOWKMeansTrainer\n",
      "        cv2.CascadeClassifier\n",
      "        cv2.CirclesGridFinderParameters\n",
      "        cv2.DMatch\n",
      "        cv2.Feature2D\n",
      "            cv2.AKAZE\n",
      "            cv2.AgastFeatureDetector\n",
      "            cv2.BRISK\n",
      "            cv2.FastFeatureDetector\n",
      "            cv2.GFTTDetector\n",
      "            cv2.KAZE\n",
      "            cv2.MSER\n",
      "            cv2.ORB\n",
      "            cv2.SimpleBlobDetector\n",
      "        cv2.FileNode\n",
      "        cv2.FileStorage\n",
      "        cv2.HOGDescriptor\n",
      "        cv2.KalmanFilter\n",
      "        cv2.KeyPoint\n",
      "        cv2.PyRotationWarper\n",
      "        cv2.QRCodeDetector\n",
      "        cv2.SimpleBlobDetector_Params\n",
      "        cv2.Stitcher\n",
      "        cv2.Subdiv2D\n",
      "        cv2.TickMeter\n",
      "        cv2.UMat\n",
      "        cv2.VideoCapture\n",
      "        cv2.VideoWriter\n",
      "        cv2.WarperCreator\n",
      "        cv2.cuda_BufferPool\n",
      "        cv2.cuda_DeviceInfo\n",
      "        cv2.cuda_Event\n",
      "        cv2.cuda_GpuMat\n",
      "        cv2.cuda_GpuMat_Allocator\n",
      "        cv2.cuda_HostMem\n",
      "        cv2.cuda_Stream\n",
      "        cv2.cuda_TargetArchs\n",
      "        cv2.detail_Blender\n",
      "            cv2.detail_FeatherBlender\n",
      "            cv2.detail_MultiBandBlender\n",
      "        cv2.detail_CameraParams\n",
      "        cv2.detail_Estimator\n",
      "            cv2.detail_AffineBasedEstimator\n",
      "            cv2.detail_BundleAdjusterBase\n",
      "                cv2.detail_BundleAdjusterAffine\n",
      "                cv2.detail_BundleAdjusterAffinePartial\n",
      "                cv2.detail_BundleAdjusterRay\n",
      "                cv2.detail_BundleAdjusterReproj\n",
      "                cv2.detail_NoBundleAdjuster\n",
      "            cv2.detail_HomographyBasedEstimator\n",
      "        cv2.detail_ExposureCompensator\n",
      "            cv2.detail_BlocksCompensator\n",
      "                cv2.detail_BlocksChannelsCompensator\n",
      "                cv2.detail_BlocksGainCompensator\n",
      "            cv2.detail_ChannelsCompensator\n",
      "            cv2.detail_GainCompensator\n",
      "            cv2.detail_NoExposureCompensator\n",
      "        cv2.detail_FeaturesMatcher\n",
      "            cv2.detail_BestOf2NearestMatcher\n",
      "                cv2.detail_AffineBestOf2NearestMatcher\n",
      "                cv2.detail_BestOf2NearestRangeMatcher\n",
      "        cv2.detail_GraphCutSeamFinder\n",
      "        cv2.detail_ImageFeatures\n",
      "        cv2.detail_MatchesInfo\n",
      "        cv2.detail_ProjectorBase\n",
      "            cv2.detail_SphericalProjector\n",
      "        cv2.detail_SeamFinder\n",
      "            cv2.detail_DpSeamFinder\n",
      "            cv2.detail_NoSeamFinder\n",
      "            cv2.detail_PairwiseSeamFinder\n",
      "                cv2.detail_VoronoiSeamFinder\n",
      "        cv2.detail_Timelapser\n",
      "            cv2.detail_TimelapserCrop\n",
      "        cv2.dnn_DictValue\n",
      "        cv2.dnn_Net\n",
      "        cv2.flann_Index\n",
      "        cv2.ml_ParamGrid\n",
      "        cv2.ml_TrainData\n",
      "        cv2.ocl_Device\n",
      "    \n",
      "    class AKAZE(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      AKAZE\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDescriptorChannels(...)\n",
      "     |      getDescriptorChannels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDescriptorSize(...)\n",
      "     |      getDescriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDescriptorType(...)\n",
      "     |      getDescriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDiffusivity(...)\n",
      "     |      getDiffusivity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNOctaveLayers(...)\n",
      "     |      getNOctaveLayers() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNOctaves(...)\n",
      "     |      getNOctaves() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDescriptorChannels(...)\n",
      "     |      setDescriptorChannels(dch) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDescriptorSize(...)\n",
      "     |      setDescriptorSize(dsize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDescriptorType(...)\n",
      "     |      setDescriptorType(dtype) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDiffusivity(...)\n",
      "     |      setDiffusivity(diff) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNOctaveLayers(...)\n",
      "     |      setNOctaveLayers(octaveLayers) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNOctaves(...)\n",
      "     |      setNOctaves(octaves) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, descriptor_type[, descriptor_size[, descriptor_channels[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]]) -> retval\n",
      "     |      .   @brief The AKAZE constructor\n",
      "     |      .   \n",
      "     |      .       @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,\n",
      "     |      .       DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.\n",
      "     |      .       @param descriptor_size Size of the descriptor in bits. 0 -\\> Full size\n",
      "     |      .       @param descriptor_channels Number of channels in the descriptor (1, 2, 3)\n",
      "     |      .       @param threshold Detector response threshold to accept point\n",
      "     |      .       @param nOctaves Maximum octave evolution of the image\n",
      "     |      .       @param nOctaveLayers Default number of sublevels per scale level\n",
      "     |      .       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n",
      "     |      .       DIFF_CHARBONNIER\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class AgastFeatureDetector(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      AgastFeatureDetector\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNonmaxSuppression(...)\n",
      "     |      getNonmaxSuppression() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getType(...)\n",
      "     |      getType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNonmaxSuppression(...)\n",
      "     |      setNonmaxSuppression(f) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setType(...)\n",
      "     |      setType(type) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class Algorithm(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class AlignExposures(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      AlignExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, dst, times, response) -> None\n",
      "     |      .   @brief Aligns images\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst vector of aligned images\n",
      "     |      .       @param times vector of exposure time values for each image\n",
      "     |      .       @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n",
      "     |      .       have the same number of channels as images.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class AlignMTB(AlignExposures)\n",
      "     |  Method resolution order:\n",
      "     |      AlignMTB\n",
      "     |      AlignExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  calculateShift(...)\n",
      "     |      calculateShift(img0, img1) -> retval\n",
      "     |      .   @brief Calculates shift between two images, i. e. how to shift the second image to correspond it with the\n",
      "     |      .       first.\n",
      "     |      .   \n",
      "     |      .       @param img0 first image\n",
      "     |      .       @param img1 second image\n",
      "     |  \n",
      "     |  computeBitmaps(...)\n",
      "     |      computeBitmaps(img[, tb[, eb]]) -> tb, eb\n",
      "     |      .   @brief Computes median threshold and exclude bitmaps of given image.\n",
      "     |      .   \n",
      "     |      .       @param img input image\n",
      "     |      .       @param tb median threshold bitmap\n",
      "     |      .       @param eb exclude bitmap\n",
      "     |  \n",
      "     |  getCut(...)\n",
      "     |      getCut() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getExcludeRange(...)\n",
      "     |      getExcludeRange() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxBits(...)\n",
      "     |      getMaxBits() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, dst, times, response) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      process(src, dst) -> None\n",
      "     |      .   @brief Short version of process, that doesn't take extra arguments.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst vector of aligned images\n",
      "     |  \n",
      "     |  setCut(...)\n",
      "     |      setCut(value) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setExcludeRange(...)\n",
      "     |      setExcludeRange(exclude_range) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxBits(...)\n",
      "     |      setMaxBits(max_bits) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  shiftMat(...)\n",
      "     |      shiftMat(src, shift[, dst]) -> dst\n",
      "     |      .   @brief Helper function, that shift Mat filling new regions with zeros.\n",
      "     |      .   \n",
      "     |      .       @param src input image\n",
      "     |      .       @param dst result image\n",
      "     |      .       @param shift shift value\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class AsyncArray(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      get([, dst]) -> dst\n",
      "     |      .   Fetch the result.\n",
      "     |      .       @param[out] dst destination array\n",
      "     |      .   \n",
      "     |      .       Waits for result until container has valid result.\n",
      "     |      .       Throws exception if exception was stored as a result.\n",
      "     |      .   \n",
      "     |      .       Throws exception on invalid container state.\n",
      "     |      .   \n",
      "     |      .       @note Result or stored exception can be fetched only once.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      get(timeoutNs[, dst]) -> retval, dst\n",
      "     |      .   Retrieving the result with timeout\n",
      "     |      .       @param[out] dst destination array\n",
      "     |      .       @param[in] timeoutNs timeout in nanoseconds, -1 for infinite wait\n",
      "     |      .   \n",
      "     |      .       @returns true if result is ready, false if the timeout has expired\n",
      "     |      .   \n",
      "     |      .       @note Result or stored exception can be fetched only once.\n",
      "     |  \n",
      "     |  release(...)\n",
      "     |      release() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  valid(...)\n",
      "     |      valid() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  wait_for(...)\n",
      "     |      wait_for(timeoutNs) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BFMatcher(DescriptorMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      BFMatcher\n",
      "     |      DescriptorMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, normType[, crossCheck]]) -> retval\n",
      "     |      .   @brief Brute-force matcher create method.\n",
      "     |      .       @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n",
      "     |      .       preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n",
      "     |      .       BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n",
      "     |      .       description).\n",
      "     |      .       @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n",
      "     |      .       nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n",
      "     |      .       k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n",
      "     |      .       matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n",
      "     |      .       pairs. Such technique usually produces best results with minimal number of outliers when there are\n",
      "     |      .       enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DescriptorMatcher:\n",
      "     |  \n",
      "     |  add(...)\n",
      "     |      add(descriptors) -> None\n",
      "     |      .   @brief Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor\n",
      "     |      .       collection.\n",
      "     |      .   \n",
      "     |      .       If the collection is not empty, the new descriptors are added to existing train descriptors.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n",
      "     |      .       train image.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the train descriptor collections.\n",
      "     |  \n",
      "     |  clone(...)\n",
      "     |      clone([, emptyTrainData]) -> retval\n",
      "     |      .   @brief Clones the matcher.\n",
      "     |      .   \n",
      "     |      .       @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n",
      "     |      .       that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n",
      "     |      .       object copy with the current parameters but with empty train data.\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if there are no train descriptors in the both collections.\n",
      "     |  \n",
      "     |  getTrainDescriptors(...)\n",
      "     |      getTrainDescriptors() -> retval\n",
      "     |      .   @brief Returns a constant link to the train descriptor collection trainDescCollection .\n",
      "     |  \n",
      "     |  isMaskSupported(...)\n",
      "     |      isMaskSupported() -> retval\n",
      "     |      .   @brief Returns true if the descriptor matcher supports masking permissible matches.\n",
      "     |  \n",
      "     |  knnMatch(...)\n",
      "     |      knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief Finds the k best matches for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .   \n",
      "     |      .       These extended variants of DescriptorMatcher::match methods find several best matches for each query\n",
      "     |      .       descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match\n",
      "     |      .       for the details about query and train descriptors.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      knnMatch(queryDescriptors, k[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  match(...)\n",
      "     |      match(queryDescriptors, trainDescriptors[, mask]) -> matches\n",
      "     |      .   @brief Finds the best match for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       In the first variant of this method, the train descriptors are passed as an input argument. In the\n",
      "     |      .       second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n",
      "     |      .       used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n",
      "     |      .       matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n",
      "     |      .       mask.at\\<uchar\\>(i,j) is non-zero.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      match(queryDescriptors[, masks]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |  \n",
      "     |  radiusMatch(...)\n",
      "     |      radiusMatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief For each query descriptor, finds the training descriptors not farther than the specified distance.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       For each query descriptor, the methods find such training descriptors that the distance between the\n",
      "     |      .       query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are\n",
      "     |      .       returned in the distance increasing order.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      radiusMatch(queryDescriptors, maxDistance[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train() -> None\n",
      "     |      .   @brief Trains a descriptor matcher\n",
      "     |      .   \n",
      "     |      .       Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n",
      "     |      .       train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)\n",
      "     |      .       have an empty implementation of this method. Other matchers really train their inner structures (for\n",
      "     |      .       example, FlannBasedMatcher trains flann::Index ).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "    \n",
      "    class BOWImgDescriptorExtractor(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, imgDescriptor]) -> imgDescriptor\n",
      "     |      .   @overload\n",
      "     |      .       @param keypointDescriptors Computed descriptors to match with vocabulary.\n",
      "     |      .       @param imgDescriptor Computed output image descriptor.\n",
      "     |      .       @param pointIdxsOfClusters Indices of keypoints that belong to the cluster. This means that\n",
      "     |      .       pointIdxsOfClusters[i] are keypoint indices that belong to the i -th cluster (word of vocabulary)\n",
      "     |      .       returned if it is non-zero.\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .   @brief Returns an image descriptor size if the vocabulary is set. Otherwise, it returns 0.\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .   @brief Returns an image descriptor type.\n",
      "     |  \n",
      "     |  getVocabulary(...)\n",
      "     |      getVocabulary() -> retval\n",
      "     |      .   @brief Returns the set vocabulary.\n",
      "     |  \n",
      "     |  setVocabulary(...)\n",
      "     |      setVocabulary(vocabulary) -> None\n",
      "     |      .   @brief Sets a visual vocabulary.\n",
      "     |      .   \n",
      "     |      .       @param vocabulary Vocabulary (can be trained using the inheritor of BOWTrainer ). Each row of the\n",
      "     |      .       vocabulary is a visual word (cluster center).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BOWKMeansTrainer(BOWTrainer)\n",
      "     |  Method resolution order:\n",
      "     |      BOWKMeansTrainer\n",
      "     |      BOWTrainer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cluster(...)\n",
      "     |      cluster() -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      cluster(descriptors) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BOWTrainer:\n",
      "     |  \n",
      "     |  add(...)\n",
      "     |      add(descriptors) -> None\n",
      "     |      .   @brief Adds descriptors to a training set.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n",
      "     |      .       descriptor.\n",
      "     |      .   \n",
      "     |      .       The training set is clustered using clustermethod to construct the vocabulary.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorsCount(...)\n",
      "     |      descriptorsCount() -> retval\n",
      "     |      .   @brief Returns the count of all descriptors stored in the training set.\n",
      "     |  \n",
      "     |  getDescriptors(...)\n",
      "     |      getDescriptors() -> retval\n",
      "     |      .   @brief Returns a training set of descriptors.\n",
      "    \n",
      "    class BOWTrainer(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add(...)\n",
      "     |      add(descriptors) -> None\n",
      "     |      .   @brief Adds descriptors to a training set.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a\n",
      "     |      .       descriptor.\n",
      "     |      .   \n",
      "     |      .       The training set is clustered using clustermethod to construct the vocabulary.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  cluster(...)\n",
      "     |      cluster() -> retval\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      cluster(descriptors) -> retval\n",
      "     |      .   @brief Clusters train descriptors.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.\n",
      "     |      .       Descriptors are not added to the inner train descriptor set.\n",
      "     |      .   \n",
      "     |      .       The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first\n",
      "     |      .       variant of the method, train descriptors stored in the object are clustered. In the second variant,\n",
      "     |      .       input descriptors are clustered.\n",
      "     |  \n",
      "     |  descriptorsCount(...)\n",
      "     |      descriptorsCount() -> retval\n",
      "     |      .   @brief Returns the count of all descriptors stored in the training set.\n",
      "     |  \n",
      "     |  getDescriptors(...)\n",
      "     |      getDescriptors() -> retval\n",
      "     |      .   @brief Returns a training set of descriptors.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BRISK(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      BRISK\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getOctaves(...)\n",
      "     |      getOctaves() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setOctaves(...)\n",
      "     |      setOctaves(octaves) -> None\n",
      "     |      .   @brief Set detection octaves.\n",
      "     |      .       @param octaves detection octaves. Use 0 to do single scale.\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .   @brief Set detection threshold.\n",
      "     |      .       @param threshold AGAST detection threshold score.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, thresh[, octaves[, patternScale]]]) -> retval\n",
      "     |      .   @brief The BRISK constructor\n",
      "     |      .   \n",
      "     |      .       @param thresh AGAST detection threshold score.\n",
      "     |      .       @param octaves detection octaves. Use 0 to do single scale.\n",
      "     |      .       @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a\n",
      "     |      .       keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      create(radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n",
      "     |      .   @brief The BRISK constructor for a custom pattern\n",
      "     |      .   \n",
      "     |      .       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n",
      "     |      .       keypoint scale 1).\n",
      "     |      .       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n",
      "     |      .       size as radiusList..\n",
      "     |      .       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n",
      "     |      .       scale 1).\n",
      "     |      .       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n",
      "     |      .       keypoint scale 1).\n",
      "     |      .   @param indexChange index remapping of the bits.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      create(thresh, octaves, radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n",
      "     |      .   @brief The BRISK constructor for a custom pattern, detection threshold and octaves\n",
      "     |      .   \n",
      "     |      .       @param thresh AGAST detection threshold score.\n",
      "     |      .       @param octaves detection octaves. Use 0 to do single scale.\n",
      "     |      .       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n",
      "     |      .       keypoint scale 1).\n",
      "     |      .       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n",
      "     |      .       size as radiusList..\n",
      "     |      .       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n",
      "     |      .       scale 1).\n",
      "     |      .       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n",
      "     |      .       keypoint scale 1).\n",
      "     |      .   @param indexChange index remapping of the bits.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class BackgroundSubtractor(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      BackgroundSubtractor\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(image[, fgmask[, learningRate]]) -> fgmask\n",
      "     |      .   @brief Computes a foreground mask.\n",
      "     |      .   \n",
      "     |      .       @param image Next video frame.\n",
      "     |      .       @param fgmask The output foreground mask as an 8-bit binary image.\n",
      "     |      .       @param learningRate The value between 0 and 1 that indicates how fast the background model is\n",
      "     |      .       learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n",
      "     |      .       rate. 0 means that the background model is not updated at all, 1 means that the background model\n",
      "     |      .       is completely reinitialized from the last frame.\n",
      "     |  \n",
      "     |  getBackgroundImage(...)\n",
      "     |      getBackgroundImage([, backgroundImage]) -> backgroundImage\n",
      "     |      .   @brief Computes a background image.\n",
      "     |      .   \n",
      "     |      .       @param backgroundImage The output background image.\n",
      "     |      .   \n",
      "     |      .       @note Sometimes the background image can be very blurry, as it contain the average background\n",
      "     |      .       statistics.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class BackgroundSubtractorKNN(BackgroundSubtractor)\n",
      "     |  Method resolution order:\n",
      "     |      BackgroundSubtractorKNN\n",
      "     |      BackgroundSubtractor\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDetectShadows(...)\n",
      "     |      getDetectShadows() -> retval\n",
      "     |      .   @brief Returns the shadow detection flag\n",
      "     |      .   \n",
      "     |      .       If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorKNN for\n",
      "     |      .       details.\n",
      "     |  \n",
      "     |  getDist2Threshold(...)\n",
      "     |      getDist2Threshold() -> retval\n",
      "     |      .   @brief Returns the threshold on the squared distance between the pixel and the sample\n",
      "     |      .   \n",
      "     |      .       The threshold on the squared distance between the pixel and the sample to decide whether a pixel is\n",
      "     |      .       close to a data sample.\n",
      "     |  \n",
      "     |  getHistory(...)\n",
      "     |      getHistory() -> retval\n",
      "     |      .   @brief Returns the number of last frames that affect the background model\n",
      "     |  \n",
      "     |  getNSamples(...)\n",
      "     |      getNSamples() -> retval\n",
      "     |      .   @brief Returns the number of data samples in the background model\n",
      "     |  \n",
      "     |  getShadowThreshold(...)\n",
      "     |      getShadowThreshold() -> retval\n",
      "     |      .   @brief Returns the shadow threshold\n",
      "     |      .   \n",
      "     |      .       A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n",
      "     |      .       the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n",
      "     |      .       is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n",
      "     |      .       *Detecting Moving Shadows...*, IEEE PAMI,2003.\n",
      "     |  \n",
      "     |  getShadowValue(...)\n",
      "     |      getShadowValue() -> retval\n",
      "     |      .   @brief Returns the shadow value\n",
      "     |      .   \n",
      "     |      .       Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n",
      "     |      .       in the mask always means background, 255 means foreground.\n",
      "     |  \n",
      "     |  getkNNSamples(...)\n",
      "     |      getkNNSamples() -> retval\n",
      "     |      .   @brief Returns the number of neighbours, the k in the kNN.\n",
      "     |      .   \n",
      "     |      .       K is the number of samples that need to be within dist2Threshold in order to decide that that\n",
      "     |      .       pixel is matching the kNN background model.\n",
      "     |  \n",
      "     |  setDetectShadows(...)\n",
      "     |      setDetectShadows(detectShadows) -> None\n",
      "     |      .   @brief Enables or disables shadow detection\n",
      "     |  \n",
      "     |  setDist2Threshold(...)\n",
      "     |      setDist2Threshold(_dist2Threshold) -> None\n",
      "     |      .   @brief Sets the threshold on the squared distance\n",
      "     |  \n",
      "     |  setHistory(...)\n",
      "     |      setHistory(history) -> None\n",
      "     |      .   @brief Sets the number of last frames that affect the background model\n",
      "     |  \n",
      "     |  setNSamples(...)\n",
      "     |      setNSamples(_nN) -> None\n",
      "     |      .   @brief Sets the number of data samples in the background model.\n",
      "     |      .   \n",
      "     |      .       The model needs to be reinitalized to reserve memory.\n",
      "     |  \n",
      "     |  setShadowThreshold(...)\n",
      "     |      setShadowThreshold(threshold) -> None\n",
      "     |      .   @brief Sets the shadow threshold\n",
      "     |  \n",
      "     |  setShadowValue(...)\n",
      "     |      setShadowValue(value) -> None\n",
      "     |      .   @brief Sets the shadow value\n",
      "     |  \n",
      "     |  setkNNSamples(...)\n",
      "     |      setkNNSamples(_nkNN) -> None\n",
      "     |      .   @brief Sets the k in the kNN. How many nearest neighbours need to match.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BackgroundSubtractor:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(image[, fgmask[, learningRate]]) -> fgmask\n",
      "     |      .   @brief Computes a foreground mask.\n",
      "     |      .   \n",
      "     |      .       @param image Next video frame.\n",
      "     |      .       @param fgmask The output foreground mask as an 8-bit binary image.\n",
      "     |      .       @param learningRate The value between 0 and 1 that indicates how fast the background model is\n",
      "     |      .       learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n",
      "     |      .       rate. 0 means that the background model is not updated at all, 1 means that the background model\n",
      "     |      .       is completely reinitialized from the last frame.\n",
      "     |  \n",
      "     |  getBackgroundImage(...)\n",
      "     |      getBackgroundImage([, backgroundImage]) -> backgroundImage\n",
      "     |      .   @brief Computes a background image.\n",
      "     |      .   \n",
      "     |      .       @param backgroundImage The output background image.\n",
      "     |      .   \n",
      "     |      .       @note Sometimes the background image can be very blurry, as it contain the average background\n",
      "     |      .       statistics.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class BackgroundSubtractorMOG2(BackgroundSubtractor)\n",
      "     |  Method resolution order:\n",
      "     |      BackgroundSubtractorMOG2\n",
      "     |      BackgroundSubtractor\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(image[, fgmask[, learningRate]]) -> fgmask\n",
      "     |      .   @brief Computes a foreground mask.\n",
      "     |      .   \n",
      "     |      .       @param image Next video frame. Floating point frame will be used without scaling and should be in range \\f$[0,255]\\f$.\n",
      "     |      .       @param fgmask The output foreground mask as an 8-bit binary image.\n",
      "     |      .       @param learningRate The value between 0 and 1 that indicates how fast the background model is\n",
      "     |      .       learnt. Negative parameter value makes the algorithm to use some automatically chosen learning\n",
      "     |      .       rate. 0 means that the background model is not updated at all, 1 means that the background model\n",
      "     |      .       is completely reinitialized from the last frame.\n",
      "     |  \n",
      "     |  getBackgroundRatio(...)\n",
      "     |      getBackgroundRatio() -> retval\n",
      "     |      .   @brief Returns the \"background ratio\" parameter of the algorithm\n",
      "     |      .   \n",
      "     |      .       If a foreground pixel keeps semi-constant value for about backgroundRatio\\*history frames, it's\n",
      "     |      .       considered background and added to the model as a center of a new component. It corresponds to TB\n",
      "     |      .       parameter in the paper.\n",
      "     |  \n",
      "     |  getComplexityReductionThreshold(...)\n",
      "     |      getComplexityReductionThreshold() -> retval\n",
      "     |      .   @brief Returns the complexity reduction threshold\n",
      "     |      .   \n",
      "     |      .       This parameter defines the number of samples needed to accept to prove the component exists. CT=0.05\n",
      "     |      .       is a default value for all the samples. By setting CT=0 you get an algorithm very similar to the\n",
      "     |      .       standard Stauffer&Grimson algorithm.\n",
      "     |  \n",
      "     |  getDetectShadows(...)\n",
      "     |      getDetectShadows() -> retval\n",
      "     |      .   @brief Returns the shadow detection flag\n",
      "     |      .   \n",
      "     |      .       If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorMOG2 for\n",
      "     |      .       details.\n",
      "     |  \n",
      "     |  getHistory(...)\n",
      "     |      getHistory() -> retval\n",
      "     |      .   @brief Returns the number of last frames that affect the background model\n",
      "     |  \n",
      "     |  getNMixtures(...)\n",
      "     |      getNMixtures() -> retval\n",
      "     |      .   @brief Returns the number of gaussian components in the background model\n",
      "     |  \n",
      "     |  getShadowThreshold(...)\n",
      "     |      getShadowThreshold() -> retval\n",
      "     |      .   @brief Returns the shadow threshold\n",
      "     |      .   \n",
      "     |      .       A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in\n",
      "     |      .       the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel\n",
      "     |      .       is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara,\n",
      "     |      .       *Detecting Moving Shadows...*, IEEE PAMI,2003.\n",
      "     |  \n",
      "     |  getShadowValue(...)\n",
      "     |      getShadowValue() -> retval\n",
      "     |      .   @brief Returns the shadow value\n",
      "     |      .   \n",
      "     |      .       Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0\n",
      "     |      .       in the mask always means background, 255 means foreground.\n",
      "     |  \n",
      "     |  getVarInit(...)\n",
      "     |      getVarInit() -> retval\n",
      "     |      .   @brief Returns the initial variance of each gaussian component\n",
      "     |  \n",
      "     |  getVarMax(...)\n",
      "     |      getVarMax() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarMin(...)\n",
      "     |      getVarMin() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarThreshold(...)\n",
      "     |      getVarThreshold() -> retval\n",
      "     |      .   @brief Returns the variance threshold for the pixel-model match\n",
      "     |      .   \n",
      "     |      .       The main threshold on the squared Mahalanobis distance to decide if the sample is well described by\n",
      "     |      .       the background model or not. Related to Cthr from the paper.\n",
      "     |  \n",
      "     |  getVarThresholdGen(...)\n",
      "     |      getVarThresholdGen() -> retval\n",
      "     |      .   @brief Returns the variance threshold for the pixel-model match used for new mixture component generation\n",
      "     |      .   \n",
      "     |      .       Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the\n",
      "     |      .       existing components (corresponds to Tg in the paper). If a pixel is not close to any component, it\n",
      "     |      .       is considered foreground or added as a new component. 3 sigma =\\> Tg=3\\*3=9 is default. A smaller Tg\n",
      "     |      .       value generates more components. A higher Tg value may result in a small number of components but\n",
      "     |      .       they can grow too large.\n",
      "     |  \n",
      "     |  setBackgroundRatio(...)\n",
      "     |      setBackgroundRatio(ratio) -> None\n",
      "     |      .   @brief Sets the \"background ratio\" parameter of the algorithm\n",
      "     |  \n",
      "     |  setComplexityReductionThreshold(...)\n",
      "     |      setComplexityReductionThreshold(ct) -> None\n",
      "     |      .   @brief Sets the complexity reduction threshold\n",
      "     |  \n",
      "     |  setDetectShadows(...)\n",
      "     |      setDetectShadows(detectShadows) -> None\n",
      "     |      .   @brief Enables or disables shadow detection\n",
      "     |  \n",
      "     |  setHistory(...)\n",
      "     |      setHistory(history) -> None\n",
      "     |      .   @brief Sets the number of last frames that affect the background model\n",
      "     |  \n",
      "     |  setNMixtures(...)\n",
      "     |      setNMixtures(nmixtures) -> None\n",
      "     |      .   @brief Sets the number of gaussian components in the background model.\n",
      "     |      .   \n",
      "     |      .       The model needs to be reinitalized to reserve memory.\n",
      "     |  \n",
      "     |  setShadowThreshold(...)\n",
      "     |      setShadowThreshold(threshold) -> None\n",
      "     |      .   @brief Sets the shadow threshold\n",
      "     |  \n",
      "     |  setShadowValue(...)\n",
      "     |      setShadowValue(value) -> None\n",
      "     |      .   @brief Sets the shadow value\n",
      "     |  \n",
      "     |  setVarInit(...)\n",
      "     |      setVarInit(varInit) -> None\n",
      "     |      .   @brief Sets the initial variance of each gaussian component\n",
      "     |  \n",
      "     |  setVarMax(...)\n",
      "     |      setVarMax(varMax) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setVarMin(...)\n",
      "     |      setVarMin(varMin) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setVarThreshold(...)\n",
      "     |      setVarThreshold(varThreshold) -> None\n",
      "     |      .   @brief Sets the variance threshold for the pixel-model match\n",
      "     |  \n",
      "     |  setVarThresholdGen(...)\n",
      "     |      setVarThresholdGen(varThresholdGen) -> None\n",
      "     |      .   @brief Sets the variance threshold for the pixel-model match used for new mixture component generation\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BackgroundSubtractor:\n",
      "     |  \n",
      "     |  getBackgroundImage(...)\n",
      "     |      getBackgroundImage([, backgroundImage]) -> backgroundImage\n",
      "     |      .   @brief Computes a background image.\n",
      "     |      .   \n",
      "     |      .       @param backgroundImage The output background image.\n",
      "     |      .   \n",
      "     |      .       @note Sometimes the background image can be very blurry, as it contain the average background\n",
      "     |      .       statistics.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class BaseCascadeClassifier(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      BaseCascadeClassifier\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class CLAHE(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      CLAHE\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(src[, dst]) -> dst\n",
      "     |      .   @brief Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.\n",
      "     |      .   \n",
      "     |      .       @param src Source image of type CV_8UC1 or CV_16UC1.\n",
      "     |      .       @param dst Destination image.\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  getClipLimit(...)\n",
      "     |      getClipLimit() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTilesGridSize(...)\n",
      "     |      getTilesGridSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setClipLimit(...)\n",
      "     |      setClipLimit(clipLimit) -> None\n",
      "     |      .   @brief Sets threshold for contrast limiting.\n",
      "     |      .   \n",
      "     |      .       @param clipLimit threshold value.\n",
      "     |  \n",
      "     |  setTilesGridSize(...)\n",
      "     |      setTilesGridSize(tileGridSize) -> None\n",
      "     |      .   @brief Sets size of grid for histogram equalization. Input image will be divided into\n",
      "     |      .       equally sized rectangular tiles.\n",
      "     |      .   \n",
      "     |      .       @param tileGridSize defines the number of tiles in row and column.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class CalibrateCRF(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      CalibrateCRF\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times[, dst]) -> dst\n",
      "     |      .   @brief Recovers inverse camera response.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst 256x1 matrix with inverse camera response function\n",
      "     |      .       @param times vector of exposure time values for each image\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class CalibrateDebevec(CalibrateCRF)\n",
      "     |  Method resolution order:\n",
      "     |      CalibrateDebevec\n",
      "     |      CalibrateCRF\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getLambda(...)\n",
      "     |      getLambda() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getRandom(...)\n",
      "     |      getRandom() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSamples(...)\n",
      "     |      getSamples() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setLambda(...)\n",
      "     |      setLambda(lambda) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRandom(...)\n",
      "     |      setRandom(random) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSamples(...)\n",
      "     |      setSamples(samples) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CalibrateCRF:\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times[, dst]) -> dst\n",
      "     |      .   @brief Recovers inverse camera response.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst 256x1 matrix with inverse camera response function\n",
      "     |      .       @param times vector of exposure time values for each image\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class CalibrateRobertson(CalibrateCRF)\n",
      "     |  Method resolution order:\n",
      "     |      CalibrateRobertson\n",
      "     |      CalibrateCRF\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getMaxIter(...)\n",
      "     |      getMaxIter() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getRadiance(...)\n",
      "     |      getRadiance() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxIter(...)\n",
      "     |      setMaxIter(max_iter) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CalibrateCRF:\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times[, dst]) -> dst\n",
      "     |      .   @brief Recovers inverse camera response.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst 256x1 matrix with inverse camera response function\n",
      "     |      .       @param times vector of exposure time values for each image\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class CascadeClassifier(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  detectMultiScale(...)\n",
      "     |      detectMultiScale(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects\n",
      "     |      .   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n",
      "     |      .       of rectangles.\n",
      "     |      .   \n",
      "     |      .       @param image Matrix of the type CV_8U containing an image where objects are detected.\n",
      "     |      .       @param objects Vector of rectangles where each rectangle contains the detected object, the\n",
      "     |      .       rectangles may be partially outside the original image.\n",
      "     |      .       @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n",
      "     |      .       @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n",
      "     |      .       to retain it.\n",
      "     |      .       @param flags Parameter with the same meaning for an old cascade as in the function\n",
      "     |      .       cvHaarDetectObjects. It is not used for a new cascade.\n",
      "     |      .       @param minSize Minimum possible object size. Objects smaller than that are ignored.\n",
      "     |      .       @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale.\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |      .   \n",
      "     |      .       @note\n",
      "     |      .          -   (Python) A face detection example using cascade classifiers can be found at\n",
      "     |      .               opencv_source_code/samples/python/facedetect.py\n",
      "     |  \n",
      "     |  detectMultiScale2(...)\n",
      "     |      detectMultiScale2(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize]]]]]) -> objects, numDetections\n",
      "     |      .   @overload\n",
      "     |      .       @param image Matrix of the type CV_8U containing an image where objects are detected.\n",
      "     |      .       @param objects Vector of rectangles where each rectangle contains the detected object, the\n",
      "     |      .       rectangles may be partially outside the original image.\n",
      "     |      .       @param numDetections Vector of detection numbers for the corresponding objects. An object's number\n",
      "     |      .       of detections is the number of neighboring positively classified rectangles that were joined\n",
      "     |      .       together to form the object.\n",
      "     |      .       @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.\n",
      "     |      .       @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have\n",
      "     |      .       to retain it.\n",
      "     |      .       @param flags Parameter with the same meaning for an old cascade as in the function\n",
      "     |      .       cvHaarDetectObjects. It is not used for a new cascade.\n",
      "     |      .       @param minSize Minimum possible object size. Objects smaller than that are ignored.\n",
      "     |      .       @param maxSize Maximum possible object size. Objects larger than that are ignored. If `maxSize == minSize` model is evaluated on single scale.\n",
      "     |  \n",
      "     |  detectMultiScale3(...)\n",
      "     |      detectMultiScale3(image[, scaleFactor[, minNeighbors[, flags[, minSize[, maxSize[, outputRejectLevels]]]]]]) -> objects, rejectLevels, levelWeights\n",
      "     |      .   @overload\n",
      "     |      .       This function allows you to retrieve the final stage decision certainty of classification.\n",
      "     |      .       For this, one needs to set `outputRejectLevels` on true and provide the `rejectLevels` and `levelWeights` parameter.\n",
      "     |      .       For each resulting detection, `levelWeights` will then contain the certainty of classification at the final stage.\n",
      "     |      .       This value can then be used to separate strong from weaker classifications.\n",
      "     |      .   \n",
      "     |      .       A code sample on how to use it efficiently can be found below:\n",
      "     |      .       @code\n",
      "     |      .       Mat img;\n",
      "     |      .       vector<double> weights;\n",
      "     |      .       vector<int> levels;\n",
      "     |      .       vector<Rect> detections;\n",
      "     |      .       CascadeClassifier model(\"/path/to/your/model.xml\");\n",
      "     |      .       model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);\n",
      "     |      .       cerr << \"Detection \" << detections[0] << \" with weight \" << weights[0] << endl;\n",
      "     |      .       @endcode\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Checks whether the classifier has been loaded.\n",
      "     |  \n",
      "     |  getFeatureType(...)\n",
      "     |      getFeatureType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getOriginalWindowSize(...)\n",
      "     |      getOriginalWindowSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isOldFormatCascade(...)\n",
      "     |      isOldFormatCascade() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filename) -> retval\n",
      "     |      .   @brief Loads a classifier from a file.\n",
      "     |      .   \n",
      "     |      .       @param filename Name of the file from which the classifier is loaded. The file may contain an old\n",
      "     |      .       HAAR classifier trained by the haartraining application or a new cascade classifier trained by the\n",
      "     |      .       traincascade application.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(node) -> retval\n",
      "     |      .   @brief Reads a classifier from a FileStorage node.\n",
      "     |      .   \n",
      "     |      .       @note The file may contain a new cascade classifier (trained traincascade application) only.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  convert(...)\n",
      "     |      convert(oldcascade, newcascade) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class CirclesGridFinderParameters(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  convexHullFactor\n",
      "     |      convexHullFactor\n",
      "     |  \n",
      "     |  densityNeighborhoodSize\n",
      "     |      densityNeighborhoodSize\n",
      "     |  \n",
      "     |  edgeGain\n",
      "     |      edgeGain\n",
      "     |  \n",
      "     |  edgePenalty\n",
      "     |      edgePenalty\n",
      "     |  \n",
      "     |  existingVertexGain\n",
      "     |      existingVertexGain\n",
      "     |  \n",
      "     |  keypointScale\n",
      "     |      keypointScale\n",
      "     |  \n",
      "     |  kmeansAttempts\n",
      "     |      kmeansAttempts\n",
      "     |  \n",
      "     |  maxRectifiedDistance\n",
      "     |      maxRectifiedDistance\n",
      "     |  \n",
      "     |  minDensity\n",
      "     |      minDensity\n",
      "     |  \n",
      "     |  minDistanceToAddKeypoint\n",
      "     |      minDistanceToAddKeypoint\n",
      "     |  \n",
      "     |  minGraphConfidence\n",
      "     |      minGraphConfidence\n",
      "     |  \n",
      "     |  minRNGEdgeSwitchDist\n",
      "     |      minRNGEdgeSwitchDist\n",
      "     |  \n",
      "     |  squareSize\n",
      "     |      squareSize\n",
      "     |  \n",
      "     |  vertexGain\n",
      "     |      vertexGain\n",
      "     |  \n",
      "     |  vertexPenalty\n",
      "     |      vertexPenalty\n",
      "    \n",
      "    class DISOpticalFlow(DenseOpticalFlow)\n",
      "     |  Method resolution order:\n",
      "     |      DISOpticalFlow\n",
      "     |      DenseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getFinestScale(...)\n",
      "     |      getFinestScale() -> retval\n",
      "     |      .   @brief Finest level of the Gaussian pyramid on which the flow is computed (zero level\n",
      "     |      .           corresponds to the original image resolution). The final flow is obtained by bilinear upscaling.\n",
      "     |      .   @see setFinestScale\n",
      "     |  \n",
      "     |  getGradientDescentIterations(...)\n",
      "     |      getGradientDescentIterations() -> retval\n",
      "     |      .   @brief Maximum number of gradient descent iterations in the patch inverse search stage. Higher values\n",
      "     |      .           may improve quality in some cases.\n",
      "     |      .   @see setGradientDescentIterations\n",
      "     |  \n",
      "     |  getPatchSize(...)\n",
      "     |      getPatchSize() -> retval\n",
      "     |      .   @brief Size of an image patch for matching (in pixels). Normally, default 8x8 patches work well\n",
      "     |      .           enough in most cases.\n",
      "     |      .   @see setPatchSize\n",
      "     |  \n",
      "     |  getPatchStride(...)\n",
      "     |      getPatchStride() -> retval\n",
      "     |      .   @brief Stride between neighbor patches. Must be less than patch size. Lower values correspond\n",
      "     |      .           to higher flow quality.\n",
      "     |      .   @see setPatchStride\n",
      "     |  \n",
      "     |  getUseMeanNormalization(...)\n",
      "     |      getUseMeanNormalization() -> retval\n",
      "     |      .   @brief Whether to use mean-normalization of patches when computing patch distance. It is turned on\n",
      "     |      .           by default as it typically provides a noticeable quality boost because of increased robustness to\n",
      "     |      .           illumination variations. Turn it off if you are certain that your sequence doesn't contain any changes\n",
      "     |      .           in illumination.\n",
      "     |      .   @see setUseMeanNormalization\n",
      "     |  \n",
      "     |  getUseSpatialPropagation(...)\n",
      "     |      getUseSpatialPropagation() -> retval\n",
      "     |      .   @brief Whether to use spatial propagation of good optical flow vectors. This option is turned on by\n",
      "     |      .           default, as it tends to work better on average and can sometimes help recover from major errors\n",
      "     |      .           introduced by the coarse-to-fine scheme employed by the DIS optical flow algorithm. Turning this\n",
      "     |      .           option off can make the output flow field a bit smoother, however.\n",
      "     |      .   @see setUseSpatialPropagation\n",
      "     |  \n",
      "     |  getVariationalRefinementAlpha(...)\n",
      "     |      getVariationalRefinementAlpha() -> retval\n",
      "     |      .   @brief Weight of the smoothness term\n",
      "     |      .   @see setVariationalRefinementAlpha\n",
      "     |  \n",
      "     |  getVariationalRefinementDelta(...)\n",
      "     |      getVariationalRefinementDelta() -> retval\n",
      "     |      .   @brief Weight of the color constancy term\n",
      "     |      .   @see setVariationalRefinementDelta\n",
      "     |  \n",
      "     |  getVariationalRefinementGamma(...)\n",
      "     |      getVariationalRefinementGamma() -> retval\n",
      "     |      .   @brief Weight of the gradient constancy term\n",
      "     |      .   @see setVariationalRefinementGamma\n",
      "     |  \n",
      "     |  getVariationalRefinementIterations(...)\n",
      "     |      getVariationalRefinementIterations() -> retval\n",
      "     |      .   @brief Number of fixed point iterations of variational refinement per scale. Set to zero to\n",
      "     |      .           disable variational refinement completely. Higher values will typically result in more smooth and\n",
      "     |      .           high-quality flow.\n",
      "     |      .   @see setGradientDescentIterations\n",
      "     |  \n",
      "     |  setFinestScale(...)\n",
      "     |      setFinestScale(val) -> None\n",
      "     |      .   @copybrief getFinestScale @see getFinestScale\n",
      "     |  \n",
      "     |  setGradientDescentIterations(...)\n",
      "     |      setGradientDescentIterations(val) -> None\n",
      "     |      .   @copybrief getGradientDescentIterations @see getGradientDescentIterations\n",
      "     |  \n",
      "     |  setPatchSize(...)\n",
      "     |      setPatchSize(val) -> None\n",
      "     |      .   @copybrief getPatchSize @see getPatchSize\n",
      "     |  \n",
      "     |  setPatchStride(...)\n",
      "     |      setPatchStride(val) -> None\n",
      "     |      .   @copybrief getPatchStride @see getPatchStride\n",
      "     |  \n",
      "     |  setUseMeanNormalization(...)\n",
      "     |      setUseMeanNormalization(val) -> None\n",
      "     |      .   @copybrief getUseMeanNormalization @see getUseMeanNormalization\n",
      "     |  \n",
      "     |  setUseSpatialPropagation(...)\n",
      "     |      setUseSpatialPropagation(val) -> None\n",
      "     |      .   @copybrief getUseSpatialPropagation @see getUseSpatialPropagation\n",
      "     |  \n",
      "     |  setVariationalRefinementAlpha(...)\n",
      "     |      setVariationalRefinementAlpha(val) -> None\n",
      "     |      .   @copybrief getVariationalRefinementAlpha @see getVariationalRefinementAlpha\n",
      "     |  \n",
      "     |  setVariationalRefinementDelta(...)\n",
      "     |      setVariationalRefinementDelta(val) -> None\n",
      "     |      .   @copybrief getVariationalRefinementDelta @see getVariationalRefinementDelta\n",
      "     |  \n",
      "     |  setVariationalRefinementGamma(...)\n",
      "     |      setVariationalRefinementGamma(val) -> None\n",
      "     |      .   @copybrief getVariationalRefinementGamma @see getVariationalRefinementGamma\n",
      "     |  \n",
      "     |  setVariationalRefinementIterations(...)\n",
      "     |      setVariationalRefinementIterations(val) -> None\n",
      "     |      .   @copybrief getGradientDescentIterations @see getGradientDescentIterations\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, preset]) -> retval\n",
      "     |      .   @brief Creates an instance of DISOpticalFlow\n",
      "     |      .   \n",
      "     |      .       @param preset one of PRESET_ULTRAFAST, PRESET_FAST and PRESET_MEDIUM\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DenseOpticalFlow:\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(I0, I1, flow) -> flow\n",
      "     |      .   @brief Calculates an optical flow.\n",
      "     |      .   \n",
      "     |      .       @param I0 first 8-bit single-channel input image.\n",
      "     |      .       @param I1 second input image of the same size and the same type as prev.\n",
      "     |      .       @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .   @brief Releases all inner buffers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class DMatch(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  distance\n",
      "     |      distance\n",
      "     |  \n",
      "     |  imgIdx\n",
      "     |      imgIdx\n",
      "     |  \n",
      "     |  queryIdx\n",
      "     |      queryIdx\n",
      "     |  \n",
      "     |  trainIdx\n",
      "     |      trainIdx\n",
      "    \n",
      "    class DenseOpticalFlow(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      DenseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(I0, I1, flow) -> flow\n",
      "     |      .   @brief Calculates an optical flow.\n",
      "     |      .   \n",
      "     |      .       @param I0 first 8-bit single-channel input image.\n",
      "     |      .       @param I1 second input image of the same size and the same type as prev.\n",
      "     |      .       @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .   @brief Releases all inner buffers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class DescriptorMatcher(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      DescriptorMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add(...)\n",
      "     |      add(descriptors) -> None\n",
      "     |      .   @brief Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor\n",
      "     |      .       collection.\n",
      "     |      .   \n",
      "     |      .       If the collection is not empty, the new descriptors are added to existing train descriptors.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n",
      "     |      .       train image.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the train descriptor collections.\n",
      "     |  \n",
      "     |  clone(...)\n",
      "     |      clone([, emptyTrainData]) -> retval\n",
      "     |      .   @brief Clones the matcher.\n",
      "     |      .   \n",
      "     |      .       @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n",
      "     |      .       that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n",
      "     |      .       object copy with the current parameters but with empty train data.\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if there are no train descriptors in the both collections.\n",
      "     |  \n",
      "     |  getTrainDescriptors(...)\n",
      "     |      getTrainDescriptors() -> retval\n",
      "     |      .   @brief Returns a constant link to the train descriptor collection trainDescCollection .\n",
      "     |  \n",
      "     |  isMaskSupported(...)\n",
      "     |      isMaskSupported() -> retval\n",
      "     |      .   @brief Returns true if the descriptor matcher supports masking permissible matches.\n",
      "     |  \n",
      "     |  knnMatch(...)\n",
      "     |      knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief Finds the k best matches for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .   \n",
      "     |      .       These extended variants of DescriptorMatcher::match methods find several best matches for each query\n",
      "     |      .       descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match\n",
      "     |      .       for the details about query and train descriptors.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      knnMatch(queryDescriptors, k[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  match(...)\n",
      "     |      match(queryDescriptors, trainDescriptors[, mask]) -> matches\n",
      "     |      .   @brief Finds the best match for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       In the first variant of this method, the train descriptors are passed as an input argument. In the\n",
      "     |      .       second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n",
      "     |      .       used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n",
      "     |      .       matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n",
      "     |      .       mask.at\\<uchar\\>(i,j) is non-zero.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      match(queryDescriptors[, masks]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |  \n",
      "     |  radiusMatch(...)\n",
      "     |      radiusMatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief For each query descriptor, finds the training descriptors not farther than the specified distance.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       For each query descriptor, the methods find such training descriptors that the distance between the\n",
      "     |      .       query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are\n",
      "     |      .       returned in the distance increasing order.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      radiusMatch(queryDescriptors, maxDistance[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train() -> None\n",
      "     |      .   @brief Trains a descriptor matcher\n",
      "     |      .   \n",
      "     |      .       Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n",
      "     |      .       train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)\n",
      "     |      .       have an empty implementation of this method. Other matchers really train their inner structures (for\n",
      "     |      .       example, FlannBasedMatcher trains flann::Index ).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create(descriptorMatcherType) -> retval\n",
      "     |      .   @brief Creates a descriptor matcher of a given type with the default parameters (using default\n",
      "     |      .       constructor).\n",
      "     |      .   \n",
      "     |      .       @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n",
      "     |      .       supported:\n",
      "     |      .       -   `BruteForce` (it uses L2 )\n",
      "     |      .       -   `BruteForce-L1`\n",
      "     |      .       -   `BruteForce-Hamming`\n",
      "     |      .       -   `BruteForce-Hamming(2)`\n",
      "     |      .       -   `FlannBased`\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      create(matcherType) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "    \n",
      "    class FarnebackOpticalFlow(DenseOpticalFlow)\n",
      "     |  Method resolution order:\n",
      "     |      FarnebackOpticalFlow\n",
      "     |      DenseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getFastPyramids(...)\n",
      "     |      getFastPyramids() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getFlags(...)\n",
      "     |      getFlags() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNumIters(...)\n",
      "     |      getNumIters() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNumLevels(...)\n",
      "     |      getNumLevels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPolyN(...)\n",
      "     |      getPolyN() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPolySigma(...)\n",
      "     |      getPolySigma() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPyrScale(...)\n",
      "     |      getPyrScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getWinSize(...)\n",
      "     |      getWinSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setFastPyramids(...)\n",
      "     |      setFastPyramids(fastPyramids) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setFlags(...)\n",
      "     |      setFlags(flags) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumIters(...)\n",
      "     |      setNumIters(numIters) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumLevels(...)\n",
      "     |      setNumLevels(numLevels) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPolyN(...)\n",
      "     |      setPolyN(polyN) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPolySigma(...)\n",
      "     |      setPolySigma(polySigma) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPyrScale(...)\n",
      "     |      setPyrScale(pyrScale) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setWinSize(...)\n",
      "     |      setWinSize(winSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, numLevels[, pyrScale[, fastPyramids[, winSize[, numIters[, polyN[, polySigma[, flags]]]]]]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DenseOpticalFlow:\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(I0, I1, flow) -> flow\n",
      "     |      .   @brief Calculates an optical flow.\n",
      "     |      .   \n",
      "     |      .       @param I0 first 8-bit single-channel input image.\n",
      "     |      .       @param I1 second input image of the same size and the same type as prev.\n",
      "     |      .       @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .   @brief Releases all inner buffers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class FastFeatureDetector(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      FastFeatureDetector\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNonmaxSuppression(...)\n",
      "     |      getNonmaxSuppression() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getType(...)\n",
      "     |      getType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNonmaxSuppression(...)\n",
      "     |      setNonmaxSuppression(f) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setType(...)\n",
      "     |      setType(type) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class Feature2D(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class FileNode(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  at(...)\n",
      "     |      at(i) -> retval\n",
      "     |      .   @overload\n",
      "     |      .        @param i Index of an element in the sequence node.\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNode(...)\n",
      "     |      getNode(nodename) -> retval\n",
      "     |      .   @overload\n",
      "     |      .        @param nodename Name of an element in the mapping node.\n",
      "     |  \n",
      "     |  isInt(...)\n",
      "     |      isInt() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isMap(...)\n",
      "     |      isMap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isNamed(...)\n",
      "     |      isNamed() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isNone(...)\n",
      "     |      isNone() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isReal(...)\n",
      "     |      isReal() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isSeq(...)\n",
      "     |      isSeq() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isString(...)\n",
      "     |      isString() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      keys() -> retval\n",
      "     |      .   @brief Returns keys of a mapping node.\n",
      "     |      .        @returns Keys of a mapping node.\n",
      "     |  \n",
      "     |  mat(...)\n",
      "     |      mat() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  name(...)\n",
      "     |      name() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  rawSize(...)\n",
      "     |      rawSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  real(...)\n",
      "     |      real() -> retval\n",
      "     |      .   Internal method used when reading FileStorage.\n",
      "     |      .        Sets the type (int, real or string) and value of the previously created node.\n",
      "     |  \n",
      "     |  size(...)\n",
      "     |      size() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  string(...)\n",
      "     |      string() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  type(...)\n",
      "     |      type() -> retval\n",
      "     |      .   @brief Returns type of the node.\n",
      "     |      .        @returns Type of the node. See FileNode::Type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class FileStorage(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getFirstTopLevelNode(...)\n",
      "     |      getFirstTopLevelNode() -> retval\n",
      "     |      .   @brief Returns the first element of the top-level mapping.\n",
      "     |      .        @returns The first element of the top-level mapping.\n",
      "     |  \n",
      "     |  getFormat(...)\n",
      "     |      getFormat() -> retval\n",
      "     |      .   @brief Returns the current format.\n",
      "     |      .        * @returns The current format, see FileStorage::Mode\n",
      "     |  \n",
      "     |  getNode(...)\n",
      "     |      getNode(nodename) -> retval\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  isOpened(...)\n",
      "     |      isOpened() -> retval\n",
      "     |      .   @brief Checks whether the file is opened.\n",
      "     |      .   \n",
      "     |      .        @returns true if the object is associated with the current file and false otherwise. It is a\n",
      "     |      .        good practice to call this method after you tried to open a file.\n",
      "     |  \n",
      "     |  open(...)\n",
      "     |      open(filename, flags[, encoding]) -> retval\n",
      "     |      .   @brief Opens a file.\n",
      "     |      .   \n",
      "     |      .        See description of parameters in FileStorage::FileStorage. The method calls FileStorage::release\n",
      "     |      .        before opening the file.\n",
      "     |      .        @param filename Name of the file to open or the text string to read the data from.\n",
      "     |      .        Extension of the file (.xml, .yml/.yaml or .json) determines its format (XML, YAML or JSON\n",
      "     |      .        respectively). Also you can append .gz to work with compressed files, for example myHugeMatrix.xml.gz. If both\n",
      "     |      .        FileStorage::WRITE and FileStorage::MEMORY flags are specified, source is used just to specify\n",
      "     |      .        the output file format (e.g. mydata.xml, .yml etc.). A file name can also contain parameters.\n",
      "     |      .        You can use this format, \"*?base64\" (e.g. \"file.json?base64\" (case sensitive)), as an alternative to\n",
      "     |      .        FileStorage::BASE64 flag.\n",
      "     |      .        @param flags Mode of operation. One of FileStorage::Mode\n",
      "     |      .        @param encoding Encoding of the file. Note that UTF-16 XML encoding is not supported currently and\n",
      "     |      .        you should use 8-bit encoding instead of it.\n",
      "     |  \n",
      "     |  release(...)\n",
      "     |      release() -> None\n",
      "     |      .   @brief Closes the file and releases all the memory buffers.\n",
      "     |      .   \n",
      "     |      .        Call this method after all I/O operations with the storage are finished.\n",
      "     |  \n",
      "     |  releaseAndGetString(...)\n",
      "     |      releaseAndGetString() -> retval\n",
      "     |      .   @brief Closes the file and releases all the memory buffers.\n",
      "     |      .   \n",
      "     |      .        Call this method after all I/O operations with the storage are finished. If the storage was\n",
      "     |      .        opened for writing data and FileStorage::WRITE was specified\n",
      "     |  \n",
      "     |  root(...)\n",
      "     |      root([, streamidx]) -> retval\n",
      "     |      .   @brief Returns the top-level mapping\n",
      "     |      .        @param streamidx Zero-based index of the stream. In most cases there is only one stream in the file.\n",
      "     |      .        However, YAML supports multiple streams and so there can be several.\n",
      "     |      .        @returns The top-level mapping.\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(name, val) -> None\n",
      "     |      .   * @brief Simplified writing API to use with bindings.\n",
      "     |      .        * @param name Name of the written object\n",
      "     |      .        * @param val Value of the written object\n",
      "     |  \n",
      "     |  writeComment(...)\n",
      "     |      writeComment(comment[, append]) -> None\n",
      "     |      .   @brief Writes a comment.\n",
      "     |      .   \n",
      "     |      .        The function writes a comment into file storage. The comments are skipped when the storage is read.\n",
      "     |      .        @param comment The written comment, single-line or multi-line\n",
      "     |      .        @param append If true, the function tries to put the comment at the end of current line.\n",
      "     |      .        Else if the comment is multi-line, or if it does not fit at the end of the current\n",
      "     |      .        line, the comment starts a new line.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class FlannBasedMatcher(DescriptorMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      FlannBasedMatcher\n",
      "     |      DescriptorMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DescriptorMatcher:\n",
      "     |  \n",
      "     |  add(...)\n",
      "     |      add(descriptors) -> None\n",
      "     |      .   @brief Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor\n",
      "     |      .       collection.\n",
      "     |      .   \n",
      "     |      .       If the collection is not empty, the new descriptors are added to existing train descriptors.\n",
      "     |      .   \n",
      "     |      .       @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same\n",
      "     |      .       train image.\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the train descriptor collections.\n",
      "     |  \n",
      "     |  clone(...)\n",
      "     |      clone([, emptyTrainData]) -> retval\n",
      "     |      .   @brief Clones the matcher.\n",
      "     |      .   \n",
      "     |      .       @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,\n",
      "     |      .       that is, copies both parameters and train data. If emptyTrainData is true, the method creates an\n",
      "     |      .       object copy with the current parameters but with empty train data.\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if there are no train descriptors in the both collections.\n",
      "     |  \n",
      "     |  getTrainDescriptors(...)\n",
      "     |      getTrainDescriptors() -> retval\n",
      "     |      .   @brief Returns a constant link to the train descriptor collection trainDescCollection .\n",
      "     |  \n",
      "     |  isMaskSupported(...)\n",
      "     |      isMaskSupported() -> retval\n",
      "     |      .   @brief Returns true if the descriptor matcher supports masking permissible matches.\n",
      "     |  \n",
      "     |  knnMatch(...)\n",
      "     |      knnMatch(queryDescriptors, trainDescriptors, k[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief Finds the k best matches for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .   \n",
      "     |      .       These extended variants of DescriptorMatcher::match methods find several best matches for each query\n",
      "     |      .       descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match\n",
      "     |      .       for the details about query and train descriptors.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      knnMatch(queryDescriptors, k[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.\n",
      "     |      .       @param k Count of best matches found per each query descriptor or less if a query descriptor has\n",
      "     |      .       less than k possible matches in total.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  match(...)\n",
      "     |      match(queryDescriptors, trainDescriptors[, mask]) -> matches\n",
      "     |      .   @brief Finds the best match for each descriptor from a query set.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       In the first variant of this method, the train descriptors are passed as an input argument. In the\n",
      "     |      .       second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is\n",
      "     |      .       used. Optional mask (or masks) can be passed to specify which query and training descriptors can be\n",
      "     |      .       matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if\n",
      "     |      .       mask.at\\<uchar\\>(i,j) is non-zero.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      match(queryDescriptors[, masks]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Matches. If a query descriptor is masked out in mask , no match is added for this\n",
      "     |      .       descriptor. So, matches size may be smaller than the query descriptors count.\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |  \n",
      "     |  radiusMatch(...)\n",
      "     |      radiusMatch(queryDescriptors, trainDescriptors, maxDistance[, mask[, compactResult]]) -> matches\n",
      "     |      .   @brief For each query descriptor, finds the training descriptors not farther than the specified distance.\n",
      "     |      .   \n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors\n",
      "     |      .       collection stored in the class object.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param mask Mask specifying permissible matches between an input query and train matrices of\n",
      "     |      .       descriptors.\n",
      "     |      .   \n",
      "     |      .       For each query descriptor, the methods find such training descriptors that the distance between the\n",
      "     |      .       query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are\n",
      "     |      .       returned in the distance increasing order.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      radiusMatch(queryDescriptors, maxDistance[, masks[, compactResult]]) -> matches\n",
      "     |      .   @overload\n",
      "     |      .       @param queryDescriptors Query set of descriptors.\n",
      "     |      .       @param matches Found matches.\n",
      "     |      .       @param maxDistance Threshold for the distance between matched descriptors. Distance means here\n",
      "     |      .       metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured\n",
      "     |      .       in Pixels)!\n",
      "     |      .       @param masks Set of masks. Each masks[i] specifies permissible matches between the input query\n",
      "     |      .       descriptors and stored train descriptors from the i-th image trainDescCollection[i].\n",
      "     |      .       @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is\n",
      "     |      .       false, the matches vector has the same size as queryDescriptors rows. If compactResult is true,\n",
      "     |      .       the matches vector does not contain matches for fully masked-out query descriptors.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train() -> None\n",
      "     |      .   @brief Trains a descriptor matcher\n",
      "     |      .   \n",
      "     |      .       Trains a descriptor matcher (for example, the flann index). In all methods to match, the method\n",
      "     |      .       train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher)\n",
      "     |      .       have an empty implementation of this method. Other matchers really train their inner structures (for\n",
      "     |      .       example, FlannBasedMatcher trains flann::Index ).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "    \n",
      "    class GFTTDetector(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      GFTTDetector\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getHarrisDetector(...)\n",
      "     |      getHarrisDetector() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getK(...)\n",
      "     |      getK() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxFeatures(...)\n",
      "     |      getMaxFeatures() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDistance(...)\n",
      "     |      getMinDistance() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getQualityLevel(...)\n",
      "     |      getQualityLevel() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(blockSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setHarrisDetector(...)\n",
      "     |      setHarrisDetector(val) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setK(...)\n",
      "     |      setK(k) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxFeatures(...)\n",
      "     |      setMaxFeatures(maxFeatures) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDistance(...)\n",
      "     |      setMinDistance(minDistance) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setQualityLevel(...)\n",
      "     |      setQualityLevel(qlevel) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, maxCorners[, qualityLevel[, minDistance[, blockSize[, useHarrisDetector[, k]]]]]]) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      create(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize[, useHarrisDetector[, k]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class GeneralizedHough(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      GeneralizedHough\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, positions[, votes]]) -> positions, votes\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(edges, dx, dy[, positions[, votes]]) -> positions, votes\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyHighThresh(...)\n",
      "     |      getCannyHighThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyLowThresh(...)\n",
      "     |      getCannyLowThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDp(...)\n",
      "     |      getDp() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxBufferSize(...)\n",
      "     |      getMaxBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDist(...)\n",
      "     |      getMinDist() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyHighThresh(...)\n",
      "     |      setCannyHighThresh(cannyHighThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyLowThresh(...)\n",
      "     |      setCannyLowThresh(cannyLowThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDp(...)\n",
      "     |      setDp(dp) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxBufferSize(...)\n",
      "     |      setMaxBufferSize(maxBufferSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDist(...)\n",
      "     |      setMinDist(minDist) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTemplate(...)\n",
      "     |      setTemplate(templ[, templCenter]) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTemplate(edges, dx, dy[, templCenter]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class GeneralizedHoughBallard(GeneralizedHough)\n",
      "     |  Method resolution order:\n",
      "     |      GeneralizedHoughBallard\n",
      "     |      GeneralizedHough\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getLevels(...)\n",
      "     |      getLevels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVotesThreshold(...)\n",
      "     |      getVotesThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setLevels(...)\n",
      "     |      setLevels(levels) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setVotesThreshold(...)\n",
      "     |      setVotesThreshold(votesThreshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedHough:\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, positions[, votes]]) -> positions, votes\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(edges, dx, dy[, positions[, votes]]) -> positions, votes\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyHighThresh(...)\n",
      "     |      getCannyHighThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyLowThresh(...)\n",
      "     |      getCannyLowThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDp(...)\n",
      "     |      getDp() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxBufferSize(...)\n",
      "     |      getMaxBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDist(...)\n",
      "     |      getMinDist() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyHighThresh(...)\n",
      "     |      setCannyHighThresh(cannyHighThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyLowThresh(...)\n",
      "     |      setCannyLowThresh(cannyLowThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDp(...)\n",
      "     |      setDp(dp) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxBufferSize(...)\n",
      "     |      setMaxBufferSize(maxBufferSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDist(...)\n",
      "     |      setMinDist(minDist) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTemplate(...)\n",
      "     |      setTemplate(templ[, templCenter]) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTemplate(edges, dx, dy[, templCenter]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class GeneralizedHoughGuil(GeneralizedHough)\n",
      "     |  Method resolution order:\n",
      "     |      GeneralizedHoughGuil\n",
      "     |      GeneralizedHough\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getAngleEpsilon(...)\n",
      "     |      getAngleEpsilon() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getAngleStep(...)\n",
      "     |      getAngleStep() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getAngleThresh(...)\n",
      "     |      getAngleThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getLevels(...)\n",
      "     |      getLevels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxAngle(...)\n",
      "     |      getMaxAngle() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxScale(...)\n",
      "     |      getMaxScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinAngle(...)\n",
      "     |      getMinAngle() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinScale(...)\n",
      "     |      getMinScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPosThresh(...)\n",
      "     |      getPosThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getScaleStep(...)\n",
      "     |      getScaleStep() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getScaleThresh(...)\n",
      "     |      getScaleThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getXi(...)\n",
      "     |      getXi() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setAngleEpsilon(...)\n",
      "     |      setAngleEpsilon(angleEpsilon) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setAngleStep(...)\n",
      "     |      setAngleStep(angleStep) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setAngleThresh(...)\n",
      "     |      setAngleThresh(angleThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setLevels(...)\n",
      "     |      setLevels(levels) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxAngle(...)\n",
      "     |      setMaxAngle(maxAngle) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxScale(...)\n",
      "     |      setMaxScale(maxScale) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinAngle(...)\n",
      "     |      setMinAngle(minAngle) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinScale(...)\n",
      "     |      setMinScale(minScale) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPosThresh(...)\n",
      "     |      setPosThresh(posThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScaleStep(...)\n",
      "     |      setScaleStep(scaleStep) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScaleThresh(...)\n",
      "     |      setScaleThresh(scaleThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setXi(...)\n",
      "     |      setXi(xi) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from GeneralizedHough:\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, positions[, votes]]) -> positions, votes\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(edges, dx, dy[, positions[, votes]]) -> positions, votes\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyHighThresh(...)\n",
      "     |      getCannyHighThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCannyLowThresh(...)\n",
      "     |      getCannyLowThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDp(...)\n",
      "     |      getDp() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxBufferSize(...)\n",
      "     |      getMaxBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDist(...)\n",
      "     |      getMinDist() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyHighThresh(...)\n",
      "     |      setCannyHighThresh(cannyHighThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCannyLowThresh(...)\n",
      "     |      setCannyLowThresh(cannyLowThresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDp(...)\n",
      "     |      setDp(dp) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxBufferSize(...)\n",
      "     |      setMaxBufferSize(maxBufferSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDist(...)\n",
      "     |      setMinDist(minDist) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTemplate(...)\n",
      "     |      setTemplate(templ[, templCenter]) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTemplate(edges, dx, dy[, templCenter]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class HOGDescriptor(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  checkDetectorSize(...)\n",
      "     |      checkDetectorSize() -> retval\n",
      "     |      .   @brief Checks if detector size equal to descriptor size.\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(img[, winStride[, padding[, locations]]]) -> descriptors\n",
      "     |      .   @brief Computes HOG descriptors of given image.\n",
      "     |      .       @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.\n",
      "     |      .       @param descriptors Matrix of the type CV_32F\n",
      "     |      .       @param winStride Window stride. It must be a multiple of block stride.\n",
      "     |      .       @param padding Padding\n",
      "     |      .       @param locations Vector of Point\n",
      "     |  \n",
      "     |  computeGradient(...)\n",
      "     |      computeGradient(img, grad, angleOfs[, paddingTL[, paddingBR]]) -> grad, angleOfs\n",
      "     |      .   @brief  Computes gradients and quantized gradient orientations.\n",
      "     |      .       @param img Matrix contains the image to be computed\n",
      "     |      .       @param grad Matrix of type CV_32FC2 contains computed gradients\n",
      "     |      .       @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations\n",
      "     |      .       @param paddingTL Padding from top-left\n",
      "     |      .       @param paddingBR Padding from bottom-right\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(img[, hitThreshold[, winStride[, padding[, searchLocations]]]]) -> foundLocations, weights\n",
      "     |      .   @brief Performs object detection without a multi-scale window.\n",
      "     |      .       @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n",
      "     |      .       @param foundLocations Vector of point where each point contains left-top corner point of detected object boundaries.\n",
      "     |      .       @param weights Vector that will contain confidence values for each detected object.\n",
      "     |      .       @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n",
      "     |      .       Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n",
      "     |      .       But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n",
      "     |      .       @param winStride Window stride. It must be a multiple of block stride.\n",
      "     |      .       @param padding Padding\n",
      "     |      .       @param searchLocations Vector of Point includes set of requested locations to be evaluated.\n",
      "     |  \n",
      "     |  detectMultiScale(...)\n",
      "     |      detectMultiScale(img[, hitThreshold[, winStride[, padding[, scale[, finalThreshold[, useMeanshiftGrouping]]]]]]) -> foundLocations, foundWeights\n",
      "     |      .   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n",
      "     |      .       of rectangles.\n",
      "     |      .       @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n",
      "     |      .       @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n",
      "     |      .       @param foundWeights Vector that will contain confidence values for each detected object.\n",
      "     |      .       @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n",
      "     |      .       Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n",
      "     |      .       But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n",
      "     |      .       @param winStride Window stride. It must be a multiple of block stride.\n",
      "     |      .       @param padding Padding\n",
      "     |      .       @param scale Coefficient of the detection window increase.\n",
      "     |      .       @param finalThreshold Final threshold\n",
      "     |      .       @param useMeanshiftGrouping indicates grouping algorithm\n",
      "     |  \n",
      "     |  getDescriptorSize(...)\n",
      "     |      getDescriptorSize() -> retval\n",
      "     |      .   @brief Returns the number of coefficients required for the classification.\n",
      "     |  \n",
      "     |  getWinSigma(...)\n",
      "     |      getWinSigma() -> retval\n",
      "     |      .   @brief Returns winSigma value\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filename[, objname]) -> retval\n",
      "     |      .   @brief loads HOGDescriptor parameters and coefficients for the linear SVM classifier from a file.\n",
      "     |      .       @param filename Path of the file to read.\n",
      "     |      .       @param objname The optional name of the node to read (if empty, the first top-level node will be used).\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename[, objname]) -> None\n",
      "     |      .   @brief saves HOGDescriptor parameters and coefficients for the linear SVM classifier to a file\n",
      "     |      .       @param filename File name\n",
      "     |      .       @param objname Object name\n",
      "     |  \n",
      "     |  setSVMDetector(...)\n",
      "     |      setSVMDetector(svmdetector) -> None\n",
      "     |      .   @brief Sets coefficients for the linear SVM classifier.\n",
      "     |      .       @param svmdetector coefficients for the linear SVM classifier.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  getDaimlerPeopleDetector(...)\n",
      "     |      getDaimlerPeopleDetector() -> retval\n",
      "     |      .   @brief Returns coefficients of the classifier trained for people detection (for 48x96 windows).\n",
      "     |  \n",
      "     |  getDefaultPeopleDetector(...)\n",
      "     |      getDefaultPeopleDetector() -> retval\n",
      "     |      .   @brief Returns coefficients of the classifier trained for people detection (for 64x128 windows).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  L2HysThreshold\n",
      "     |      L2HysThreshold\n",
      "     |  \n",
      "     |  blockSize\n",
      "     |      blockSize\n",
      "     |  \n",
      "     |  blockStride\n",
      "     |      blockStride\n",
      "     |  \n",
      "     |  cellSize\n",
      "     |      cellSize\n",
      "     |  \n",
      "     |  derivAperture\n",
      "     |      derivAperture\n",
      "     |  \n",
      "     |  gammaCorrection\n",
      "     |      gammaCorrection\n",
      "     |  \n",
      "     |  histogramNormType\n",
      "     |      histogramNormType\n",
      "     |  \n",
      "     |  nbins\n",
      "     |      nbins\n",
      "     |  \n",
      "     |  nlevels\n",
      "     |      nlevels\n",
      "     |  \n",
      "     |  signedGradient\n",
      "     |      signedGradient\n",
      "     |  \n",
      "     |  svmDetector\n",
      "     |      svmDetector\n",
      "     |  \n",
      "     |  winSigma\n",
      "     |      winSigma\n",
      "     |  \n",
      "     |  winSize\n",
      "     |      winSize\n",
      "    \n",
      "    class KAZE(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      KAZE\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDiffusivity(...)\n",
      "     |      getDiffusivity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getExtended(...)\n",
      "     |      getExtended() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNOctaveLayers(...)\n",
      "     |      getNOctaveLayers() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNOctaves(...)\n",
      "     |      getNOctaves() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getThreshold(...)\n",
      "     |      getThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getUpright(...)\n",
      "     |      getUpright() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDiffusivity(...)\n",
      "     |      setDiffusivity(diff) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setExtended(...)\n",
      "     |      setExtended(extended) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNOctaveLayers(...)\n",
      "     |      setNOctaveLayers(octaveLayers) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNOctaves(...)\n",
      "     |      setNOctaves(octaves) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setThreshold(...)\n",
      "     |      setThreshold(threshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpright(...)\n",
      "     |      setUpright(upright) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, extended[, upright[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]) -> retval\n",
      "     |      .   @brief The KAZE constructor\n",
      "     |      .   \n",
      "     |      .       @param extended Set to enable extraction of extended (128-byte) descriptor.\n",
      "     |      .       @param upright Set to enable use of upright descriptors (non rotation-invariant).\n",
      "     |      .       @param threshold Detector response threshold to accept point\n",
      "     |      .       @param nOctaves Maximum octave evolution of the image\n",
      "     |      .       @param nOctaveLayers Default number of sublevels per scale level\n",
      "     |      .       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n",
      "     |      .       DIFF_CHARBONNIER\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class KalmanFilter(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  correct(...)\n",
      "     |      correct(measurement) -> retval\n",
      "     |      .   @brief Updates the predicted state from the measurement.\n",
      "     |      .   \n",
      "     |      .       @param measurement The measured system parameters\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict([, control]) -> retval\n",
      "     |      .   @brief Computes a predicted state.\n",
      "     |      .   \n",
      "     |      .       @param control The optional input control\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  controlMatrix\n",
      "     |      controlMatrix\n",
      "     |  \n",
      "     |  errorCovPost\n",
      "     |      errorCovPost\n",
      "     |  \n",
      "     |  errorCovPre\n",
      "     |      errorCovPre\n",
      "     |  \n",
      "     |  gain\n",
      "     |      gain\n",
      "     |  \n",
      "     |  measurementMatrix\n",
      "     |      measurementMatrix\n",
      "     |  \n",
      "     |  measurementNoiseCov\n",
      "     |      measurementNoiseCov\n",
      "     |  \n",
      "     |  processNoiseCov\n",
      "     |      processNoiseCov\n",
      "     |  \n",
      "     |  statePost\n",
      "     |      statePost\n",
      "     |  \n",
      "     |  statePre\n",
      "     |      statePre\n",
      "     |  \n",
      "     |  transitionMatrix\n",
      "     |      transitionMatrix\n",
      "    \n",
      "    class KeyPoint(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  convert(...)\n",
      "     |      convert(keypoints[, keypointIndexes]) -> points2f\n",
      "     |      .   This method converts vector of keypoints to vector of points or the reverse, where each keypoint is\n",
      "     |      .       assigned the same size and the same orientation.\n",
      "     |      .   \n",
      "     |      .       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n",
      "     |      .       @param points2f Array of (x,y) coordinates of each keypoint\n",
      "     |      .       @param keypointIndexes Array of indexes of keypoints to be converted to points. (Acts like a mask to\n",
      "     |      .       convert only specified keypoints)\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      convert(points2f[, size[, response[, octave[, class_id]]]]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param points2f Array of (x,y) coordinates of each keypoint\n",
      "     |      .       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n",
      "     |      .       @param size keypoint diameter\n",
      "     |      .       @param response keypoint detector response on the keypoint (that is, strength of the keypoint)\n",
      "     |      .       @param octave pyramid octave in which the keypoint has been detected\n",
      "     |      .       @param class_id object id\n",
      "     |  \n",
      "     |  overlap(...)\n",
      "     |      overlap(kp1, kp2) -> retval\n",
      "     |      .   This method computes overlap for pair of keypoints. Overlap is the ratio between area of keypoint\n",
      "     |      .       regions' intersection and area of keypoint regions' union (considering keypoint region as circle).\n",
      "     |      .       If they don't overlap, we get zero. If they coincide at same location with same size, we get 1.\n",
      "     |      .       @param kp1 First keypoint\n",
      "     |      .       @param kp2 Second keypoint\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  angle\n",
      "     |      angle\n",
      "     |  \n",
      "     |  class_id\n",
      "     |      class_id\n",
      "     |  \n",
      "     |  octave\n",
      "     |      octave\n",
      "     |  \n",
      "     |  pt\n",
      "     |      pt\n",
      "     |  \n",
      "     |  response\n",
      "     |      response\n",
      "     |  \n",
      "     |  size\n",
      "     |      size\n",
      "    \n",
      "    class LineSegmentDetector(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      LineSegmentDetector\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  compareSegments(...)\n",
      "     |      compareSegments(size, lines1, lines2[, _image]) -> retval, _image\n",
      "     |      .   @brief Draws two groups of lines in blue and red, counting the non overlapping (mismatching) pixels.\n",
      "     |      .   \n",
      "     |      .       @param size The size of the image, where lines1 and lines2 were found.\n",
      "     |      .       @param lines1 The first group of lines that needs to be drawn. It is visualized in blue color.\n",
      "     |      .       @param lines2 The second group of lines. They visualized in red color.\n",
      "     |      .       @param _image Optional image, where the lines will be drawn. The image should be color(3-channel)\n",
      "     |      .       in order for lines1 and lines2 to be drawn in the above mentioned colors.\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(_image[, _lines[, width[, prec[, nfa]]]]) -> _lines, width, prec, nfa\n",
      "     |      .   @brief Finds lines in the input image.\n",
      "     |      .   \n",
      "     |      .       This is the output of the default parameters of the algorithm on the above shown image.\n",
      "     |      .   \n",
      "     |      .       ![image](pics/building_lsd.png)\n",
      "     |      .   \n",
      "     |      .       @param _image A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use:\n",
      "     |      .       `lsd_ptr-\\>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);`\n",
      "     |      .       @param _lines A vector of Vec4i or Vec4f elements specifying the beginning and ending point of a line. Where\n",
      "     |      .       Vec4i/Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are strictly\n",
      "     |      .       oriented depending on the gradient.\n",
      "     |      .       @param width Vector of widths of the regions, where the lines are found. E.g. Width of line.\n",
      "     |      .       @param prec Vector of precisions with which the lines are found.\n",
      "     |      .       @param nfa Vector containing number of false alarms in the line region, with precision of 10%. The\n",
      "     |      .       bigger the value, logarithmically better the detection.\n",
      "     |      .       - -1 corresponds to 10 mean false alarms\n",
      "     |      .       - 0 corresponds to 1 mean false alarm\n",
      "     |      .       - 1 corresponds to 0.1 mean false alarms\n",
      "     |      .       This vector will be calculated only when the objects type is #LSD_REFINE_ADV.\n",
      "     |  \n",
      "     |  drawSegments(...)\n",
      "     |      drawSegments(_image, lines) -> _image\n",
      "     |      .   @brief Draws the line segments on a given image.\n",
      "     |      .       @param _image The image, where the lines will be drawn. Should be bigger or equal to the image,\n",
      "     |      .       where the lines were found.\n",
      "     |      .       @param lines A vector of the lines that needed to be drawn.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class MSER(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      MSER\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  detectRegions(...)\n",
      "     |      detectRegions(image) -> msers, bboxes\n",
      "     |      .   @brief Detect %MSER regions\n",
      "     |      .   \n",
      "     |      .       @param image input image (8UC1, 8UC3 or 8UC4, must be greater or equal than 3x3)\n",
      "     |      .       @param msers resulting list of point sets\n",
      "     |      .       @param bboxes resulting bounding boxes\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDelta(...)\n",
      "     |      getDelta() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxArea(...)\n",
      "     |      getMaxArea() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinArea(...)\n",
      "     |      getMinArea() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPass2Only(...)\n",
      "     |      getPass2Only() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDelta(...)\n",
      "     |      setDelta(delta) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxArea(...)\n",
      "     |      setMaxArea(maxArea) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinArea(...)\n",
      "     |      setMinArea(minArea) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPass2Only(...)\n",
      "     |      setPass2Only(f) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, _delta[, _min_area[, _max_area[, _max_variation[, _min_diversity[, _max_evolution[, _area_threshold[, _min_margin[, _edge_blur_size]]]]]]]]]) -> retval\n",
      "     |      .   @brief Full consturctor for %MSER detector\n",
      "     |      .   \n",
      "     |      .       @param _delta it compares \\f$(size_{i}-size_{i-delta})/size_{i-delta}\\f$\n",
      "     |      .       @param _min_area prune the area which smaller than minArea\n",
      "     |      .       @param _max_area prune the area which bigger than maxArea\n",
      "     |      .       @param _max_variation prune the area have similar size to its children\n",
      "     |      .       @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity\n",
      "     |      .       @param _max_evolution  for color image, the evolution steps\n",
      "     |      .       @param _area_threshold for color image, the area threshold to cause re-initialize\n",
      "     |      .       @param _min_margin for color image, ignore too small margin\n",
      "     |      .       @param _edge_blur_size for color image, the aperture size for edge blur\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class MergeDebevec(MergeExposures)\n",
      "     |  Method resolution order:\n",
      "     |      MergeDebevec\n",
      "     |      MergeExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times, response[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      process(src, times[, dst]) -> dst\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class MergeExposures(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      MergeExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times, response[, dst]) -> dst\n",
      "     |      .   @brief Merges images.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst result image\n",
      "     |      .       @param times vector of exposure time values for each image\n",
      "     |      .       @param response 256x1 matrix with inverse camera response function for each pixel value, it should\n",
      "     |      .       have the same number of channels as images.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class MergeMertens(MergeExposures)\n",
      "     |  Method resolution order:\n",
      "     |      MergeMertens\n",
      "     |      MergeExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getContrastWeight(...)\n",
      "     |      getContrastWeight() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getExposureWeight(...)\n",
      "     |      getExposureWeight() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSaturationWeight(...)\n",
      "     |      getSaturationWeight() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times, response[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      process(src[, dst]) -> dst\n",
      "     |      .   @brief Short version of process, that doesn't take extra arguments.\n",
      "     |      .   \n",
      "     |      .       @param src vector of input images\n",
      "     |      .       @param dst result image\n",
      "     |  \n",
      "     |  setContrastWeight(...)\n",
      "     |      setContrastWeight(contrast_weiht) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setExposureWeight(...)\n",
      "     |      setExposureWeight(exposure_weight) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSaturationWeight(...)\n",
      "     |      setSaturationWeight(saturation_weight) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class MergeRobertson(MergeExposures)\n",
      "     |  Method resolution order:\n",
      "     |      MergeRobertson\n",
      "     |      MergeExposures\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src, times, response[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      process(src, times[, dst]) -> dst\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ORB(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      ORB\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getEdgeThreshold(...)\n",
      "     |      getEdgeThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getFastThreshold(...)\n",
      "     |      getFastThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getFirstLevel(...)\n",
      "     |      getFirstLevel() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxFeatures(...)\n",
      "     |      getMaxFeatures() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNLevels(...)\n",
      "     |      getNLevels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPatchSize(...)\n",
      "     |      getPatchSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getScaleFactor(...)\n",
      "     |      getScaleFactor() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getScoreType(...)\n",
      "     |      getScoreType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getWTA_K(...)\n",
      "     |      getWTA_K() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setEdgeThreshold(...)\n",
      "     |      setEdgeThreshold(edgeThreshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setFastThreshold(...)\n",
      "     |      setFastThreshold(fastThreshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setFirstLevel(...)\n",
      "     |      setFirstLevel(firstLevel) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxFeatures(...)\n",
      "     |      setMaxFeatures(maxFeatures) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNLevels(...)\n",
      "     |      setNLevels(nlevels) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPatchSize(...)\n",
      "     |      setPatchSize(patchSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScaleFactor(...)\n",
      "     |      setScaleFactor(scaleFactor) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScoreType(...)\n",
      "     |      setScoreType(scoreType) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setWTA_K(...)\n",
      "     |      setWTA_K(wta_k) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, nfeatures[, scaleFactor[, nlevels[, edgeThreshold[, firstLevel[, WTA_K[, scoreType[, patchSize[, fastThreshold]]]]]]]]]) -> retval\n",
      "     |      .   @brief The ORB constructor\n",
      "     |      .   \n",
      "     |      .       @param nfeatures The maximum number of features to retain.\n",
      "     |      .       @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical\n",
      "     |      .       pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor\n",
      "     |      .       will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor\n",
      "     |      .       will mean that to cover certain scale range you will need more pyramid levels and so the speed\n",
      "     |      .       will suffer.\n",
      "     |      .       @param nlevels The number of pyramid levels. The smallest level will have linear size equal to\n",
      "     |      .       input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).\n",
      "     |      .       @param edgeThreshold This is size of the border where the features are not detected. It should\n",
      "     |      .       roughly match the patchSize parameter.\n",
      "     |      .       @param firstLevel The level of pyramid to put source image to. Previous layers are filled\n",
      "     |      .       with upscaled source image.\n",
      "     |      .       @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The\n",
      "     |      .       default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,\n",
      "     |      .       so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3\n",
      "     |      .       random points (of course, those point coordinates are random, but they are generated from the\n",
      "     |      .       pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel\n",
      "     |      .       rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such\n",
      "     |      .       output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,\n",
      "     |      .       denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each\n",
      "     |      .       bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).\n",
      "     |      .       @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features\n",
      "     |      .       (the score is written to KeyPoint::score and is used to retain best nfeatures features);\n",
      "     |      .       FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,\n",
      "     |      .       but it is a little faster to compute.\n",
      "     |      .       @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller\n",
      "     |      .       pyramid layers the perceived image area covered by a feature will be larger.\n",
      "     |      .       @param fastThreshold the fast threshold\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class PyRotationWarper(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  buildMaps(...)\n",
      "     |      buildMaps(src_size, K, R[, xmap[, ymap]]) -> retval, xmap, ymap\n",
      "     |      .   @brief Builds the projection maps according to the given camera data.\n",
      "     |      .   \n",
      "     |      .           @param src_size Source image size\n",
      "     |      .           @param K Camera intrinsic parameters\n",
      "     |      .           @param R Camera rotation matrix\n",
      "     |      .           @param xmap Projection map for the x axis\n",
      "     |      .           @param ymap Projection map for the y axis\n",
      "     |      .           @return Projected image minimum bounding box\n",
      "     |  \n",
      "     |  getScale(...)\n",
      "     |      getScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScale(...)\n",
      "     |      setScale(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  warp(...)\n",
      "     |      warp(src, K, R, interp_mode, border_mode[, dst]) -> retval, dst\n",
      "     |      .   @brief Projects the image.\n",
      "     |      .   \n",
      "     |      .           @param src Source image\n",
      "     |      .           @param K Camera intrinsic parameters\n",
      "     |      .           @param R Camera rotation matrix\n",
      "     |      .           @param interp_mode Interpolation mode\n",
      "     |      .           @param border_mode Border extrapolation mode\n",
      "     |      .           @param dst Projected image\n",
      "     |      .           @return Project image top-left corner\n",
      "     |  \n",
      "     |  warpBackward(...)\n",
      "     |      warpBackward(src, K, R, interp_mode, border_mode, dst_size[, dst]) -> dst\n",
      "     |      .   @brief Projects the image backward.\n",
      "     |      .   \n",
      "     |      .           @param src Projected image\n",
      "     |      .           @param K Camera intrinsic parameters\n",
      "     |      .           @param R Camera rotation matrix\n",
      "     |      .           @param interp_mode Interpolation mode\n",
      "     |      .           @param border_mode Border extrapolation mode\n",
      "     |      .           @param dst_size Backward-projected image size\n",
      "     |      .           @param dst Backward-projected image\n",
      "     |  \n",
      "     |  warpPoint(...)\n",
      "     |      warpPoint(pt, K, R) -> retval\n",
      "     |      .   @brief Projects the image point.\n",
      "     |      .   \n",
      "     |      .           @param pt Source point\n",
      "     |      .           @param K Camera intrinsic parameters\n",
      "     |      .           @param R Camera rotation matrix\n",
      "     |      .           @return Projected point\n",
      "     |  \n",
      "     |  warpRoi(...)\n",
      "     |      warpRoi(src_size, K, R) -> retval\n",
      "     |      .   @param src_size Source image bounding box\n",
      "     |      .           @param K Camera intrinsic parameters\n",
      "     |      .           @param R Camera rotation matrix\n",
      "     |      .           @return Projected image minimum bounding box\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class QRCodeDetector(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  decode(...)\n",
      "     |      decode(img, points[, straight_qrcode]) -> retval, straight_qrcode\n",
      "     |      .   @brief Decodes QR code in image once it's found by the detect() method.\n",
      "     |      .        Returns UTF8-encoded output string or empty string if the code cannot be decoded.\n",
      "     |      .   \n",
      "     |      .        @param img grayscale or color (BGR) image containing QR code.\n",
      "     |      .        @param points Quadrangle vertices found by detect() method (or some other algorithm).\n",
      "     |      .        @param straight_qrcode The optional output image containing rectified and binarized QR code\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(img[, points]) -> retval, points\n",
      "     |      .   @brief Detects QR code in image and returns the quadrangle containing the code.\n",
      "     |      .        @param img grayscale or color (BGR) image containing (or not) QR code.\n",
      "     |      .        @param points Output vector of vertices of the minimum-area quadrangle containing the code.\n",
      "     |  \n",
      "     |  detectAndDecode(...)\n",
      "     |      detectAndDecode(img[, points[, straight_qrcode]]) -> retval, points, straight_qrcode\n",
      "     |      .   @brief Both detects and decodes QR code\n",
      "     |      .   \n",
      "     |      .        @param img grayscale or color (BGR) image containing QR code.\n",
      "     |      .        @param points opiotnal output array of vertices of the found QR code quadrangle. Will be empty if not found.\n",
      "     |      .        @param straight_qrcode The optional output image containing rectified and binarized QR code\n",
      "     |  \n",
      "     |  setEpsX(...)\n",
      "     |      setEpsX(epsX) -> None\n",
      "     |      .   @brief sets the epsilon used during the horizontal scan of QR code stop marker detection.\n",
      "     |      .        @param epsX Epsilon neighborhood, which allows you to determine the horizontal pattern\n",
      "     |      .        of the scheme 1:1:3:1:1 according to QR code standard.\n",
      "     |  \n",
      "     |  setEpsY(...)\n",
      "     |      setEpsY(epsY) -> None\n",
      "     |      .   @brief sets the epsilon used during the vertical scan of QR code stop marker detection.\n",
      "     |      .        @param epsY Epsilon neighborhood, which allows you to determine the vertical pattern\n",
      "     |      .        of the scheme 1:1:3:1:1 according to QR code standard.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SimpleBlobDetector(Feature2D)\n",
      "     |  Method resolution order:\n",
      "     |      SimpleBlobDetector\n",
      "     |      Feature2D\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, parameters]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Feature2D:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(image, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @brief Computes the descriptors for a set of keypoints detected in an image (first variant) or image set\n",
      "     |      .       (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      compute(images, keypoints[, descriptors]) -> keypoints, descriptors\n",
      "     |      .   @overload\n",
      "     |      .   \n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints Input collection of keypoints. Keypoints for which a descriptor cannot be\n",
      "     |      .       computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint\n",
      "     |      .       with several dominant orientations (for each orientation).\n",
      "     |      .       @param descriptors Computed descriptors. In the second variant of the method descriptors[i] are\n",
      "     |      .       descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the\n",
      "     |      .       descriptor for keypoint j-th keypoint.\n",
      "     |  \n",
      "     |  defaultNorm(...)\n",
      "     |      defaultNorm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorSize(...)\n",
      "     |      descriptorSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  descriptorType(...)\n",
      "     |      descriptorType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  detect(...)\n",
      "     |      detect(image[, mask]) -> keypoints\n",
      "     |      .   @brief Detects keypoints in an image (first variant) or image set (second variant).\n",
      "     |      .   \n",
      "     |      .       @param image Image.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param mask Mask specifying where to look for keypoints (optional). It must be a 8-bit integer\n",
      "     |      .       matrix with non-zero values in the region of interest.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      detect(images[, masks]) -> keypoints\n",
      "     |      .   @overload\n",
      "     |      .       @param images Image set.\n",
      "     |      .       @param keypoints The detected keypoints. In the second variant of the method keypoints[i] is a set\n",
      "     |      .       of keypoints detected in images[i] .\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       masks[i] is a mask for images[i].\n",
      "     |  \n",
      "     |  detectAndCompute(...)\n",
      "     |      detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "     |      .   Detects keypoints and computes the descriptors\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      read(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fileName) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .\n",
      "    \n",
      "    class SimpleBlobDetector_Params(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  blobColor\n",
      "     |      blobColor\n",
      "     |  \n",
      "     |  filterByArea\n",
      "     |      filterByArea\n",
      "     |  \n",
      "     |  filterByCircularity\n",
      "     |      filterByCircularity\n",
      "     |  \n",
      "     |  filterByColor\n",
      "     |      filterByColor\n",
      "     |  \n",
      "     |  filterByConvexity\n",
      "     |      filterByConvexity\n",
      "     |  \n",
      "     |  filterByInertia\n",
      "     |      filterByInertia\n",
      "     |  \n",
      "     |  maxArea\n",
      "     |      maxArea\n",
      "     |  \n",
      "     |  maxCircularity\n",
      "     |      maxCircularity\n",
      "     |  \n",
      "     |  maxConvexity\n",
      "     |      maxConvexity\n",
      "     |  \n",
      "     |  maxInertiaRatio\n",
      "     |      maxInertiaRatio\n",
      "     |  \n",
      "     |  maxThreshold\n",
      "     |      maxThreshold\n",
      "     |  \n",
      "     |  minArea\n",
      "     |      minArea\n",
      "     |  \n",
      "     |  minCircularity\n",
      "     |      minCircularity\n",
      "     |  \n",
      "     |  minConvexity\n",
      "     |      minConvexity\n",
      "     |  \n",
      "     |  minDistBetweenBlobs\n",
      "     |      minDistBetweenBlobs\n",
      "     |  \n",
      "     |  minInertiaRatio\n",
      "     |      minInertiaRatio\n",
      "     |  \n",
      "     |  minRepeatability\n",
      "     |      minRepeatability\n",
      "     |  \n",
      "     |  minThreshold\n",
      "     |      minThreshold\n",
      "     |  \n",
      "     |  thresholdStep\n",
      "     |      thresholdStep\n",
      "    \n",
      "    class SparseOpticalFlow(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      SparseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(prevImg, nextImg, prevPts, nextPts[, status[, err]]) -> nextPts, status, err\n",
      "     |      .   @brief Calculates a sparse optical flow.\n",
      "     |      .   \n",
      "     |      .       @param prevImg First input image.\n",
      "     |      .       @param nextImg Second input image of the same size and the same type as prevImg.\n",
      "     |      .       @param prevPts Vector of 2D points for which the flow needs to be found.\n",
      "     |      .       @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.\n",
      "     |      .       @param status Output status vector. Each element of the vector is set to 1 if the\n",
      "     |      .                     flow for the corresponding features has been found. Otherwise, it is set to 0.\n",
      "     |      .       @param err Optional output vector that contains error response for each point (inverse confidence).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class SparsePyrLKOpticalFlow(SparseOpticalFlow)\n",
      "     |  Method resolution order:\n",
      "     |      SparsePyrLKOpticalFlow\n",
      "     |      SparseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getFlags(...)\n",
      "     |      getFlags() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMaxLevel(...)\n",
      "     |      getMaxLevel() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinEigThreshold(...)\n",
      "     |      getMinEigThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getWinSize(...)\n",
      "     |      getWinSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setFlags(...)\n",
      "     |      setFlags(flags) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMaxLevel(...)\n",
      "     |      setMaxLevel(maxLevel) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinEigThreshold(...)\n",
      "     |      setMinEigThreshold(minEigThreshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(crit) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setWinSize(...)\n",
      "     |      setWinSize(winSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, winSize[, maxLevel[, crit[, flags[, minEigThreshold]]]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SparseOpticalFlow:\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(prevImg, nextImg, prevPts, nextPts[, status[, err]]) -> nextPts, status, err\n",
      "     |      .   @brief Calculates a sparse optical flow.\n",
      "     |      .   \n",
      "     |      .       @param prevImg First input image.\n",
      "     |      .       @param nextImg Second input image of the same size and the same type as prevImg.\n",
      "     |      .       @param prevPts Vector of 2D points for which the flow needs to be found.\n",
      "     |      .       @param nextPts Output vector of 2D points containing the calculated new positions of input features in the second image.\n",
      "     |      .       @param status Output status vector. Each element of the vector is set to 1 if the\n",
      "     |      .                     flow for the corresponding features has been found. Otherwise, it is set to 0.\n",
      "     |      .       @param err Optional output vector that contains error response for each point (inverse confidence).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class StereoBM(StereoMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      StereoBM\n",
      "     |      StereoMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getPreFilterCap(...)\n",
      "     |      getPreFilterCap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPreFilterSize(...)\n",
      "     |      getPreFilterSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPreFilterType(...)\n",
      "     |      getPreFilterType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getROI1(...)\n",
      "     |      getROI1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getROI2(...)\n",
      "     |      getROI2() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSmallerBlockSize(...)\n",
      "     |      getSmallerBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTextureThreshold(...)\n",
      "     |      getTextureThreshold() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getUniquenessRatio(...)\n",
      "     |      getUniquenessRatio() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPreFilterCap(...)\n",
      "     |      setPreFilterCap(preFilterCap) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPreFilterSize(...)\n",
      "     |      setPreFilterSize(preFilterSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPreFilterType(...)\n",
      "     |      setPreFilterType(preFilterType) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setROI1(...)\n",
      "     |      setROI1(roi1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setROI2(...)\n",
      "     |      setROI2(roi2) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSmallerBlockSize(...)\n",
      "     |      setSmallerBlockSize(blockSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTextureThreshold(...)\n",
      "     |      setTextureThreshold(textureThreshold) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUniquenessRatio(...)\n",
      "     |      setUniquenessRatio(uniquenessRatio) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, numDisparities[, blockSize]]) -> retval\n",
      "     |      .   @brief Creates StereoBM object\n",
      "     |      .   \n",
      "     |      .       @param numDisparities the disparity search range. For each pixel algorithm will find the best\n",
      "     |      .       disparity from 0 (default minimum disparity) to numDisparities. The search range can then be\n",
      "     |      .       shifted by changing the minimum disparity.\n",
      "     |      .       @param blockSize the linear size of the blocks compared by the algorithm. The size should be odd\n",
      "     |      .       (as the block is centered at the current pixel). Larger block size implies smoother, though less\n",
      "     |      .       accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher\n",
      "     |      .       chance for algorithm to find a wrong correspondence.\n",
      "     |      .   \n",
      "     |      .       The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for\n",
      "     |      .       a specific stereo pair.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from StereoMatcher:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(left, right[, disparity]) -> disparity\n",
      "     |      .   @brief Computes disparity map for the specified stereo pair\n",
      "     |      .   \n",
      "     |      .       @param left Left 8-bit single-channel image.\n",
      "     |      .       @param right Right image of the same size and the same type as the left one.\n",
      "     |      .       @param disparity Output disparity map. It has the same size as the input images. Some algorithms,\n",
      "     |      .       like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value\n",
      "     |      .       has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map.\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDisp12MaxDiff(...)\n",
      "     |      getDisp12MaxDiff() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDisparity(...)\n",
      "     |      getMinDisparity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNumDisparities(...)\n",
      "     |      getNumDisparities() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleRange(...)\n",
      "     |      getSpeckleRange() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleWindowSize(...)\n",
      "     |      getSpeckleWindowSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(blockSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDisp12MaxDiff(...)\n",
      "     |      setDisp12MaxDiff(disp12MaxDiff) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDisparity(...)\n",
      "     |      setMinDisparity(minDisparity) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumDisparities(...)\n",
      "     |      setNumDisparities(numDisparities) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleRange(...)\n",
      "     |      setSpeckleRange(speckleRange) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleWindowSize(...)\n",
      "     |      setSpeckleWindowSize(speckleWindowSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class StereoMatcher(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      StereoMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(left, right[, disparity]) -> disparity\n",
      "     |      .   @brief Computes disparity map for the specified stereo pair\n",
      "     |      .   \n",
      "     |      .       @param left Left 8-bit single-channel image.\n",
      "     |      .       @param right Right image of the same size and the same type as the left one.\n",
      "     |      .       @param disparity Output disparity map. It has the same size as the input images. Some algorithms,\n",
      "     |      .       like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value\n",
      "     |      .       has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map.\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDisp12MaxDiff(...)\n",
      "     |      getDisp12MaxDiff() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDisparity(...)\n",
      "     |      getMinDisparity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNumDisparities(...)\n",
      "     |      getNumDisparities() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleRange(...)\n",
      "     |      getSpeckleRange() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleWindowSize(...)\n",
      "     |      getSpeckleWindowSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(blockSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDisp12MaxDiff(...)\n",
      "     |      setDisp12MaxDiff(disp12MaxDiff) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDisparity(...)\n",
      "     |      setMinDisparity(minDisparity) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumDisparities(...)\n",
      "     |      setNumDisparities(numDisparities) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleRange(...)\n",
      "     |      setSpeckleRange(speckleRange) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleWindowSize(...)\n",
      "     |      setSpeckleWindowSize(speckleWindowSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class StereoSGBM(StereoMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      StereoSGBM\n",
      "     |      StereoMatcher\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getMode(...)\n",
      "     |      getMode() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getP1(...)\n",
      "     |      getP1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getP2(...)\n",
      "     |      getP2() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getPreFilterCap(...)\n",
      "     |      getPreFilterCap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getUniquenessRatio(...)\n",
      "     |      getUniquenessRatio() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMode(...)\n",
      "     |      setMode(mode) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setP1(...)\n",
      "     |      setP1(P1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setP2(...)\n",
      "     |      setP2(P2) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPreFilterCap(...)\n",
      "     |      setPreFilterCap(preFilterCap) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUniquenessRatio(...)\n",
      "     |      setUniquenessRatio(uniquenessRatio) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, minDisparity[, numDisparities[, blockSize[, P1[, P2[, disp12MaxDiff[, preFilterCap[, uniquenessRatio[, speckleWindowSize[, speckleRange[, mode]]]]]]]]]]]) -> retval\n",
      "     |      .   @brief Creates StereoSGBM object\n",
      "     |      .   \n",
      "     |      .       @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes\n",
      "     |      .       rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.\n",
      "     |      .       @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than\n",
      "     |      .       zero. In the current implementation, this parameter must be divisible by 16.\n",
      "     |      .       @param blockSize Matched block size. It must be an odd number \\>=1 . Normally, it should be\n",
      "     |      .       somewhere in the 3..11 range.\n",
      "     |      .       @param P1 The first parameter controlling the disparity smoothness. See below.\n",
      "     |      .       @param P2 The second parameter controlling the disparity smoothness. The larger the values are,\n",
      "     |      .       the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1\n",
      "     |      .       between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor\n",
      "     |      .       pixels. The algorithm requires P2 \\> P1 . See stereo_match.cpp sample where some reasonably good\n",
      "     |      .       P1 and P2 values are shown (like 8\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize and\n",
      "     |      .       32\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize , respectively).\n",
      "     |      .       @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right\n",
      "     |      .       disparity check. Set it to a non-positive value to disable the check.\n",
      "     |      .       @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first\n",
      "     |      .       computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.\n",
      "     |      .       The result values are passed to the Birchfield-Tomasi pixel cost function.\n",
      "     |      .       @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function\n",
      "     |      .       value should \"win\" the second best value to consider the found match correct. Normally, a value\n",
      "     |      .       within the 5-15 range is good enough.\n",
      "     |      .       @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles\n",
      "     |      .       and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the\n",
      "     |      .       50-200 range.\n",
      "     |      .       @param speckleRange Maximum disparity variation within each connected component. If you do speckle\n",
      "     |      .       filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.\n",
      "     |      .       Normally, 1 or 2 is good enough.\n",
      "     |      .       @param mode Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming\n",
      "     |      .       algorithm. It will consume O(W\\*H\\*numDisparities) bytes, which is large for 640x480 stereo and\n",
      "     |      .       huge for HD-size pictures. By default, it is set to false .\n",
      "     |      .   \n",
      "     |      .       The first constructor initializes StereoSGBM with all the default parameters. So, you only have to\n",
      "     |      .       set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter\n",
      "     |      .       to a custom value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from StereoMatcher:\n",
      "     |  \n",
      "     |  compute(...)\n",
      "     |      compute(left, right[, disparity]) -> disparity\n",
      "     |      .   @brief Computes disparity map for the specified stereo pair\n",
      "     |      .   \n",
      "     |      .       @param left Left 8-bit single-channel image.\n",
      "     |      .       @param right Right image of the same size and the same type as the left one.\n",
      "     |      .       @param disparity Output disparity map. It has the same size as the input images. Some algorithms,\n",
      "     |      .       like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value\n",
      "     |      .       has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map.\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDisp12MaxDiff(...)\n",
      "     |      getDisp12MaxDiff() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMinDisparity(...)\n",
      "     |      getMinDisparity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNumDisparities(...)\n",
      "     |      getNumDisparities() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleRange(...)\n",
      "     |      getSpeckleRange() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSpeckleWindowSize(...)\n",
      "     |      getSpeckleWindowSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(blockSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDisp12MaxDiff(...)\n",
      "     |      setDisp12MaxDiff(disp12MaxDiff) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMinDisparity(...)\n",
      "     |      setMinDisparity(minDisparity) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumDisparities(...)\n",
      "     |      setNumDisparities(numDisparities) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleRange(...)\n",
      "     |      setSpeckleRange(speckleRange) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSpeckleWindowSize(...)\n",
      "     |      setSpeckleWindowSize(speckleWindowSize) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class Stitcher(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  composePanorama(...)\n",
      "     |      composePanorama([, pano]) -> retval, pano\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  compositingResol(...)\n",
      "     |      compositingResol() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  estimateTransform(...)\n",
      "     |      estimateTransform(images[, masks]) -> retval\n",
      "     |      .   @brief These functions try to match the given images and to estimate rotations of each camera.\n",
      "     |      .   \n",
      "     |      .       @note Use the functions only if you're aware of the stitching pipeline, otherwise use\n",
      "     |      .       Stitcher::stitch.\n",
      "     |      .   \n",
      "     |      .       @param images Input images.\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       @return Status code.\n",
      "     |  \n",
      "     |  interpolationFlags(...)\n",
      "     |      interpolationFlags() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  panoConfidenceThresh(...)\n",
      "     |      panoConfidenceThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  registrationResol(...)\n",
      "     |      registrationResol() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  seamEstimationResol(...)\n",
      "     |      seamEstimationResol() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setCompositingResol(...)\n",
      "     |      setCompositingResol(resol_mpx) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setInterpolationFlags(...)\n",
      "     |      setInterpolationFlags(interp_flags) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setPanoConfidenceThresh(...)\n",
      "     |      setPanoConfidenceThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRegistrationResol(...)\n",
      "     |      setRegistrationResol(resol_mpx) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSeamEstimationResol(...)\n",
      "     |      setSeamEstimationResol(resol_mpx) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setWaveCorrection(...)\n",
      "     |      setWaveCorrection(flag) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  stitch(...)\n",
      "     |      stitch(images[, pano]) -> retval, pano\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      stitch(images, masks[, pano]) -> retval, pano\n",
      "     |      .   @brief These functions try to stitch the given images.\n",
      "     |      .   \n",
      "     |      .       @param images Input images.\n",
      "     |      .       @param masks Masks for each input image specifying where to look for keypoints (optional).\n",
      "     |      .       @param pano Final pano.\n",
      "     |      .       @return Status code.\n",
      "     |  \n",
      "     |  waveCorrection(...)\n",
      "     |      waveCorrection() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  workScale(...)\n",
      "     |      workScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, mode]) -> retval\n",
      "     |      .   @brief Creates a Stitcher configured in one of the stitching modes.\n",
      "     |      .   \n",
      "     |      .       @param mode Scenario for stitcher operation. This is usually determined by source of images\n",
      "     |      .       to stitch and their transformation. Default parameters will be chosen for operation in given\n",
      "     |      .       scenario.\n",
      "     |      .       @return Stitcher class instance.\n",
      "    \n",
      "    class Subdiv2D(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  edgeDst(...)\n",
      "     |      edgeDst(edge) -> retval, dstpt\n",
      "     |      .   @brief Returns the edge destination.\n",
      "     |      .   \n",
      "     |      .       @param edge Subdivision edge ID.\n",
      "     |      .       @param dstpt Output vertex location.\n",
      "     |      .   \n",
      "     |      .       @returns vertex ID.\n",
      "     |  \n",
      "     |  edgeOrg(...)\n",
      "     |      edgeOrg(edge) -> retval, orgpt\n",
      "     |      .   @brief Returns the edge origin.\n",
      "     |      .   \n",
      "     |      .       @param edge Subdivision edge ID.\n",
      "     |      .       @param orgpt Output vertex location.\n",
      "     |      .   \n",
      "     |      .       @returns vertex ID.\n",
      "     |  \n",
      "     |  findNearest(...)\n",
      "     |      findNearest(pt) -> retval, nearestPt\n",
      "     |      .   @brief Finds the subdivision vertex closest to the given point.\n",
      "     |      .   \n",
      "     |      .       @param pt Input point.\n",
      "     |      .       @param nearestPt Output subdivision vertex point.\n",
      "     |      .   \n",
      "     |      .       The function is another function that locates the input point within the subdivision. It finds the\n",
      "     |      .       subdivision vertex that is the closest to the input point. It is not necessarily one of vertices\n",
      "     |      .       of the facet containing the input point, though the facet (located using locate() ) is used as a\n",
      "     |      .       starting point.\n",
      "     |      .   \n",
      "     |      .       @returns vertex ID.\n",
      "     |  \n",
      "     |  getEdge(...)\n",
      "     |      getEdge(edge, nextEdgeType) -> retval\n",
      "     |      .   @brief Returns one of the edges related to the given edge.\n",
      "     |      .   \n",
      "     |      .       @param edge Subdivision edge ID.\n",
      "     |      .       @param nextEdgeType Parameter specifying which of the related edges to return.\n",
      "     |      .       The following values are possible:\n",
      "     |      .       -   NEXT_AROUND_ORG next around the edge origin ( eOnext on the picture below if e is the input edge)\n",
      "     |      .       -   NEXT_AROUND_DST next around the edge vertex ( eDnext )\n",
      "     |      .       -   PREV_AROUND_ORG previous around the edge origin (reversed eRnext )\n",
      "     |      .       -   PREV_AROUND_DST previous around the edge destination (reversed eLnext )\n",
      "     |      .       -   NEXT_AROUND_LEFT next around the left facet ( eLnext )\n",
      "     |      .       -   NEXT_AROUND_RIGHT next around the right facet ( eRnext )\n",
      "     |      .       -   PREV_AROUND_LEFT previous around the left facet (reversed eOnext )\n",
      "     |      .       -   PREV_AROUND_RIGHT previous around the right facet (reversed eDnext )\n",
      "     |      .   \n",
      "     |      .       ![sample output](pics/quadedge.png)\n",
      "     |      .   \n",
      "     |      .       @returns edge ID related to the input edge.\n",
      "     |  \n",
      "     |  getEdgeList(...)\n",
      "     |      getEdgeList() -> edgeList\n",
      "     |      .   @brief Returns a list of all edges.\n",
      "     |      .   \n",
      "     |      .       @param edgeList Output vector.\n",
      "     |      .   \n",
      "     |      .       The function gives each edge as a 4 numbers vector, where each two are one of the edge\n",
      "     |      .       vertices. i.e. org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3].\n",
      "     |  \n",
      "     |  getLeadingEdgeList(...)\n",
      "     |      getLeadingEdgeList() -> leadingEdgeList\n",
      "     |      .   @brief Returns a list of the leading edge ID connected to each triangle.\n",
      "     |      .   \n",
      "     |      .       @param leadingEdgeList Output vector.\n",
      "     |      .   \n",
      "     |      .       The function gives one edge ID for each triangle.\n",
      "     |  \n",
      "     |  getTriangleList(...)\n",
      "     |      getTriangleList() -> triangleList\n",
      "     |      .   @brief Returns a list of all triangles.\n",
      "     |      .   \n",
      "     |      .       @param triangleList Output vector.\n",
      "     |      .   \n",
      "     |      .       The function gives each triangle as a 6 numbers vector, where each two are one of the triangle\n",
      "     |      .       vertices. i.e. p1_x = v[0], p1_y = v[1], p2_x = v[2], p2_y = v[3], p3_x = v[4], p3_y = v[5].\n",
      "     |  \n",
      "     |  getVertex(...)\n",
      "     |      getVertex(vertex) -> retval, firstEdge\n",
      "     |      .   @brief Returns vertex location from vertex ID.\n",
      "     |      .   \n",
      "     |      .       @param vertex vertex ID.\n",
      "     |      .       @param firstEdge Optional. The first edge ID which is connected to the vertex.\n",
      "     |      .       @returns vertex (x,y)\n",
      "     |  \n",
      "     |  getVoronoiFacetList(...)\n",
      "     |      getVoronoiFacetList(idx) -> facetList, facetCenters\n",
      "     |      .   @brief Returns a list of all Voroni facets.\n",
      "     |      .   \n",
      "     |      .       @param idx Vector of vertices IDs to consider. For all vertices you can pass empty vector.\n",
      "     |      .       @param facetList Output vector of the Voroni facets.\n",
      "     |      .       @param facetCenters Output vector of the Voroni facets center points.\n",
      "     |  \n",
      "     |  initDelaunay(...)\n",
      "     |      initDelaunay(rect) -> None\n",
      "     |      .   @brief Creates a new empty Delaunay subdivision\n",
      "     |      .   \n",
      "     |      .       @param rect Rectangle that includes all of the 2D points that are to be added to the subdivision.\n",
      "     |  \n",
      "     |  insert(...)\n",
      "     |      insert(pt) -> retval\n",
      "     |      .   @brief Insert a single point into a Delaunay triangulation.\n",
      "     |      .   \n",
      "     |      .       @param pt Point to insert.\n",
      "     |      .   \n",
      "     |      .       The function inserts a single point into a subdivision and modifies the subdivision topology\n",
      "     |      .       appropriately. If a point with the same coordinates exists already, no new point is added.\n",
      "     |      .       @returns the ID of the point.\n",
      "     |      .   \n",
      "     |      .       @note If the point is outside of the triangulation specified rect a runtime error is raised.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      insert(ptvec) -> None\n",
      "     |      .   @brief Insert multiple points into a Delaunay triangulation.\n",
      "     |      .   \n",
      "     |      .       @param ptvec Points to insert.\n",
      "     |      .   \n",
      "     |      .       The function inserts a vector of points into a subdivision and modifies the subdivision topology\n",
      "     |      .       appropriately.\n",
      "     |  \n",
      "     |  locate(...)\n",
      "     |      locate(pt) -> retval, edge, vertex\n",
      "     |      .   @brief Returns the location of a point within a Delaunay triangulation.\n",
      "     |      .   \n",
      "     |      .       @param pt Point to locate.\n",
      "     |      .       @param edge Output edge that the point belongs to or is located to the right of it.\n",
      "     |      .       @param vertex Optional output vertex the input point coincides with.\n",
      "     |      .   \n",
      "     |      .       The function locates the input point within the subdivision and gives one of the triangle edges\n",
      "     |      .       or vertices.\n",
      "     |      .   \n",
      "     |      .       @returns an integer which specify one of the following five cases for point location:\n",
      "     |      .       -  The point falls into some facet. The function returns #PTLOC_INSIDE and edge will contain one of\n",
      "     |      .          edges of the facet.\n",
      "     |      .       -  The point falls onto the edge. The function returns #PTLOC_ON_EDGE and edge will contain this edge.\n",
      "     |      .       -  The point coincides with one of the subdivision vertices. The function returns #PTLOC_VERTEX and\n",
      "     |      .          vertex will contain a pointer to the vertex.\n",
      "     |      .       -  The point is outside the subdivision reference rectangle. The function returns #PTLOC_OUTSIDE_RECT\n",
      "     |      .          and no pointers are filled.\n",
      "     |      .       -  One of input arguments is invalid. A runtime error is raised or, if silent or \"parent\" error\n",
      "     |      .          processing mode is selected, #PTLOC_ERROR is returned.\n",
      "     |  \n",
      "     |  nextEdge(...)\n",
      "     |      nextEdge(edge) -> retval\n",
      "     |      .   @brief Returns next edge around the edge origin.\n",
      "     |      .   \n",
      "     |      .       @param edge Subdivision edge ID.\n",
      "     |      .   \n",
      "     |      .       @returns an integer which is next edge ID around the edge origin: eOnext on the\n",
      "     |      .       picture above if e is the input edge).\n",
      "     |  \n",
      "     |  rotateEdge(...)\n",
      "     |      rotateEdge(edge, rotate) -> retval\n",
      "     |      .   @brief Returns another edge of the same quad-edge.\n",
      "     |      .   \n",
      "     |      .       @param edge Subdivision edge ID.\n",
      "     |      .       @param rotate Parameter specifying which of the edges of the same quad-edge as the input\n",
      "     |      .       one to return. The following values are possible:\n",
      "     |      .       -   0 - the input edge ( e on the picture below if e is the input edge)\n",
      "     |      .       -   1 - the rotated edge ( eRot )\n",
      "     |      .       -   2 - the reversed edge (reversed e (in green))\n",
      "     |      .       -   3 - the reversed rotated edge (reversed eRot (in green))\n",
      "     |      .   \n",
      "     |      .       @returns one of the edges ID of the same quad-edge as the input edge.\n",
      "     |  \n",
      "     |  symEdge(...)\n",
      "     |      symEdge(edge) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class TickMeter(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getCounter(...)\n",
      "     |      getCounter() -> retval\n",
      "     |      .   returns internal counter value.\n",
      "     |  \n",
      "     |  getTimeMicro(...)\n",
      "     |      getTimeMicro() -> retval\n",
      "     |      .   returns passed time in microseconds.\n",
      "     |  \n",
      "     |  getTimeMilli(...)\n",
      "     |      getTimeMilli() -> retval\n",
      "     |      .   returns passed time in milliseconds.\n",
      "     |  \n",
      "     |  getTimeSec(...)\n",
      "     |      getTimeSec() -> retval\n",
      "     |      .   returns passed time in seconds.\n",
      "     |  \n",
      "     |  getTimeTicks(...)\n",
      "     |      getTimeTicks() -> retval\n",
      "     |      .   returns counted ticks.\n",
      "     |  \n",
      "     |  reset(...)\n",
      "     |      reset() -> None\n",
      "     |      .   resets internal values.\n",
      "     |  \n",
      "     |  start(...)\n",
      "     |      start() -> None\n",
      "     |      .   starts counting ticks.\n",
      "     |  \n",
      "     |  stop(...)\n",
      "     |      stop() -> None\n",
      "     |      .   stops counting ticks.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Tonemap(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      Tonemap\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src[, dst]) -> dst\n",
      "     |      .   @brief Tonemaps image\n",
      "     |      .   \n",
      "     |      .       @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)\n",
      "     |      .       @param dst destination image - CV_32FC3 Mat with values in [0, 1] range\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(gamma) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class TonemapDrago(Tonemap)\n",
      "     |  Method resolution order:\n",
      "     |      TonemapDrago\n",
      "     |      Tonemap\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getBias(...)\n",
      "     |      getBias() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSaturation(...)\n",
      "     |      getSaturation() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBias(...)\n",
      "     |      setBias(bias) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSaturation(...)\n",
      "     |      setSaturation(saturation) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Tonemap:\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src[, dst]) -> dst\n",
      "     |      .   @brief Tonemaps image\n",
      "     |      .   \n",
      "     |      .       @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)\n",
      "     |      .       @param dst destination image - CV_32FC3 Mat with values in [0, 1] range\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(gamma) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class TonemapMantiuk(Tonemap)\n",
      "     |  Method resolution order:\n",
      "     |      TonemapMantiuk\n",
      "     |      Tonemap\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getSaturation(...)\n",
      "     |      getSaturation() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getScale(...)\n",
      "     |      getScale() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSaturation(...)\n",
      "     |      setSaturation(saturation) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setScale(...)\n",
      "     |      setScale(scale) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Tonemap:\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src[, dst]) -> dst\n",
      "     |      .   @brief Tonemaps image\n",
      "     |      .   \n",
      "     |      .       @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)\n",
      "     |      .       @param dst destination image - CV_32FC3 Mat with values in [0, 1] range\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(gamma) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class TonemapReinhard(Tonemap)\n",
      "     |  Method resolution order:\n",
      "     |      TonemapReinhard\n",
      "     |      Tonemap\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getColorAdaptation(...)\n",
      "     |      getColorAdaptation() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getIntensity(...)\n",
      "     |      getIntensity() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getLightAdaptation(...)\n",
      "     |      getLightAdaptation() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setColorAdaptation(...)\n",
      "     |      setColorAdaptation(color_adapt) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setIntensity(...)\n",
      "     |      setIntensity(intensity) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setLightAdaptation(...)\n",
      "     |      setLightAdaptation(light_adapt) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Tonemap:\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(src[, dst]) -> dst\n",
      "     |      .   @brief Tonemaps image\n",
      "     |      .   \n",
      "     |      .       @param src source image - CV_32FC3 Mat (float 32 bits 3 channels)\n",
      "     |      .       @param dst destination image - CV_32FC3 Mat with values in [0, 1] range\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(gamma) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class UMat(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      get() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  handle(...)\n",
      "     |      handle(accessFlags) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isContinuous(...)\n",
      "     |      isContinuous() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isSubmatrix(...)\n",
      "     |      isSubmatrix() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  context(...)\n",
      "     |      context() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  queue(...)\n",
      "     |      queue() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  offset\n",
      "     |      offset\n",
      "    \n",
      "    class VariationalRefinement(DenseOpticalFlow)\n",
      "     |  Method resolution order:\n",
      "     |      VariationalRefinement\n",
      "     |      DenseOpticalFlow\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  calcUV(...)\n",
      "     |      calcUV(I0, I1, flow_u, flow_v) -> flow_u, flow_v\n",
      "     |      .   @brief @ref calc function overload to handle separate horizontal (u) and vertical (v) flow components\n",
      "     |      .   (to avoid extra splits/merges)\n",
      "     |  \n",
      "     |  getAlpha(...)\n",
      "     |      getAlpha() -> retval\n",
      "     |      .   @brief Weight of the smoothness term\n",
      "     |      .   @see setAlpha\n",
      "     |  \n",
      "     |  getDelta(...)\n",
      "     |      getDelta() -> retval\n",
      "     |      .   @brief Weight of the color constancy term\n",
      "     |      .   @see setDelta\n",
      "     |  \n",
      "     |  getFixedPointIterations(...)\n",
      "     |      getFixedPointIterations() -> retval\n",
      "     |      .   @brief Number of outer (fixed-point) iterations in the minimization procedure.\n",
      "     |      .   @see setFixedPointIterations\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .   @brief Weight of the gradient constancy term\n",
      "     |      .   @see setGamma\n",
      "     |  \n",
      "     |  getOmega(...)\n",
      "     |      getOmega() -> retval\n",
      "     |      .   @brief Relaxation factor in SOR\n",
      "     |      .   @see setOmega\n",
      "     |  \n",
      "     |  getSorIterations(...)\n",
      "     |      getSorIterations() -> retval\n",
      "     |      .   @brief Number of inner successive over-relaxation (SOR) iterations\n",
      "     |      .           in the minimization procedure to solve the respective linear system.\n",
      "     |      .   @see setSorIterations\n",
      "     |  \n",
      "     |  setAlpha(...)\n",
      "     |      setAlpha(val) -> None\n",
      "     |      .   @copybrief getAlpha @see getAlpha\n",
      "     |  \n",
      "     |  setDelta(...)\n",
      "     |      setDelta(val) -> None\n",
      "     |      .   @copybrief getDelta @see getDelta\n",
      "     |  \n",
      "     |  setFixedPointIterations(...)\n",
      "     |      setFixedPointIterations(val) -> None\n",
      "     |      .   @copybrief getFixedPointIterations @see getFixedPointIterations\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(val) -> None\n",
      "     |      .   @copybrief getGamma @see getGamma\n",
      "     |  \n",
      "     |  setOmega(...)\n",
      "     |      setOmega(val) -> None\n",
      "     |      .   @copybrief getOmega @see getOmega\n",
      "     |  \n",
      "     |  setSorIterations(...)\n",
      "     |      setSorIterations(val) -> None\n",
      "     |      .   @copybrief getSorIterations @see getSorIterations\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates an instance of VariationalRefinement\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DenseOpticalFlow:\n",
      "     |  \n",
      "     |  calc(...)\n",
      "     |      calc(I0, I1, flow) -> flow\n",
      "     |      .   @brief Calculates an optical flow.\n",
      "     |      .   \n",
      "     |      .       @param I0 first 8-bit single-channel input image.\n",
      "     |      .       @param I1 second input image of the same size and the same type as prev.\n",
      "     |      .       @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .   @brief Releases all inner buffers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class VideoCapture(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      get(propId) -> retval\n",
      "     |      .   @brief Returns the specified VideoCapture property\n",
      "     |      .   \n",
      "     |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      "     |      .       or one from @ref videoio_flags_others\n",
      "     |      .       @return Value for the specified property. Value 0 is returned when querying a property that is\n",
      "     |      .       not supported by the backend used by the VideoCapture instance.\n",
      "     |      .   \n",
      "     |      .       @note Reading / writing properties involves many layers. Some unexpected result might happens\n",
      "     |      .       along this chain.\n",
      "     |      .       @code{.txt}\n",
      "     |      .       VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware\n",
      "     |      .       @endcode\n",
      "     |      .       The returned value might be different from what really used by the device or it could be encoded\n",
      "     |      .       using device dependent rules (eg. steps or percentage). Effective behaviour depends from device\n",
      "     |      .       driver and API Backend\n",
      "     |  \n",
      "     |  getBackendName(...)\n",
      "     |      getBackendName() -> retval\n",
      "     |      .   @brief Returns used backend API name\n",
      "     |      .   \n",
      "     |      .        @note Stream should be opened.\n",
      "     |  \n",
      "     |  getExceptionMode(...)\n",
      "     |      getExceptionMode() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  grab(...)\n",
      "     |      grab() -> retval\n",
      "     |      .   @brief Grabs the next frame from video file or capturing device.\n",
      "     |      .   \n",
      "     |      .       @return `true` (non-zero) in the case of success.\n",
      "     |      .   \n",
      "     |      .       The method/function grabs the next frame from video file or camera and returns true (non-zero) in\n",
      "     |      .       the case of success.\n",
      "     |      .   \n",
      "     |      .       The primary use of the function is in multi-camera environments, especially when the cameras do not\n",
      "     |      .       have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that\n",
      "     |      .       call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way\n",
      "     |      .       the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames\n",
      "     |      .       from different cameras will be closer in time.\n",
      "     |      .   \n",
      "     |      .       Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the\n",
      "     |      .       correct way of retrieving data from it is to call VideoCapture::grab() first and then call\n",
      "     |      .       VideoCapture::retrieve() one or more times with different values of the channel parameter.\n",
      "     |      .   \n",
      "     |      .       @ref tutorial_kinect_openni\n",
      "     |  \n",
      "     |  isOpened(...)\n",
      "     |      isOpened() -> retval\n",
      "     |      .   @brief Returns true if video capturing has been initialized already.\n",
      "     |      .   \n",
      "     |      .       If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns\n",
      "     |      .       true.\n",
      "     |  \n",
      "     |  open(...)\n",
      "     |      open(filename[, apiPreference]) -> retval\n",
      "     |      .   @brief  Opens a video file or a capturing device or an IP video stream for video capturing.\n",
      "     |      .   \n",
      "     |      .       @overload\n",
      "     |      .   \n",
      "     |      .       Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference = CAP_ANY)\n",
      "     |      .       @return `true` if the file has been successfully opened\n",
      "     |      .   \n",
      "     |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      open(index[, apiPreference]) -> retval\n",
      "     |      .   @brief  Opens a camera for video capturing\n",
      "     |      .   \n",
      "     |      .       @overload\n",
      "     |      .   \n",
      "     |      .       Parameters are same as the constructor VideoCapture(int index, int apiPreference = CAP_ANY)\n",
      "     |      .       @return `true` if the camera has been successfully opened.\n",
      "     |      .   \n",
      "     |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read([, image]) -> retval, image\n",
      "     |      .   @brief Grabs, decodes and returns the next video frame.\n",
      "     |      .   \n",
      "     |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      "     |      .       @return `false` if no frames has been grabbed\n",
      "     |      .   \n",
      "     |      .       The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the\n",
      "     |      .       most convenient method for reading video files or capturing data from decode and returns the just\n",
      "     |      .       grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more\n",
      "     |      .       frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).\n",
      "     |      .   \n",
      "     |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      "     |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      "     |      .       cvCloneImage and then do whatever you want with the copy.\n",
      "     |  \n",
      "     |  release(...)\n",
      "     |      release() -> None\n",
      "     |      .   @brief Closes video file or capturing device.\n",
      "     |      .   \n",
      "     |      .       The method is automatically called by subsequent VideoCapture::open and by VideoCapture\n",
      "     |      .       destructor.\n",
      "     |      .   \n",
      "     |      .       The C function also deallocates memory and clears \\*capture pointer.\n",
      "     |  \n",
      "     |  retrieve(...)\n",
      "     |      retrieve([, image[, flag]]) -> retval, image\n",
      "     |      .   @brief Decodes and returns the grabbed video frame.\n",
      "     |      .   \n",
      "     |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      "     |      .       @param flag it could be a frame index or a driver specific flag\n",
      "     |      .       @return `false` if no frames has been grabbed\n",
      "     |      .   \n",
      "     |      .       The method decodes and returns the just grabbed frame. If no frames has been grabbed\n",
      "     |      .       (camera has been disconnected, or there are no more frames in video file), the method returns false\n",
      "     |      .       and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).\n",
      "     |      .   \n",
      "     |      .       @sa read()\n",
      "     |      .   \n",
      "     |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      "     |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      "     |      .       cvCloneImage and then do whatever you want with the copy.\n",
      "     |  \n",
      "     |  set(...)\n",
      "     |      set(propId, value) -> retval\n",
      "     |      .   @brief Sets a property in the VideoCapture.\n",
      "     |      .   \n",
      "     |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      "     |      .       or one from @ref videoio_flags_others\n",
      "     |      .       @param value Value of the property.\n",
      "     |      .       @return `true` if the property is supported by backend used by the VideoCapture instance.\n",
      "     |      .       @note Even if it returns `true` this doesn't ensure that the property\n",
      "     |      .       value has been accepted by the capture device. See note in VideoCapture::get()\n",
      "     |  \n",
      "     |  setExceptionMode(...)\n",
      "     |      setExceptionMode(enable) -> None\n",
      "     |      .   Switches exceptions mode\n",
      "     |      .        *\n",
      "     |      .        * methods raise exceptions if not successful instead of returning an error code\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class VideoWriter(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      get(propId) -> retval\n",
      "     |      .   @brief Returns the specified VideoWriter property\n",
      "     |      .   \n",
      "     |      .        @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n",
      "     |      .        or one of @ref videoio_flags_others\n",
      "     |      .   \n",
      "     |      .        @return Value for the specified property. Value 0 is returned when querying a property that is\n",
      "     |      .        not supported by the backend used by the VideoWriter instance.\n",
      "     |  \n",
      "     |  getBackendName(...)\n",
      "     |      getBackendName() -> retval\n",
      "     |      .   @brief Returns used backend API name\n",
      "     |      .   \n",
      "     |      .        @note Stream should be opened.\n",
      "     |  \n",
      "     |  isOpened(...)\n",
      "     |      isOpened() -> retval\n",
      "     |      .   @brief Returns true if video writer has been successfully initialized.\n",
      "     |  \n",
      "     |  open(...)\n",
      "     |      open(filename, fourcc, fps, frameSize[, isColor]) -> retval\n",
      "     |      .   @brief Initializes or reinitializes video writer.\n",
      "     |      .   \n",
      "     |      .       The method opens video writer. Parameters are the same as in the constructor\n",
      "     |      .       VideoWriter::VideoWriter.\n",
      "     |      .       @return `true` if video writer has been successfully initialized\n",
      "     |      .   \n",
      "     |      .       The method first calls VideoWriter::release to close the already opened file.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      open(filename, apiPreference, fourcc, fps, frameSize[, isColor]) -> retval\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  release(...)\n",
      "     |      release() -> None\n",
      "     |      .   @brief Closes the video writer.\n",
      "     |      .   \n",
      "     |      .       The method is automatically called by subsequent VideoWriter::open and by the VideoWriter\n",
      "     |      .       destructor.\n",
      "     |  \n",
      "     |  set(...)\n",
      "     |      set(propId, value) -> retval\n",
      "     |      .   @brief Sets a property in the VideoWriter.\n",
      "     |      .   \n",
      "     |      .        @param propId Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY)\n",
      "     |      .        or one of @ref videoio_flags_others\n",
      "     |      .   \n",
      "     |      .        @param value Value of the property.\n",
      "     |      .        @return  `true` if the property is supported by the backend used by the VideoWriter instance.\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(image) -> None\n",
      "     |      .   @brief Writes the next video frame\n",
      "     |      .   \n",
      "     |      .       @param image The written frame. In general, color images are expected in BGR format.\n",
      "     |      .   \n",
      "     |      .       The function/method writes the specified image to video file. It must have the same size as has\n",
      "     |      .       been specified when opening the video writer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  fourcc(...)\n",
      "     |      fourcc(c1, c2, c3, c4) -> retval\n",
      "     |      .   @brief Concatenates 4 chars to a fourcc code\n",
      "     |      .   \n",
      "     |      .       @return a fourcc code\n",
      "     |      .   \n",
      "     |      .       This static method constructs the fourcc code of the codec to be used in the constructor\n",
      "     |      .       VideoWriter::VideoWriter or VideoWriter::open.\n",
      "    \n",
      "    class WarperCreator(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class cuda_BufferPool(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getAllocator(...)\n",
      "     |      getAllocator() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getBuffer(...)\n",
      "     |      getBuffer(rows, cols, type) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getBuffer(size, type) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class cuda_DeviceInfo(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  ECCEnabled(...)\n",
      "     |      ECCEnabled() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  asyncEngineCount(...)\n",
      "     |      asyncEngineCount() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  canMapHostMemory(...)\n",
      "     |      canMapHostMemory() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  clockRate(...)\n",
      "     |      clockRate() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  computeMode(...)\n",
      "     |      computeMode() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  concurrentKernels(...)\n",
      "     |      concurrentKernels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  deviceID(...)\n",
      "     |      deviceID() -> retval\n",
      "     |      .   @brief Returns system index of the CUDA device starting with 0.\n",
      "     |  \n",
      "     |  freeMemory(...)\n",
      "     |      freeMemory() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  integrated(...)\n",
      "     |      integrated() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isCompatible(...)\n",
      "     |      isCompatible() -> retval\n",
      "     |      .   @brief Checks the CUDA module and device compatibility.\n",
      "     |      .   \n",
      "     |      .       This function returns true if the CUDA module can be run on the specified device. Otherwise, it\n",
      "     |      .       returns false .\n",
      "     |  \n",
      "     |  kernelExecTimeoutEnabled(...)\n",
      "     |      kernelExecTimeoutEnabled() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  l2CacheSize(...)\n",
      "     |      l2CacheSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  majorVersion(...)\n",
      "     |      majorVersion() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxGridSize(...)\n",
      "     |      maxGridSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurface1D(...)\n",
      "     |      maxSurface1D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurface1DLayered(...)\n",
      "     |      maxSurface1DLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurface2D(...)\n",
      "     |      maxSurface2D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurface2DLayered(...)\n",
      "     |      maxSurface2DLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurface3D(...)\n",
      "     |      maxSurface3D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurfaceCubemap(...)\n",
      "     |      maxSurfaceCubemap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSurfaceCubemapLayered(...)\n",
      "     |      maxSurfaceCubemapLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture1D(...)\n",
      "     |      maxTexture1D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture1DLayered(...)\n",
      "     |      maxTexture1DLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture1DLinear(...)\n",
      "     |      maxTexture1DLinear() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture1DMipmap(...)\n",
      "     |      maxTexture1DMipmap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture2D(...)\n",
      "     |      maxTexture2D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture2DGather(...)\n",
      "     |      maxTexture2DGather() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture2DLayered(...)\n",
      "     |      maxTexture2DLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture2DLinear(...)\n",
      "     |      maxTexture2DLinear() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture2DMipmap(...)\n",
      "     |      maxTexture2DMipmap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTexture3D(...)\n",
      "     |      maxTexture3D() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTextureCubemap(...)\n",
      "     |      maxTextureCubemap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxTextureCubemapLayered(...)\n",
      "     |      maxTextureCubemapLayered() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxThreadsDim(...)\n",
      "     |      maxThreadsDim() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxThreadsPerBlock(...)\n",
      "     |      maxThreadsPerBlock() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxThreadsPerMultiProcessor(...)\n",
      "     |      maxThreadsPerMultiProcessor() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  memPitch(...)\n",
      "     |      memPitch() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  memoryBusWidth(...)\n",
      "     |      memoryBusWidth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  memoryClockRate(...)\n",
      "     |      memoryClockRate() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  minorVersion(...)\n",
      "     |      minorVersion() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  multiProcessorCount(...)\n",
      "     |      multiProcessorCount() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  pciBusID(...)\n",
      "     |      pciBusID() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  pciDeviceID(...)\n",
      "     |      pciDeviceID() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  pciDomainID(...)\n",
      "     |      pciDomainID() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  queryMemory(...)\n",
      "     |      queryMemory(totalMemory, freeMemory) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  regsPerBlock(...)\n",
      "     |      regsPerBlock() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  sharedMemPerBlock(...)\n",
      "     |      sharedMemPerBlock() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  surfaceAlignment(...)\n",
      "     |      surfaceAlignment() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  tccDriver(...)\n",
      "     |      tccDriver() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  textureAlignment(...)\n",
      "     |      textureAlignment() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  texturePitchAlignment(...)\n",
      "     |      texturePitchAlignment() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  totalConstMem(...)\n",
      "     |      totalConstMem() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  totalGlobalMem(...)\n",
      "     |      totalGlobalMem() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  totalMemory(...)\n",
      "     |      totalMemory() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  unifiedAddressing(...)\n",
      "     |      unifiedAddressing() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  warpSize(...)\n",
      "     |      warpSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class cuda_Event(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  queryIfComplete(...)\n",
      "     |      queryIfComplete() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  record(...)\n",
      "     |      record([, stream]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  waitForCompletion(...)\n",
      "     |      waitForCompletion() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  elapsedTime(...)\n",
      "     |      elapsedTime(start, end) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class cuda_GpuMat(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  adjustROI(...)\n",
      "     |      adjustROI(dtop, dbottom, dleft, dright) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  assignTo(...)\n",
      "     |      assignTo(m[, type]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  channels(...)\n",
      "     |      channels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  clone(...)\n",
      "     |      clone() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  col(...)\n",
      "     |      col(x) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  colRange(...)\n",
      "     |      colRange(startcol, endcol) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      colRange(r) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  convertTo(...)\n",
      "     |      convertTo(rtype[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      convertTo(rtype, stream[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      convertTo(rtype, alpha[, dst[, beta]]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      convertTo(rtype, alpha, stream[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      convertTo(rtype, alpha, beta, stream[, dst]) -> dst\n",
      "     |      .\n",
      "     |  \n",
      "     |  copyTo(...)\n",
      "     |      copyTo([, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      copyTo(stream[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      copyTo(mask[, dst]) -> dst\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      copyTo(mask, stream[, dst]) -> dst\n",
      "     |      .\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create(rows, cols, type) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      create(size, type) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  depth(...)\n",
      "     |      depth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  download(...)\n",
      "     |      download([, dst]) -> dst\n",
      "     |      .   @brief Performs data download from GpuMat (Blocking call)\n",
      "     |      .   \n",
      "     |      .       This function copies data from device memory to host memory. As being a blocking call, it is\n",
      "     |      .       guaranteed that the copy operation is finished when this function returns.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      download(stream[, dst]) -> dst\n",
      "     |      .   @brief Performs data download from GpuMat (Non-Blocking call)\n",
      "     |      .   \n",
      "     |      .       This function copies data from device memory to host memory. As being a non-blocking call, this\n",
      "     |      .       function may return even if the copy operation is not finished.\n",
      "     |      .   \n",
      "     |      .       The copy operation may be overlapped with operations in other non-default streams if \\p stream is\n",
      "     |      .       not the default stream and \\p dst is HostMem allocated with HostMem::PAGE_LOCKED option.\n",
      "     |  \n",
      "     |  elemSize(...)\n",
      "     |      elemSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  elemSize1(...)\n",
      "     |      elemSize1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isContinuous(...)\n",
      "     |      isContinuous() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  locateROI(...)\n",
      "     |      locateROI(wholeSize, ofs) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  reshape(...)\n",
      "     |      reshape(cn[, rows]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  row(...)\n",
      "     |      row(y) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  rowRange(...)\n",
      "     |      rowRange(startrow, endrow) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      rowRange(r) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTo(...)\n",
      "     |      setTo(s) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTo(s, stream) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTo(s, mask) -> retval\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setTo(s, mask, stream) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  size(...)\n",
      "     |      size() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  step1(...)\n",
      "     |      step1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  swap(...)\n",
      "     |      swap(mat) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  type(...)\n",
      "     |      type() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  updateContinuityFlag(...)\n",
      "     |      updateContinuityFlag() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  upload(...)\n",
      "     |      upload(arr) -> None\n",
      "     |      .   @brief Performs data upload to GpuMat (Blocking call)\n",
      "     |      .   \n",
      "     |      .       This function copies data from host memory to device memory. As being a blocking call, it is\n",
      "     |      .       guaranteed that the copy operation is finished when this function returns.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      upload(arr, stream) -> None\n",
      "     |      .   @brief Performs data upload to GpuMat (Non-Blocking call)\n",
      "     |      .   \n",
      "     |      .       This function copies data from host memory to device memory. As being a non-blocking call, this\n",
      "     |      .       function may return even if the copy operation is not finished.\n",
      "     |      .   \n",
      "     |      .       The copy operation may be overlapped with operations in other non-default streams if \\p stream is\n",
      "     |      .       not the default stream and \\p dst is HostMem allocated with HostMem::PAGE_LOCKED option.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  defaultAllocator(...)\n",
      "     |      defaultAllocator() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setDefaultAllocator(...)\n",
      "     |      setDefaultAllocator(allocator) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  step\n",
      "     |      step\n",
      "    \n",
      "    class cuda_GpuMat_Allocator(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class cuda_HostMem(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  channels(...)\n",
      "     |      channels() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  clone(...)\n",
      "     |      clone() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create(rows, cols, type) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  createMatHeader(...)\n",
      "     |      createMatHeader() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  depth(...)\n",
      "     |      depth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  elemSize(...)\n",
      "     |      elemSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  elemSize1(...)\n",
      "     |      elemSize1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isContinuous(...)\n",
      "     |      isContinuous() -> retval\n",
      "     |      .   @brief Maps CPU memory to GPU address space and creates the cuda::GpuMat header without reference counting\n",
      "     |      .       for it.\n",
      "     |      .   \n",
      "     |      .       This can be done only if memory was allocated with the SHARED flag and if it is supported by the\n",
      "     |      .       hardware. Laptops often share video and CPU memory, so address spaces can be mapped, which\n",
      "     |      .       eliminates an extra copy.\n",
      "     |  \n",
      "     |  reshape(...)\n",
      "     |      reshape(cn[, rows]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  size(...)\n",
      "     |      size() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  step1(...)\n",
      "     |      step1() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  swap(...)\n",
      "     |      swap(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  type(...)\n",
      "     |      type() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  step\n",
      "     |      step\n",
      "    \n",
      "    class cuda_Stream(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  queryIfComplete(...)\n",
      "     |      queryIfComplete() -> retval\n",
      "     |      .   @brief Returns true if the current stream queue is finished. Otherwise, it returns false.\n",
      "     |  \n",
      "     |  waitEvent(...)\n",
      "     |      waitEvent(event) -> None\n",
      "     |      .   @brief Makes a compute stream wait on an event.\n",
      "     |  \n",
      "     |  waitForCompletion(...)\n",
      "     |      waitForCompletion() -> None\n",
      "     |      .   @brief Blocks the current CPU thread until all operations in the stream are complete.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  Null(...)\n",
      "     |      Null() -> retval\n",
      "     |      .   @brief Adds a callback to be called on the host after all currently enqueued items in the stream have\n",
      "     |      .       completed.\n",
      "     |      .   \n",
      "     |      .       @note Callbacks must not make any CUDA API calls. Callbacks must not perform any synchronization\n",
      "     |      .       that may depend on outstanding device work or other callbacks that are not mandated to run earlier.\n",
      "     |      .       Callbacks without a mandated order (in independent streams) execute in undefined order and may be\n",
      "     |      .       serialized.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class cuda_TargetArchs(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  has(...)\n",
      "     |      has(major, minor) -> retval\n",
      "     |      .   @brief There is a set of methods to check whether the module contains intermediate (PTX) or binary CUDA\n",
      "     |      .       code for the given architecture(s):\n",
      "     |      .   \n",
      "     |      .       @param major Major compute capability version.\n",
      "     |      .       @param minor Minor compute capability version.\n",
      "     |  \n",
      "     |  hasBin(...)\n",
      "     |      hasBin(major, minor) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hasEqualOrGreater(...)\n",
      "     |      hasEqualOrGreater(major, minor) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hasEqualOrGreaterBin(...)\n",
      "     |      hasEqualOrGreaterBin(major, minor) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hasEqualOrGreaterPtx(...)\n",
      "     |      hasEqualOrGreaterPtx(major, minor) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hasEqualOrLessPtx(...)\n",
      "     |      hasEqualOrLessPtx(major, minor) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hasPtx(...)\n",
      "     |      hasPtx(major, minor) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_AffineBasedEstimator(detail_Estimator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_AffineBasedEstimator\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_AffineBestOf2NearestMatcher(detail_BestOf2NearestMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      detail_AffineBestOf2NearestMatcher\n",
      "     |      detail_BestOf2NearestMatcher\n",
      "     |      detail_FeaturesMatcher\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BestOf2NearestMatcher:\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_BestOf2NearestMatcher:\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, try_use_gpu[, match_conf[, num_matches_thresh1[, num_matches_thresh2]]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_FeaturesMatcher:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features1, features2) -> matches_info\n",
      "     |      .   @overload\n",
      "     |      .       @param features1 First image features\n",
      "     |      .       @param features2 Second image features\n",
      "     |      .       @param matches_info Found matches\n",
      "     |  \n",
      "     |  apply2(...)\n",
      "     |      apply2(features[, mask]) -> pairwise_matches\n",
      "     |      .   @brief Performs images matching.\n",
      "     |      .   \n",
      "     |      .       @param features Features of the source images\n",
      "     |      .       @param pairwise_matches Found pairwise matches\n",
      "     |      .       @param mask Mask indicating which image pairs must be matched\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |      .   \n",
      "     |      .       @sa detail::MatchesInfo\n",
      "     |  \n",
      "     |  isThreadSafe(...)\n",
      "     |      isThreadSafe() -> retval\n",
      "     |      .   @return True, if it's possible to use the same matcher instance in parallel, false otherwise\n",
      "    \n",
      "    class detail_BestOf2NearestMatcher(detail_FeaturesMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BestOf2NearestMatcher\n",
      "     |      detail_FeaturesMatcher\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, try_use_gpu[, match_conf[, num_matches_thresh1[, num_matches_thresh2]]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_FeaturesMatcher:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features1, features2) -> matches_info\n",
      "     |      .   @overload\n",
      "     |      .       @param features1 First image features\n",
      "     |      .       @param features2 Second image features\n",
      "     |      .       @param matches_info Found matches\n",
      "     |  \n",
      "     |  apply2(...)\n",
      "     |      apply2(features[, mask]) -> pairwise_matches\n",
      "     |      .   @brief Performs images matching.\n",
      "     |      .   \n",
      "     |      .       @param features Features of the source images\n",
      "     |      .       @param pairwise_matches Found pairwise matches\n",
      "     |      .       @param mask Mask indicating which image pairs must be matched\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |      .   \n",
      "     |      .       @sa detail::MatchesInfo\n",
      "     |  \n",
      "     |  isThreadSafe(...)\n",
      "     |      isThreadSafe() -> retval\n",
      "     |      .   @return True, if it's possible to use the same matcher instance in parallel, false otherwise\n",
      "    \n",
      "    class detail_BestOf2NearestRangeMatcher(detail_BestOf2NearestMatcher)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BestOf2NearestRangeMatcher\n",
      "     |      detail_BestOf2NearestMatcher\n",
      "     |      detail_FeaturesMatcher\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BestOf2NearestMatcher:\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_BestOf2NearestMatcher:\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, try_use_gpu[, match_conf[, num_matches_thresh1[, num_matches_thresh2]]]]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_FeaturesMatcher:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features1, features2) -> matches_info\n",
      "     |      .   @overload\n",
      "     |      .       @param features1 First image features\n",
      "     |      .       @param features2 Second image features\n",
      "     |      .       @param matches_info Found matches\n",
      "     |  \n",
      "     |  apply2(...)\n",
      "     |      apply2(features[, mask]) -> pairwise_matches\n",
      "     |      .   @brief Performs images matching.\n",
      "     |      .   \n",
      "     |      .       @param features Features of the source images\n",
      "     |      .       @param pairwise_matches Found pairwise matches\n",
      "     |      .       @param mask Mask indicating which image pairs must be matched\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |      .   \n",
      "     |      .       @sa detail::MatchesInfo\n",
      "     |  \n",
      "     |  isThreadSafe(...)\n",
      "     |      isThreadSafe() -> retval\n",
      "     |      .   @return True, if it's possible to use the same matcher instance in parallel, false otherwise\n",
      "    \n",
      "    class detail_Blender(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  blend(...)\n",
      "     |      blend(dst, dst_mask) -> dst, dst_mask\n",
      "     |      .   @brief Blends and returns the final pano.\n",
      "     |      .   \n",
      "     |      .       @param dst Final pano\n",
      "     |      .       @param dst_mask Final pano mask\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(img, mask, tl) -> None\n",
      "     |      .   @brief Processes the image.\n",
      "     |      .   \n",
      "     |      .       @param img Source image\n",
      "     |      .       @param mask Source image mask\n",
      "     |      .       @param tl Source image top-left corners\n",
      "     |  \n",
      "     |  prepare(...)\n",
      "     |      prepare(corners, sizes) -> None\n",
      "     |      .   @brief Prepares the blender for blending.\n",
      "     |      .   \n",
      "     |      .       @param corners Source images top-left corners\n",
      "     |      .       @param sizes Source image sizes\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      prepare(dst_roi) -> None\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type[, try_gpu]) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_BlocksChannelsCompensator(detail_BlocksCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BlocksChannelsCompensator\n",
      "     |      detail_BlocksCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BlocksCompensator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrFeeds(...)\n",
      "     |      getNrFeeds() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrGainsFilteringIterations(...)\n",
      "     |      getNrGainsFilteringIterations() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(width, height) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setBlockSize(size) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrFeeds(...)\n",
      "     |      setNrFeeds(nr_feeds) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrGainsFilteringIterations(...)\n",
      "     |      setNrGainsFilteringIterations(nr_iterations) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_BlocksCompensator(detail_ExposureCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BlocksCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrFeeds(...)\n",
      "     |      getNrFeeds() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrGainsFilteringIterations(...)\n",
      "     |      getNrGainsFilteringIterations() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(width, height) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setBlockSize(size) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrFeeds(...)\n",
      "     |      setNrFeeds(nr_feeds) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrGainsFilteringIterations(...)\n",
      "     |      setNrGainsFilteringIterations(nr_iterations) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_BlocksGainCompensator(detail_BlocksCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BlocksGainCompensator\n",
      "     |      detail_BlocksCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BlocksCompensator:\n",
      "     |  \n",
      "     |  getBlockSize(...)\n",
      "     |      getBlockSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrFeeds(...)\n",
      "     |      getNrFeeds() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrGainsFilteringIterations(...)\n",
      "     |      getNrGainsFilteringIterations() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setBlockSize(...)\n",
      "     |      setBlockSize(width, height) -> None\n",
      "     |      .   \n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      setBlockSize(size) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrFeeds(...)\n",
      "     |      setNrFeeds(nr_feeds) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrGainsFilteringIterations(...)\n",
      "     |      setNrGainsFilteringIterations(nr_iterations) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_BundleAdjusterAffine(detail_BundleAdjusterBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BundleAdjusterAffine\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BundleAdjusterBase:\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_BundleAdjusterAffinePartial(detail_BundleAdjusterBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BundleAdjusterAffinePartial\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BundleAdjusterBase:\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_BundleAdjusterBase(detail_Estimator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_BundleAdjusterRay(detail_BundleAdjusterBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BundleAdjusterRay\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BundleAdjusterBase:\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_BundleAdjusterReproj(detail_BundleAdjusterBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_BundleAdjusterReproj\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BundleAdjusterBase:\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_CameraParams(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  K(...)\n",
      "     |      K() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  R\n",
      "     |      R\n",
      "     |  \n",
      "     |  aspect\n",
      "     |      aspect\n",
      "     |  \n",
      "     |  focal\n",
      "     |      focal\n",
      "     |  \n",
      "     |  ppx\n",
      "     |      ppx\n",
      "     |  \n",
      "     |  ppy\n",
      "     |      ppy\n",
      "     |  \n",
      "     |  t\n",
      "     |      t\n",
      "    \n",
      "    class detail_ChannelsCompensator(detail_ExposureCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_ChannelsCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrFeeds(...)\n",
      "     |      getNrFeeds() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrFeeds(...)\n",
      "     |      setNrFeeds(nr_feeds) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_DpSeamFinder(detail_SeamFinder)\n",
      "     |  Method resolution order:\n",
      "     |      detail_DpSeamFinder\n",
      "     |      detail_SeamFinder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  setCostFunction(...)\n",
      "     |      setCostFunction(val) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_SeamFinder:\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(src, corners, masks) -> masks\n",
      "     |      .   @brief Estimates seams.\n",
      "     |      .   \n",
      "     |      .       @param src Source images\n",
      "     |      .       @param corners Source image top-left corners\n",
      "     |      .       @param masks Source image masks to update\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_SeamFinder:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_Estimator(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class detail_ExposureCompensator(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .   @brief Compensate exposure in the specified image.\n",
      "     |      .   \n",
      "     |      .       @param index Image index\n",
      "     |      .       @param corner Image top-left corner\n",
      "     |      .       @param image Image to process\n",
      "     |      .       @param mask Image mask\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, arg1]) -> arg1\n",
      "     |      .\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(arg1) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_FeatherBlender(detail_Blender)\n",
      "     |  Method resolution order:\n",
      "     |      detail_FeatherBlender\n",
      "     |      detail_Blender\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  blend(...)\n",
      "     |      blend(dst, dst_mask) -> dst, dst_mask\n",
      "     |      .\n",
      "     |  \n",
      "     |  createWeightMaps(...)\n",
      "     |      createWeightMaps(masks, corners, weight_maps) -> retval, weight_maps\n",
      "     |      .\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(img, mask, tl) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  prepare(...)\n",
      "     |      prepare(dst_roi) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setSharpness(...)\n",
      "     |      setSharpness(val) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  sharpness(...)\n",
      "     |      sharpness() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_Blender:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type[, try_gpu]) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_FeaturesMatcher(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features1, features2) -> matches_info\n",
      "     |      .   @overload\n",
      "     |      .       @param features1 First image features\n",
      "     |      .       @param features2 Second image features\n",
      "     |      .       @param matches_info Found matches\n",
      "     |  \n",
      "     |  apply2(...)\n",
      "     |      apply2(features[, mask]) -> pairwise_matches\n",
      "     |      .   @brief Performs images matching.\n",
      "     |      .   \n",
      "     |      .       @param features Features of the source images\n",
      "     |      .       @param pairwise_matches Found pairwise matches\n",
      "     |      .       @param mask Mask indicating which image pairs must be matched\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |      .   \n",
      "     |      .       @sa detail::MatchesInfo\n",
      "     |  \n",
      "     |  collectGarbage(...)\n",
      "     |      collectGarbage() -> None\n",
      "     |      .   @brief Frees unused memory allocated before if there is any.\n",
      "     |  \n",
      "     |  isThreadSafe(...)\n",
      "     |      isThreadSafe() -> retval\n",
      "     |      .   @return True, if it's possible to use the same matcher instance in parallel, false otherwise\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class detail_GainCompensator(detail_ExposureCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_GainCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(index, corner, image, mask) -> image\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNrFeeds(...)\n",
      "     |      getNrFeeds() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNrFeeds(...)\n",
      "     |      setNrFeeds(nr_feeds) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_GraphCutSeamFinder(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(src, corners, masks) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class detail_HomographyBasedEstimator(detail_Estimator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_HomographyBasedEstimator\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_ImageFeatures(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getKeypoints(...)\n",
      "     |      getKeypoints() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  descriptors\n",
      "     |      descriptors\n",
      "     |  \n",
      "     |  img_idx\n",
      "     |      img_idx\n",
      "     |  \n",
      "     |  img_size\n",
      "     |      img_size\n",
      "    \n",
      "    class detail_MatchesInfo(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getInliers(...)\n",
      "     |      getInliers() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatches(...)\n",
      "     |      getMatches() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  H\n",
      "     |      H\n",
      "     |  \n",
      "     |  confidence\n",
      "     |      confidence\n",
      "     |  \n",
      "     |  dst_img_idx\n",
      "     |      dst_img_idx\n",
      "     |  \n",
      "     |  num_inliers\n",
      "     |      num_inliers\n",
      "     |  \n",
      "     |  src_img_idx\n",
      "     |      src_img_idx\n",
      "    \n",
      "    class detail_MultiBandBlender(detail_Blender)\n",
      "     |  Method resolution order:\n",
      "     |      detail_MultiBandBlender\n",
      "     |      detail_Blender\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  blend(...)\n",
      "     |      blend(dst, dst_mask) -> dst, dst_mask\n",
      "     |      .\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(img, mask, tl) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  numBands(...)\n",
      "     |      numBands() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  prepare(...)\n",
      "     |      prepare(dst_roi) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setNumBands(...)\n",
      "     |      setNumBands(val) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_Blender:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type[, try_gpu]) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_NoBundleAdjuster(detail_BundleAdjusterBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_NoBundleAdjuster\n",
      "     |      detail_BundleAdjusterBase\n",
      "     |      detail_Estimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_BundleAdjusterBase:\n",
      "     |  \n",
      "     |  confThresh(...)\n",
      "     |      confThresh() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  refinementMask(...)\n",
      "     |      refinementMask() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setConfThresh(...)\n",
      "     |      setConfThresh(conf_thresh) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setRefinementMask(...)\n",
      "     |      setRefinementMask(mask) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(term_criteria) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  termCriteria(...)\n",
      "     |      termCriteria() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Estimator:\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(features, pairwise_matches, cameras) -> retval, cameras\n",
      "     |      .   @brief Estimates camera parameters.\n",
      "     |      .   \n",
      "     |      .       @param features Features of images\n",
      "     |      .       @param pairwise_matches Pairwise matches of images\n",
      "     |      .       @param cameras Estimated camera parameters\n",
      "     |      .       @return True in case of success, false otherwise\n",
      "    \n",
      "    class detail_NoExposureCompensator(detail_ExposureCompensator)\n",
      "     |  Method resolution order:\n",
      "     |      detail_NoExposureCompensator\n",
      "     |      detail_ExposureCompensator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  apply(...)\n",
      "     |      apply(arg1, arg2, arg3, arg4) -> arg3\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMatGains(...)\n",
      "     |      getMatGains([, umv]) -> umv\n",
      "     |      .\n",
      "     |  \n",
      "     |  setMatGains(...)\n",
      "     |      setMatGains(umv) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  feed(...)\n",
      "     |      feed(corners, images, masks) -> None\n",
      "     |      .   @param corners Source image top-left corners\n",
      "     |      .       @param images Source images\n",
      "     |      .       @param masks Image masks to update (second value in pair specifies the value which should be used\n",
      "     |      .       to detect where image is)\n",
      "     |  \n",
      "     |  getUpdateGain(...)\n",
      "     |      getUpdateGain() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setUpdateGain(...)\n",
      "     |      setUpdateGain(b) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_ExposureCompensator:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_NoSeamFinder(detail_SeamFinder)\n",
      "     |  Method resolution order:\n",
      "     |      detail_NoSeamFinder\n",
      "     |      detail_SeamFinder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(arg1, arg2, arg3) -> arg3\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_SeamFinder:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_PairwiseSeamFinder(detail_SeamFinder)\n",
      "     |  Method resolution order:\n",
      "     |      detail_PairwiseSeamFinder\n",
      "     |      detail_SeamFinder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(src, corners, masks) -> masks\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_SeamFinder:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_ProjectorBase(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class detail_SeamFinder(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(src, corners, masks) -> masks\n",
      "     |      .   @brief Estimates seams.\n",
      "     |      .   \n",
      "     |      .       @param src Source images\n",
      "     |      .       @param corners Source image top-left corners\n",
      "     |      .       @param masks Source image masks to update\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_SphericalProjector(detail_ProjectorBase)\n",
      "     |  Method resolution order:\n",
      "     |      detail_SphericalProjector\n",
      "     |      detail_ProjectorBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  mapBackward(...)\n",
      "     |      mapBackward(u, v, x, y) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  mapForward(...)\n",
      "     |      mapForward(x, y, u, v) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class detail_Timelapser(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getDst(...)\n",
      "     |      getDst() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  initialize(...)\n",
      "     |      initialize(corners, sizes) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(img, mask, tl) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_TimelapserCrop(detail_Timelapser)\n",
      "     |  Method resolution order:\n",
      "     |      detail_TimelapserCrop\n",
      "     |      detail_Timelapser\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from detail_Timelapser:\n",
      "     |  \n",
      "     |  getDst(...)\n",
      "     |      getDst() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  initialize(...)\n",
      "     |      initialize(corners, sizes) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  process(...)\n",
      "     |      process(img, mask, tl) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_Timelapser:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class detail_VoronoiSeamFinder(detail_PairwiseSeamFinder)\n",
      "     |  Method resolution order:\n",
      "     |      detail_VoronoiSeamFinder\n",
      "     |      detail_PairwiseSeamFinder\n",
      "     |      detail_SeamFinder\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  find(...)\n",
      "     |      find(src, corners, masks) -> masks\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from detail_SeamFinder:\n",
      "     |  \n",
      "     |  createDefault(...)\n",
      "     |      createDefault(type) -> retval\n",
      "     |      .\n",
      "    \n",
      "    class dnn_DictValue(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getIntValue(...)\n",
      "     |      getIntValue([, idx]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getRealValue(...)\n",
      "     |      getRealValue([, idx]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getStringValue(...)\n",
      "     |      getStringValue([, idx]) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isInt(...)\n",
      "     |      isInt() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isReal(...)\n",
      "     |      isReal() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isString(...)\n",
      "     |      isString() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class dnn_Layer(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      dnn_Layer\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  finalize(...)\n",
      "     |      finalize(inputs[, outputs]) -> outputs\n",
      "     |      .   @brief Computes and sets internal parameters according to inputs, outputs and blobs.\n",
      "     |      .            *  @param[in]  inputs  vector of already allocated input blobs\n",
      "     |      .            *  @param[out] outputs vector of already allocated output blobs\n",
      "     |      .            *\n",
      "     |      .            * If this method is called after network has allocated all memory for input and output blobs\n",
      "     |      .            * and before inferencing.\n",
      "     |  \n",
      "     |  outputNameToIndex(...)\n",
      "     |      outputNameToIndex(outputName) -> retval\n",
      "     |      .   @brief Returns index of output blob in output array.\n",
      "     |      .            *  @see inputNameToIndex()\n",
      "     |  \n",
      "     |  run(...)\n",
      "     |      run(inputs, internals[, outputs]) -> outputs, internals\n",
      "     |      .   @brief Allocates layer and computes output.\n",
      "     |      .            *  @deprecated This method will be removed in the future release.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  blobs\n",
      "     |      blobs\n",
      "     |  \n",
      "     |  name\n",
      "     |      name\n",
      "     |  \n",
      "     |  preferableTarget\n",
      "     |      preferableTarget\n",
      "     |  \n",
      "     |  type\n",
      "     |      type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   @brief Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class dnn_Net(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  connect(...)\n",
      "     |      connect(outPin, inpPin) -> None\n",
      "     |      .   @brief Connects output of the first layer to input of the second layer.\n",
      "     |      .            *  @param outPin descriptor of the first layer output.\n",
      "     |      .            *  @param inpPin descriptor of the second layer input.\n",
      "     |      .            *\n",
      "     |      .            * Descriptors have the following template <DFN>&lt;layer_name&gt;[.input_number]</DFN>:\n",
      "     |      .            * - the first part of the template <DFN>layer_name</DFN> is sting name of the added layer.\n",
      "     |      .            *   If this part is empty then the network input pseudo layer will be used;\n",
      "     |      .            * - the second optional part of the template <DFN>input_number</DFN>\n",
      "     |      .            *   is either number of the layer input, either label one.\n",
      "     |      .            *   If this part is omitted then the first layer input will be used.\n",
      "     |      .            *\n",
      "     |      .            *  @see setNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()\n",
      "     |  \n",
      "     |  dump(...)\n",
      "     |      dump() -> retval\n",
      "     |      .   @brief Dump net to String\n",
      "     |      .            *  @returns String with structure, hyperparameters, backend, target and fusion\n",
      "     |      .            *  Call method after setInput(). To see correct backend, target and fusion run after forward().\n",
      "     |  \n",
      "     |  dumpToFile(...)\n",
      "     |      dumpToFile(path) -> None\n",
      "     |      .   @brief Dump net structure, hyperparameters, backend, target and fusion to dot file\n",
      "     |      .            *  @param path   path to output file with .dot extension\n",
      "     |      .            *  @see dump()\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .   Returns true if there are no layers in the network.\n",
      "     |  \n",
      "     |  enableFusion(...)\n",
      "     |      enableFusion(fusion) -> None\n",
      "     |      .   @brief Enables or disables layer fusion in the network.\n",
      "     |      .            * @param fusion true to enable the fusion, false to disable. The fusion is enabled by default.\n",
      "     |  \n",
      "     |  forward(...)\n",
      "     |      forward([, outputName]) -> retval\n",
      "     |      .   @brief Runs forward pass to compute output of layer with name @p outputName.\n",
      "     |      .            *  @param outputName name for layer which output is needed to get\n",
      "     |      .            *  @return blob for first output of specified layer.\n",
      "     |      .            *  @details By default runs forward pass for the whole network.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      forward([, outputBlobs[, outputName]]) -> outputBlobs\n",
      "     |      .   @brief Runs forward pass to compute output of layer with name @p outputName.\n",
      "     |      .            *  @param outputBlobs contains all output blobs for specified layer.\n",
      "     |      .            *  @param outputName name for layer which output is needed to get\n",
      "     |      .            *  @details If @p outputName is empty, runs forward pass for the whole network.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      forward(outBlobNames[, outputBlobs]) -> outputBlobs\n",
      "     |      .   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n",
      "     |      .            *  @param outputBlobs contains blobs for first outputs of specified layers.\n",
      "     |      .            *  @param outBlobNames names for layers which outputs are needed to get\n",
      "     |  \n",
      "     |  forwardAndRetrieve(...)\n",
      "     |      forwardAndRetrieve(outBlobNames) -> outputBlobs\n",
      "     |      .   @brief Runs forward pass to compute outputs of layers listed in @p outBlobNames.\n",
      "     |      .            *  @param outputBlobs contains all output blobs for each layer specified in @p outBlobNames.\n",
      "     |      .            *  @param outBlobNames names for layers which outputs are needed to get\n",
      "     |  \n",
      "     |  forwardAsync(...)\n",
      "     |      forwardAsync([, outputName]) -> retval\n",
      "     |      .   @brief Runs forward pass to compute output of layer with name @p outputName.\n",
      "     |      .            *  @param outputName name for layer which output is needed to get\n",
      "     |      .            *  @details By default runs forward pass for the whole network.\n",
      "     |      .            *\n",
      "     |      .            *  This is an asynchronous version of forward(const String&).\n",
      "     |      .            *  dnn::DNN_BACKEND_INFERENCE_ENGINE backend is required.\n",
      "     |  \n",
      "     |  getFLOPS(...)\n",
      "     |      getFLOPS(netInputShapes) -> retval\n",
      "     |      .   @brief Computes FLOP for whole loaded model with specified input shapes.\n",
      "     |      .            * @param netInputShapes vector of shapes for all net inputs.\n",
      "     |      .            * @returns computed FLOP.\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getFLOPS(netInputShape) -> retval\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getFLOPS(layerId, netInputShapes) -> retval\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getFLOPS(layerId, netInputShape) -> retval\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  getLayer(...)\n",
      "     |      getLayer(layerId) -> retval\n",
      "     |      .   @brief Returns pointer to layer with specified id or name which the network use.\n",
      "     |  \n",
      "     |  getLayerId(...)\n",
      "     |      getLayerId(layer) -> retval\n",
      "     |      .   @brief Converts string name of the layer to the integer identifier.\n",
      "     |      .            *  @returns id of the layer, or -1 if the layer wasn't found.\n",
      "     |  \n",
      "     |  getLayerNames(...)\n",
      "     |      getLayerNames() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getLayerTypes(...)\n",
      "     |      getLayerTypes() -> layersTypes\n",
      "     |      .   @brief Returns list of types for layer used in model.\n",
      "     |      .            * @param layersTypes output parameter for returning types.\n",
      "     |  \n",
      "     |  getLayersCount(...)\n",
      "     |      getLayersCount(layerType) -> retval\n",
      "     |      .   @brief Returns count of layers of specified type.\n",
      "     |      .            * @param layerType type.\n",
      "     |      .            * @returns count of layers\n",
      "     |  \n",
      "     |  getLayersShapes(...)\n",
      "     |      getLayersShapes(netInputShapes) -> layersIds, inLayersShapes, outLayersShapes\n",
      "     |      .   @brief Returns input and output shapes for all layers in loaded model;\n",
      "     |      .            *  preliminary inferencing isn't necessary.\n",
      "     |      .            *  @param netInputShapes shapes for all input blobs in net input layer.\n",
      "     |      .            *  @param layersIds output parameter for layer IDs.\n",
      "     |      .            *  @param inLayersShapes output parameter for input layers shapes;\n",
      "     |      .            * order is the same as in layersIds\n",
      "     |      .            *  @param outLayersShapes output parameter for output layers shapes;\n",
      "     |      .            * order is the same as in layersIds\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getLayersShapes(netInputShape) -> layersIds, inLayersShapes, outLayersShapes\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  getMemoryConsumption(...)\n",
      "     |      getMemoryConsumption(netInputShape) -> weights, blobs\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getMemoryConsumption(layerId, netInputShapes) -> weights, blobs\n",
      "     |      .   @overload\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      getMemoryConsumption(layerId, netInputShape) -> weights, blobs\n",
      "     |      .   @overload\n",
      "     |  \n",
      "     |  getParam(...)\n",
      "     |      getParam(layer[, numParam]) -> retval\n",
      "     |      .   @brief Returns parameter blob of the layer.\n",
      "     |      .            *  @param layer name or id of the layer.\n",
      "     |      .            *  @param numParam index of the layer parameter in the Layer::blobs array.\n",
      "     |      .            *  @see Layer::blobs\n",
      "     |  \n",
      "     |  getPerfProfile(...)\n",
      "     |      getPerfProfile() -> retval, timings\n",
      "     |      .   @brief Returns overall time for inference and timings (in ticks) for layers.\n",
      "     |      .            * Indexes in returned vector correspond to layers ids. Some layers can be fused with others,\n",
      "     |      .            * in this case zero ticks count will be return for that skipped layers.\n",
      "     |      .            * @param timings vector for tick timings for all layers.\n",
      "     |      .            * @return overall ticks for model inference.\n",
      "     |  \n",
      "     |  getUnconnectedOutLayers(...)\n",
      "     |      getUnconnectedOutLayers() -> retval\n",
      "     |      .   @brief Returns indexes of layers with unconnected outputs.\n",
      "     |  \n",
      "     |  getUnconnectedOutLayersNames(...)\n",
      "     |      getUnconnectedOutLayersNames() -> retval\n",
      "     |      .   @brief Returns names of layers with unconnected outputs.\n",
      "     |  \n",
      "     |  setHalideScheduler(...)\n",
      "     |      setHalideScheduler(scheduler) -> None\n",
      "     |      .   * @brief Compile Halide layers.\n",
      "     |      .            * @param[in] scheduler Path to YAML file with scheduling directives.\n",
      "     |      .            * @see setPreferableBackend\n",
      "     |      .            *\n",
      "     |      .            * Schedule layers that support Halide backend. Then compile them for\n",
      "     |      .            * specific target. For layers that not represented in scheduling file\n",
      "     |      .            * or if no manual scheduling used at all, automatic scheduling will be applied.\n",
      "     |  \n",
      "     |  setInput(...)\n",
      "     |      setInput(blob[, name[, scalefactor[, mean]]]) -> None\n",
      "     |      .   @brief Sets the new input value for the network\n",
      "     |      .            *  @param blob        A new blob. Should have CV_32F or CV_8U depth.\n",
      "     |      .            *  @param name        A name of input layer.\n",
      "     |      .            *  @param scalefactor An optional normalization scale.\n",
      "     |      .            *  @param mean        An optional mean subtraction values.\n",
      "     |      .            *  @see connect(String, String) to know format of the descriptor.\n",
      "     |      .            *\n",
      "     |      .            *  If scale or mean values are specified, a final input blob is computed\n",
      "     |      .            *  as:\n",
      "     |      .            * \\f[input(n,c,h,w) = scalefactor \\times (blob(n,c,h,w) - mean_c)\\f]\n",
      "     |  \n",
      "     |  setInputsNames(...)\n",
      "     |      setInputsNames(inputBlobNames) -> None\n",
      "     |      .   @brief Sets outputs names of the network input pseudo layer.\n",
      "     |      .            *\n",
      "     |      .            * Each net always has special own the network input pseudo layer with id=0.\n",
      "     |      .            * This layer stores the user blobs only and don't make any computations.\n",
      "     |      .            * In fact, this layer provides the only way to pass user data into the network.\n",
      "     |      .            * As any other layer, this layer can label its outputs and this function provides an easy way to do this.\n",
      "     |  \n",
      "     |  setParam(...)\n",
      "     |      setParam(layer, numParam, blob) -> None\n",
      "     |      .   @brief Sets the new value for the learned param of the layer.\n",
      "     |      .            *  @param layer name or id of the layer.\n",
      "     |      .            *  @param numParam index of the layer parameter in the Layer::blobs array.\n",
      "     |      .            *  @param blob the new value.\n",
      "     |      .            *  @see Layer::blobs\n",
      "     |      .            *  @note If shape of the new blob differs from the previous shape,\n",
      "     |      .            *  then the following forward pass may fail.\n",
      "     |  \n",
      "     |  setPreferableBackend(...)\n",
      "     |      setPreferableBackend(backendId) -> None\n",
      "     |      .   * @brief Ask network to use specific computation backend where it supported.\n",
      "     |      .            * @param[in] backendId backend identifier.\n",
      "     |      .            * @see Backend\n",
      "     |      .            *\n",
      "     |      .            * If OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT\n",
      "     |      .            * means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV.\n",
      "     |  \n",
      "     |  setPreferableTarget(...)\n",
      "     |      setPreferableTarget(targetId) -> None\n",
      "     |      .   * @brief Ask network to make computations on specific target device.\n",
      "     |      .            * @param[in] targetId target identifier.\n",
      "     |      .            * @see Target\n",
      "     |      .            *\n",
      "     |      .            * List of supported combinations backend / target:\n",
      "     |      .            * |                        | DNN_BACKEND_OPENCV | DNN_BACKEND_INFERENCE_ENGINE | DNN_BACKEND_HALIDE |\n",
      "     |      .            * |------------------------|--------------------|------------------------------|--------------------|\n",
      "     |      .            * | DNN_TARGET_CPU         |                  + |                            + |                  + |\n",
      "     |      .            * | DNN_TARGET_OPENCL      |                  + |                            + |                  + |\n",
      "     |      .            * | DNN_TARGET_OPENCL_FP16 |                  + |                            + |                    |\n",
      "     |      .            * | DNN_TARGET_MYRIAD      |                    |                            + |                    |\n",
      "     |      .            * | DNN_TARGET_FPGA        |                    |                            + |                    |\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  readFromModelOptimizer(...)\n",
      "     |      readFromModelOptimizer(xml, bin) -> retval\n",
      "     |      .   @brief Create a network from Intel's Model Optimizer intermediate representation.\n",
      "     |      .            *  @param[in] xml XML configuration file with network's topology.\n",
      "     |      .            *  @param[in] bin Binary file with trained weights.\n",
      "     |      .            *  Networks imported from Intel's Model Optimizer are launched in Intel's Inference Engine\n",
      "     |      .            *  backend.\n",
      "    \n",
      "    class error(builtins.Exception)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      error\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  code = None\n",
      "     |  \n",
      "     |  err = None\n",
      "     |  \n",
      "     |  file = None\n",
      "     |  \n",
      "     |  func = None\n",
      "     |  \n",
      "     |  line = None\n",
      "     |  \n",
      "     |  msg = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class flann_Index(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  build(...)\n",
      "     |      build(features, params[, distType]) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  getAlgorithm(...)\n",
      "     |      getAlgorithm() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getDistance(...)\n",
      "     |      getDistance() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  knnSearch(...)\n",
      "     |      knnSearch(query, knn[, indices[, dists[, params]]]) -> indices, dists\n",
      "     |      .\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(features, filename) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  radiusSearch(...)\n",
      "     |      radiusSearch(query, radius, maxResults[, indices[, dists[, params]]]) -> retval, indices, dists\n",
      "     |      .\n",
      "     |  \n",
      "     |  release(...)\n",
      "     |      release() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class ml_ANN_MLP(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_ANN_MLP\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getAnnealCoolingRatio(...)\n",
      "     |      getAnnealCoolingRatio() -> retval\n",
      "     |      .   @see setAnnealCoolingRatio\n",
      "     |  \n",
      "     |  getAnnealFinalT(...)\n",
      "     |      getAnnealFinalT() -> retval\n",
      "     |      .   @see setAnnealFinalT\n",
      "     |  \n",
      "     |  getAnnealInitialT(...)\n",
      "     |      getAnnealInitialT() -> retval\n",
      "     |      .   @see setAnnealInitialT\n",
      "     |  \n",
      "     |  getAnnealItePerStep(...)\n",
      "     |      getAnnealItePerStep() -> retval\n",
      "     |      .   @see setAnnealItePerStep\n",
      "     |  \n",
      "     |  getBackpropMomentumScale(...)\n",
      "     |      getBackpropMomentumScale() -> retval\n",
      "     |      .   @see setBackpropMomentumScale\n",
      "     |  \n",
      "     |  getBackpropWeightScale(...)\n",
      "     |      getBackpropWeightScale() -> retval\n",
      "     |      .   @see setBackpropWeightScale\n",
      "     |  \n",
      "     |  getLayerSizes(...)\n",
      "     |      getLayerSizes() -> retval\n",
      "     |      .   Integer vector specifying the number of neurons in each layer including the input and output layers.\n",
      "     |      .       The very first element specifies the number of elements in the input layer.\n",
      "     |      .       The last element - number of elements in the output layer.\n",
      "     |      .   @sa setLayerSizes\n",
      "     |  \n",
      "     |  getRpropDW0(...)\n",
      "     |      getRpropDW0() -> retval\n",
      "     |      .   @see setRpropDW0\n",
      "     |  \n",
      "     |  getRpropDWMax(...)\n",
      "     |      getRpropDWMax() -> retval\n",
      "     |      .   @see setRpropDWMax\n",
      "     |  \n",
      "     |  getRpropDWMin(...)\n",
      "     |      getRpropDWMin() -> retval\n",
      "     |      .   @see setRpropDWMin\n",
      "     |  \n",
      "     |  getRpropDWMinus(...)\n",
      "     |      getRpropDWMinus() -> retval\n",
      "     |      .   @see setRpropDWMinus\n",
      "     |  \n",
      "     |  getRpropDWPlus(...)\n",
      "     |      getRpropDWPlus() -> retval\n",
      "     |      .   @see setRpropDWPlus\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getTrainMethod(...)\n",
      "     |      getTrainMethod() -> retval\n",
      "     |      .   Returns current training method\n",
      "     |  \n",
      "     |  getWeights(...)\n",
      "     |      getWeights(layerIdx) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setActivationFunction(...)\n",
      "     |      setActivationFunction(type[, param1[, param2]]) -> None\n",
      "     |      .   Initialize the activation function for each neuron.\n",
      "     |      .       Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.\n",
      "     |      .       @param type The type of activation function. See ANN_MLP::ActivationFunctions.\n",
      "     |      .       @param param1 The first parameter of the activation function, \\f$\\alpha\\f$. Default value is 0.\n",
      "     |      .       @param param2 The second parameter of the activation function, \\f$\\beta\\f$. Default value is 0.\n",
      "     |  \n",
      "     |  setAnnealCoolingRatio(...)\n",
      "     |      setAnnealCoolingRatio(val) -> None\n",
      "     |      .   @copybrief getAnnealCoolingRatio @see getAnnealCoolingRatio\n",
      "     |  \n",
      "     |  setAnnealFinalT(...)\n",
      "     |      setAnnealFinalT(val) -> None\n",
      "     |      .   @copybrief getAnnealFinalT @see getAnnealFinalT\n",
      "     |  \n",
      "     |  setAnnealInitialT(...)\n",
      "     |      setAnnealInitialT(val) -> None\n",
      "     |      .   @copybrief getAnnealInitialT @see getAnnealInitialT\n",
      "     |  \n",
      "     |  setAnnealItePerStep(...)\n",
      "     |      setAnnealItePerStep(val) -> None\n",
      "     |      .   @copybrief getAnnealItePerStep @see getAnnealItePerStep\n",
      "     |  \n",
      "     |  setBackpropMomentumScale(...)\n",
      "     |      setBackpropMomentumScale(val) -> None\n",
      "     |      .   @copybrief getBackpropMomentumScale @see getBackpropMomentumScale\n",
      "     |  \n",
      "     |  setBackpropWeightScale(...)\n",
      "     |      setBackpropWeightScale(val) -> None\n",
      "     |      .   @copybrief getBackpropWeightScale @see getBackpropWeightScale\n",
      "     |  \n",
      "     |  setLayerSizes(...)\n",
      "     |      setLayerSizes(_layer_sizes) -> None\n",
      "     |      .   Integer vector specifying the number of neurons in each layer including the input and output layers.\n",
      "     |      .       The very first element specifies the number of elements in the input layer.\n",
      "     |      .       The last element - number of elements in the output layer. Default value is empty Mat.\n",
      "     |      .   @sa getLayerSizes\n",
      "     |  \n",
      "     |  setRpropDW0(...)\n",
      "     |      setRpropDW0(val) -> None\n",
      "     |      .   @copybrief getRpropDW0 @see getRpropDW0\n",
      "     |  \n",
      "     |  setRpropDWMax(...)\n",
      "     |      setRpropDWMax(val) -> None\n",
      "     |      .   @copybrief getRpropDWMax @see getRpropDWMax\n",
      "     |  \n",
      "     |  setRpropDWMin(...)\n",
      "     |      setRpropDWMin(val) -> None\n",
      "     |      .   @copybrief getRpropDWMin @see getRpropDWMin\n",
      "     |  \n",
      "     |  setRpropDWMinus(...)\n",
      "     |      setRpropDWMinus(val) -> None\n",
      "     |      .   @copybrief getRpropDWMinus @see getRpropDWMinus\n",
      "     |  \n",
      "     |  setRpropDWPlus(...)\n",
      "     |      setRpropDWPlus(val) -> None\n",
      "     |      .   @copybrief getRpropDWPlus @see getRpropDWPlus\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  setTrainMethod(...)\n",
      "     |      setTrainMethod(method[, param1[, param2]]) -> None\n",
      "     |      .   Sets training method and common parameters.\n",
      "     |      .       @param method Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.\n",
      "     |      .       @param param1 passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.\n",
      "     |      .       @param param2 passed to setRpropDWMin for ANN_MLP::RPROP and to setBackpropMomentumScale for ANN_MLP::BACKPROP and to finalT for ANN_MLP::ANNEAL.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates empty model\n",
      "     |      .   \n",
      "     |      .       Use StatModel::train to train the model, Algorithm::load\\<ANN_MLP\\>(filename) to load the pre-trained model.\n",
      "     |      .       Note that the train method has optional flags: ANN_MLP::TrainFlags.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath) -> retval\n",
      "     |      .   @brief Loads and creates a serialized ANN from a file\n",
      "     |      .        *\n",
      "     |      .        * Use ANN::save to serialize and store an ANN to disk.\n",
      "     |      .        * Load the ANN from this file again, by calling this function with the path to the file.\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized ANN\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_Boost(ml_DTrees)\n",
      "     |  Method resolution order:\n",
      "     |      ml_Boost\n",
      "     |      ml_DTrees\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getBoostType(...)\n",
      "     |      getBoostType() -> retval\n",
      "     |      .   @see setBoostType\n",
      "     |  \n",
      "     |  getWeakCount(...)\n",
      "     |      getWeakCount() -> retval\n",
      "     |      .   @see setWeakCount\n",
      "     |  \n",
      "     |  getWeightTrimRate(...)\n",
      "     |      getWeightTrimRate() -> retval\n",
      "     |      .   @see setWeightTrimRate\n",
      "     |  \n",
      "     |  setBoostType(...)\n",
      "     |      setBoostType(val) -> None\n",
      "     |      .   @copybrief getBoostType @see getBoostType\n",
      "     |  \n",
      "     |  setWeakCount(...)\n",
      "     |      setWeakCount(val) -> None\n",
      "     |      .   @copybrief getWeakCount @see getWeakCount\n",
      "     |  \n",
      "     |  setWeightTrimRate(...)\n",
      "     |      setWeightTrimRate(val) -> None\n",
      "     |      .   @copybrief getWeightTrimRate @see getWeightTrimRate\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   Creates the empty model.\n",
      "     |      .   Use StatModel::train to train the model, Algorithm::load\\<Boost\\>(filename) to load the pre-trained model.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized Boost from a file\n",
      "     |      .        *\n",
      "     |      .        * Use Boost::save to serialize and store an RTree to disk.\n",
      "     |      .        * Load the Boost from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized Boost\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_DTrees:\n",
      "     |  \n",
      "     |  getCVFolds(...)\n",
      "     |      getCVFolds() -> retval\n",
      "     |      .   @see setCVFolds\n",
      "     |  \n",
      "     |  getMaxCategories(...)\n",
      "     |      getMaxCategories() -> retval\n",
      "     |      .   @see setMaxCategories\n",
      "     |  \n",
      "     |  getMaxDepth(...)\n",
      "     |      getMaxDepth() -> retval\n",
      "     |      .   @see setMaxDepth\n",
      "     |  \n",
      "     |  getMinSampleCount(...)\n",
      "     |      getMinSampleCount() -> retval\n",
      "     |      .   @see setMinSampleCount\n",
      "     |  \n",
      "     |  getPriors(...)\n",
      "     |      getPriors() -> retval\n",
      "     |      .   @see setPriors\n",
      "     |  \n",
      "     |  getRegressionAccuracy(...)\n",
      "     |      getRegressionAccuracy() -> retval\n",
      "     |      .   @see setRegressionAccuracy\n",
      "     |  \n",
      "     |  getTruncatePrunedTree(...)\n",
      "     |      getTruncatePrunedTree() -> retval\n",
      "     |      .   @see setTruncatePrunedTree\n",
      "     |  \n",
      "     |  getUse1SERule(...)\n",
      "     |      getUse1SERule() -> retval\n",
      "     |      .   @see setUse1SERule\n",
      "     |  \n",
      "     |  getUseSurrogates(...)\n",
      "     |      getUseSurrogates() -> retval\n",
      "     |      .   @see setUseSurrogates\n",
      "     |  \n",
      "     |  setCVFolds(...)\n",
      "     |      setCVFolds(val) -> None\n",
      "     |      .   @copybrief getCVFolds @see getCVFolds\n",
      "     |  \n",
      "     |  setMaxCategories(...)\n",
      "     |      setMaxCategories(val) -> None\n",
      "     |      .   @copybrief getMaxCategories @see getMaxCategories\n",
      "     |  \n",
      "     |  setMaxDepth(...)\n",
      "     |      setMaxDepth(val) -> None\n",
      "     |      .   @copybrief getMaxDepth @see getMaxDepth\n",
      "     |  \n",
      "     |  setMinSampleCount(...)\n",
      "     |      setMinSampleCount(val) -> None\n",
      "     |      .   @copybrief getMinSampleCount @see getMinSampleCount\n",
      "     |  \n",
      "     |  setPriors(...)\n",
      "     |      setPriors(val) -> None\n",
      "     |      .   @copybrief getPriors @see getPriors\n",
      "     |  \n",
      "     |  setRegressionAccuracy(...)\n",
      "     |      setRegressionAccuracy(val) -> None\n",
      "     |      .   @copybrief getRegressionAccuracy @see getRegressionAccuracy\n",
      "     |  \n",
      "     |  setTruncatePrunedTree(...)\n",
      "     |      setTruncatePrunedTree(val) -> None\n",
      "     |      .   @copybrief getTruncatePrunedTree @see getTruncatePrunedTree\n",
      "     |  \n",
      "     |  setUse1SERule(...)\n",
      "     |      setUse1SERule(val) -> None\n",
      "     |      .   @copybrief getUse1SERule @see getUse1SERule\n",
      "     |  \n",
      "     |  setUseSurrogates(...)\n",
      "     |      setUseSurrogates(val) -> None\n",
      "     |      .   @copybrief getUseSurrogates @see getUseSurrogates\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_DTrees(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_DTrees\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getCVFolds(...)\n",
      "     |      getCVFolds() -> retval\n",
      "     |      .   @see setCVFolds\n",
      "     |  \n",
      "     |  getMaxCategories(...)\n",
      "     |      getMaxCategories() -> retval\n",
      "     |      .   @see setMaxCategories\n",
      "     |  \n",
      "     |  getMaxDepth(...)\n",
      "     |      getMaxDepth() -> retval\n",
      "     |      .   @see setMaxDepth\n",
      "     |  \n",
      "     |  getMinSampleCount(...)\n",
      "     |      getMinSampleCount() -> retval\n",
      "     |      .   @see setMinSampleCount\n",
      "     |  \n",
      "     |  getPriors(...)\n",
      "     |      getPriors() -> retval\n",
      "     |      .   @see setPriors\n",
      "     |  \n",
      "     |  getRegressionAccuracy(...)\n",
      "     |      getRegressionAccuracy() -> retval\n",
      "     |      .   @see setRegressionAccuracy\n",
      "     |  \n",
      "     |  getTruncatePrunedTree(...)\n",
      "     |      getTruncatePrunedTree() -> retval\n",
      "     |      .   @see setTruncatePrunedTree\n",
      "     |  \n",
      "     |  getUse1SERule(...)\n",
      "     |      getUse1SERule() -> retval\n",
      "     |      .   @see setUse1SERule\n",
      "     |  \n",
      "     |  getUseSurrogates(...)\n",
      "     |      getUseSurrogates() -> retval\n",
      "     |      .   @see setUseSurrogates\n",
      "     |  \n",
      "     |  setCVFolds(...)\n",
      "     |      setCVFolds(val) -> None\n",
      "     |      .   @copybrief getCVFolds @see getCVFolds\n",
      "     |  \n",
      "     |  setMaxCategories(...)\n",
      "     |      setMaxCategories(val) -> None\n",
      "     |      .   @copybrief getMaxCategories @see getMaxCategories\n",
      "     |  \n",
      "     |  setMaxDepth(...)\n",
      "     |      setMaxDepth(val) -> None\n",
      "     |      .   @copybrief getMaxDepth @see getMaxDepth\n",
      "     |  \n",
      "     |  setMinSampleCount(...)\n",
      "     |      setMinSampleCount(val) -> None\n",
      "     |      .   @copybrief getMinSampleCount @see getMinSampleCount\n",
      "     |  \n",
      "     |  setPriors(...)\n",
      "     |      setPriors(val) -> None\n",
      "     |      .   @copybrief getPriors @see getPriors\n",
      "     |  \n",
      "     |  setRegressionAccuracy(...)\n",
      "     |      setRegressionAccuracy(val) -> None\n",
      "     |      .   @copybrief getRegressionAccuracy @see getRegressionAccuracy\n",
      "     |  \n",
      "     |  setTruncatePrunedTree(...)\n",
      "     |      setTruncatePrunedTree(val) -> None\n",
      "     |      .   @copybrief getTruncatePrunedTree @see getTruncatePrunedTree\n",
      "     |  \n",
      "     |  setUse1SERule(...)\n",
      "     |      setUse1SERule(val) -> None\n",
      "     |      .   @copybrief getUse1SERule @see getUse1SERule\n",
      "     |  \n",
      "     |  setUseSurrogates(...)\n",
      "     |      setUseSurrogates(val) -> None\n",
      "     |      .   @copybrief getUseSurrogates @see getUseSurrogates\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates the empty model\n",
      "     |      .   \n",
      "     |      .       The static method creates empty decision tree with the specified parameters. It should be then\n",
      "     |      .       trained using train method (see StatModel::train). Alternatively, you can load the model from\n",
      "     |      .       file using Algorithm::load\\<DTrees\\>(filename).\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized DTrees from a file\n",
      "     |      .        *\n",
      "     |      .        * Use DTree::save to serialize and store an DTree to disk.\n",
      "     |      .        * Load the DTree from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized DTree\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_EM(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_EM\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getClustersNumber(...)\n",
      "     |      getClustersNumber() -> retval\n",
      "     |      .   @see setClustersNumber\n",
      "     |  \n",
      "     |  getCovarianceMatrixType(...)\n",
      "     |      getCovarianceMatrixType() -> retval\n",
      "     |      .   @see setCovarianceMatrixType\n",
      "     |  \n",
      "     |  getCovs(...)\n",
      "     |      getCovs([, covs]) -> covs\n",
      "     |      .   @brief Returns covariation matrices\n",
      "     |      .   \n",
      "     |      .       Returns vector of covariation matrices. Number of matrices is the number of gaussian mixtures,\n",
      "     |      .       each matrix is a square floating-point matrix NxN, where N is the space dimensionality.\n",
      "     |  \n",
      "     |  getMeans(...)\n",
      "     |      getMeans() -> retval\n",
      "     |      .   @brief Returns the cluster centers (means of the Gaussian mixture)\n",
      "     |      .   \n",
      "     |      .       Returns matrix with the number of rows equal to the number of mixtures and number of columns\n",
      "     |      .       equal to the space dimensionality.\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getWeights(...)\n",
      "     |      getWeights() -> retval\n",
      "     |      .   @brief Returns weights of the mixtures\n",
      "     |      .   \n",
      "     |      .       Returns vector with the number of elements equal to the number of mixtures.\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Returns posterior probabilities for the provided samples\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output \\f$ nSamples \\times nClusters\\f$ matrix of results. It contains\n",
      "     |      .       posterior probabilities for each sample from the input\n",
      "     |      .       @param flags This parameter will be ignored\n",
      "     |  \n",
      "     |  predict2(...)\n",
      "     |      predict2(sample[, probs]) -> retval, probs\n",
      "     |      .   @brief Returns a likelihood logarithm value and an index of the most probable mixture component\n",
      "     |      .       for the given sample.\n",
      "     |      .   \n",
      "     |      .       @param sample A sample for classification. It should be a one-channel matrix of\n",
      "     |      .           \\f$1 \\times dims\\f$ or \\f$dims \\times 1\\f$ size.\n",
      "     |      .       @param probs Optional output matrix that contains posterior probabilities of each component\n",
      "     |      .           given the sample. It has \\f$1 \\times nclusters\\f$ size and CV_64FC1 type.\n",
      "     |      .   \n",
      "     |      .       The method returns a two-element double vector. Zero element is a likelihood logarithm value for\n",
      "     |      .       the sample. First element is an index of the most probable mixture component for the given\n",
      "     |      .       sample.\n",
      "     |  \n",
      "     |  setClustersNumber(...)\n",
      "     |      setClustersNumber(val) -> None\n",
      "     |      .   @copybrief getClustersNumber @see getClustersNumber\n",
      "     |  \n",
      "     |  setCovarianceMatrixType(...)\n",
      "     |      setCovarianceMatrixType(val) -> None\n",
      "     |      .   @copybrief getCovarianceMatrixType @see getCovarianceMatrixType\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  trainE(...)\n",
      "     |      trainE(samples, means0[, covs0[, weights0[, logLikelihoods[, labels[, probs]]]]]) -> retval, logLikelihoods, labels, probs\n",
      "     |      .   @brief Estimate the Gaussian mixture parameters from a samples set.\n",
      "     |      .   \n",
      "     |      .       This variation starts with Expectation step. You need to provide initial means \\f$a_k\\f$ of\n",
      "     |      .       mixture components. Optionally you can pass initial weights \\f$\\pi_k\\f$ and covariance matrices\n",
      "     |      .       \\f$S_k\\f$ of mixture components.\n",
      "     |      .   \n",
      "     |      .       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n",
      "     |      .           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n",
      "     |      .           it will be converted to the inner matrix of such type for the further computing.\n",
      "     |      .       @param means0 Initial means \\f$a_k\\f$ of mixture components. It is a one-channel matrix of\n",
      "     |      .           \\f$nclusters \\times dims\\f$ size. If the matrix does not have CV_64F type it will be\n",
      "     |      .           converted to the inner matrix of such type for the further computing.\n",
      "     |      .       @param covs0 The vector of initial covariance matrices \\f$S_k\\f$ of mixture components. Each of\n",
      "     |      .           covariance matrices is a one-channel matrix of \\f$dims \\times dims\\f$ size. If the matrices\n",
      "     |      .           do not have CV_64F type they will be converted to the inner matrices of such type for the\n",
      "     |      .           further computing.\n",
      "     |      .       @param weights0 Initial weights \\f$\\pi_k\\f$ of mixture components. It should be a one-channel\n",
      "     |      .           floating-point matrix with \\f$1 \\times nclusters\\f$ or \\f$nclusters \\times 1\\f$ size.\n",
      "     |      .       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n",
      "     |      .           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n",
      "     |      .       @param labels The optional output \"class label\" for each sample:\n",
      "     |      .           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n",
      "     |      .           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n",
      "     |      .       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n",
      "     |      .           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n",
      "     |      .           CV_64FC1 type.\n",
      "     |  \n",
      "     |  trainEM(...)\n",
      "     |      trainEM(samples[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n",
      "     |      .   @brief Estimate the Gaussian mixture parameters from a samples set.\n",
      "     |      .   \n",
      "     |      .       This variation starts with Expectation step. Initial values of the model parameters will be\n",
      "     |      .       estimated by the k-means algorithm.\n",
      "     |      .   \n",
      "     |      .       Unlike many of the ML models, %EM is an unsupervised learning algorithm and it does not take\n",
      "     |      .       responses (class labels or function values) as input. Instead, it computes the *Maximum\n",
      "     |      .       Likelihood Estimate* of the Gaussian mixture parameters from an input sample set, stores all the\n",
      "     |      .       parameters inside the structure: \\f$p_{i,k}\\f$ in probs, \\f$a_k\\f$ in means , \\f$S_k\\f$ in\n",
      "     |      .       covs[k], \\f$\\pi_k\\f$ in weights , and optionally computes the output \"class label\" for each\n",
      "     |      .       sample: \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most\n",
      "     |      .       probable mixture component for each sample).\n",
      "     |      .   \n",
      "     |      .       The trained model can be used further for prediction, just like any other classifier. The\n",
      "     |      .       trained model is similar to the NormalBayesClassifier.\n",
      "     |      .   \n",
      "     |      .       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n",
      "     |      .           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n",
      "     |      .           it will be converted to the inner matrix of such type for the further computing.\n",
      "     |      .       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n",
      "     |      .           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n",
      "     |      .       @param labels The optional output \"class label\" for each sample:\n",
      "     |      .           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n",
      "     |      .           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n",
      "     |      .       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n",
      "     |      .           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n",
      "     |      .           CV_64FC1 type.\n",
      "     |  \n",
      "     |  trainM(...)\n",
      "     |      trainM(samples, probs0[, logLikelihoods[, labels[, probs]]]) -> retval, logLikelihoods, labels, probs\n",
      "     |      .   @brief Estimate the Gaussian mixture parameters from a samples set.\n",
      "     |      .   \n",
      "     |      .       This variation starts with Maximization step. You need to provide initial probabilities\n",
      "     |      .       \\f$p_{i,k}\\f$ to use this option.\n",
      "     |      .   \n",
      "     |      .       @param samples Samples from which the Gaussian mixture model will be estimated. It should be a\n",
      "     |      .           one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type\n",
      "     |      .           it will be converted to the inner matrix of such type for the further computing.\n",
      "     |      .       @param probs0 the probabilities\n",
      "     |      .       @param logLikelihoods The optional output matrix that contains a likelihood logarithm value for\n",
      "     |      .           each sample. It has \\f$nsamples \\times 1\\f$ size and CV_64FC1 type.\n",
      "     |      .       @param labels The optional output \"class label\" for each sample:\n",
      "     |      .           \\f$\\texttt{labels}_i=\\texttt{arg max}_k(p_{i,k}), i=1..N\\f$ (indices of the most probable\n",
      "     |      .           mixture component for each sample). It has \\f$nsamples \\times 1\\f$ size and CV_32SC1 type.\n",
      "     |      .       @param probs The optional output matrix that contains posterior probabilities of each Gaussian\n",
      "     |      .           mixture component given the each sample. It has \\f$nsamples \\times nclusters\\f$ size and\n",
      "     |      .           CV_64FC1 type.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   Creates empty %EM model.\n",
      "     |      .       The model should be trained then using StatModel::train(traindata, flags) method. Alternatively, you\n",
      "     |      .       can use one of the EM::train\\* methods or load it from file using Algorithm::load\\<EM\\>(filename).\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized EM from a file\n",
      "     |      .        *\n",
      "     |      .        * Use EM::save to serialize and store an EM to disk.\n",
      "     |      .        * Load the EM from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized EM\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_KNearest(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_KNearest\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  findNearest(...)\n",
      "     |      findNearest(samples, k[, results[, neighborResponses[, dist]]]) -> retval, results, neighborResponses, dist\n",
      "     |      .   @brief Finds the neighbors and predicts responses for input vectors.\n",
      "     |      .   \n",
      "     |      .       @param samples Input samples stored by rows. It is a single-precision floating-point matrix of\n",
      "     |      .           `<number_of_samples> * k` size.\n",
      "     |      .       @param k Number of used nearest neighbors. Should be greater than 1.\n",
      "     |      .       @param results Vector with results of prediction (regression or classification) for each input\n",
      "     |      .           sample. It is a single-precision floating-point vector with `<number_of_samples>` elements.\n",
      "     |      .       @param neighborResponses Optional output values for corresponding neighbors. It is a single-\n",
      "     |      .           precision floating-point matrix of `<number_of_samples> * k` size.\n",
      "     |      .       @param dist Optional output distances from the input vectors to the corresponding neighbors. It\n",
      "     |      .           is a single-precision floating-point matrix of `<number_of_samples> * k` size.\n",
      "     |      .   \n",
      "     |      .       For each input vector (a row of the matrix samples), the method finds the k nearest neighbors.\n",
      "     |      .       In case of regression, the predicted result is a mean value of the particular vector's neighbor\n",
      "     |      .       responses. In case of classification, the class is determined by voting.\n",
      "     |      .   \n",
      "     |      .       For each input vector, the neighbors are sorted by their distances to the vector.\n",
      "     |      .   \n",
      "     |      .       In case of C++ interface you can use output pointers to empty matrices and the function will\n",
      "     |      .       allocate memory itself.\n",
      "     |      .   \n",
      "     |      .       If only a single input vector is passed, all output matrices are optional and the predicted\n",
      "     |      .       value is returned by the method.\n",
      "     |      .   \n",
      "     |      .       The function is parallelized with the TBB library.\n",
      "     |  \n",
      "     |  getAlgorithmType(...)\n",
      "     |      getAlgorithmType() -> retval\n",
      "     |      .   @see setAlgorithmType\n",
      "     |  \n",
      "     |  getDefaultK(...)\n",
      "     |      getDefaultK() -> retval\n",
      "     |      .   @see setDefaultK\n",
      "     |  \n",
      "     |  getEmax(...)\n",
      "     |      getEmax() -> retval\n",
      "     |      .   @see setEmax\n",
      "     |  \n",
      "     |  getIsClassifier(...)\n",
      "     |      getIsClassifier() -> retval\n",
      "     |      .   @see setIsClassifier\n",
      "     |  \n",
      "     |  setAlgorithmType(...)\n",
      "     |      setAlgorithmType(val) -> None\n",
      "     |      .   @copybrief getAlgorithmType @see getAlgorithmType\n",
      "     |  \n",
      "     |  setDefaultK(...)\n",
      "     |      setDefaultK(val) -> None\n",
      "     |      .   @copybrief getDefaultK @see getDefaultK\n",
      "     |  \n",
      "     |  setEmax(...)\n",
      "     |      setEmax(val) -> None\n",
      "     |      .   @copybrief getEmax @see getEmax\n",
      "     |  \n",
      "     |  setIsClassifier(...)\n",
      "     |      setIsClassifier(val) -> None\n",
      "     |      .   @copybrief getIsClassifier @see getIsClassifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates the empty model\n",
      "     |      .   \n",
      "     |      .       The static method creates empty %KNearest classifier. It should be then trained using StatModel::train method.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath) -> retval\n",
      "     |      .   @brief Loads and creates a serialized knearest from a file\n",
      "     |      .        *\n",
      "     |      .        * Use KNearest::save to serialize and store an KNearest to disk.\n",
      "     |      .        * Load the KNearest from this file again, by calling this function with the path to the file.\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized KNearest\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_LogisticRegression(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_LogisticRegression\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getIterations(...)\n",
      "     |      getIterations() -> retval\n",
      "     |      .   @see setIterations\n",
      "     |  \n",
      "     |  getLearningRate(...)\n",
      "     |      getLearningRate() -> retval\n",
      "     |      .   @see setLearningRate\n",
      "     |  \n",
      "     |  getMiniBatchSize(...)\n",
      "     |      getMiniBatchSize() -> retval\n",
      "     |      .   @see setMiniBatchSize\n",
      "     |  \n",
      "     |  getRegularization(...)\n",
      "     |      getRegularization() -> retval\n",
      "     |      .   @see setRegularization\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getTrainMethod(...)\n",
      "     |      getTrainMethod() -> retval\n",
      "     |      .   @see setTrainMethod\n",
      "     |  \n",
      "     |  get_learnt_thetas(...)\n",
      "     |      get_learnt_thetas() -> retval\n",
      "     |      .   @brief This function returns the trained parameters arranged across rows.\n",
      "     |      .   \n",
      "     |      .       For a two class classifcation problem, it returns a row matrix. It returns learnt parameters of\n",
      "     |      .       the Logistic Regression as a matrix of type CV_32F.\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts responses for input samples and returns a float type.\n",
      "     |      .   \n",
      "     |      .       @param samples The input data for the prediction algorithm. Matrix [m x n], where each row\n",
      "     |      .           contains variables (features) of one object being classified. Should have data type CV_32F.\n",
      "     |      .       @param results Predicted labels as a column matrix of type CV_32S.\n",
      "     |      .       @param flags Not used.\n",
      "     |  \n",
      "     |  setIterations(...)\n",
      "     |      setIterations(val) -> None\n",
      "     |      .   @copybrief getIterations @see getIterations\n",
      "     |  \n",
      "     |  setLearningRate(...)\n",
      "     |      setLearningRate(val) -> None\n",
      "     |      .   @copybrief getLearningRate @see getLearningRate\n",
      "     |  \n",
      "     |  setMiniBatchSize(...)\n",
      "     |      setMiniBatchSize(val) -> None\n",
      "     |      .   @copybrief getMiniBatchSize @see getMiniBatchSize\n",
      "     |  \n",
      "     |  setRegularization(...)\n",
      "     |      setRegularization(val) -> None\n",
      "     |      .   @copybrief getRegularization @see getRegularization\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  setTrainMethod(...)\n",
      "     |      setTrainMethod(val) -> None\n",
      "     |      .   @copybrief getTrainMethod @see getTrainMethod\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates empty model.\n",
      "     |      .   \n",
      "     |      .       Creates Logistic Regression model with parameters given.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized LogisticRegression from a file\n",
      "     |      .        *\n",
      "     |      .        * Use LogisticRegression::save to serialize and store an LogisticRegression to disk.\n",
      "     |      .        * Load the LogisticRegression from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized LogisticRegression\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_NormalBayesClassifier(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_NormalBayesClassifier\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  predictProb(...)\n",
      "     |      predictProb(inputs[, outputs[, outputProbs[, flags]]]) -> retval, outputs, outputProbs\n",
      "     |      .   @brief Predicts the response for sample(s).\n",
      "     |      .   \n",
      "     |      .       The method estimates the most probable classes for input vectors. Input vectors (one or more)\n",
      "     |      .       are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one\n",
      "     |      .       output vector outputs. The predicted class for a single input vector is returned by the method.\n",
      "     |      .       The vector outputProbs contains the output probabilities corresponding to each element of\n",
      "     |      .       result.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   Creates empty model\n",
      "     |      .   Use StatModel::train to train the model after creation.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized NormalBayesClassifier from a file\n",
      "     |      .        *\n",
      "     |      .        * Use NormalBayesClassifier::save to serialize and store an NormalBayesClassifier to disk.\n",
      "     |      .        * Load the NormalBayesClassifier from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized NormalBayesClassifier\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_ParamGrid(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create([, minVal[, maxVal[, logstep]]]) -> retval\n",
      "     |      .   @brief Creates a ParamGrid Ptr that can be given to the %SVM::trainAuto method\n",
      "     |      .   \n",
      "     |      .       @param minVal minimum value of the parameter grid\n",
      "     |      .       @param maxVal maximum value of the parameter grid\n",
      "     |      .       @param logstep Logarithmic step for iterating the statmodel parameter\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  logStep\n",
      "     |      logStep\n",
      "     |  \n",
      "     |  maxVal\n",
      "     |      maxVal\n",
      "     |  \n",
      "     |  minVal\n",
      "     |      minVal\n",
      "    \n",
      "    class ml_RTrees(ml_DTrees)\n",
      "     |  Method resolution order:\n",
      "     |      ml_RTrees\n",
      "     |      ml_DTrees\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getActiveVarCount(...)\n",
      "     |      getActiveVarCount() -> retval\n",
      "     |      .   @see setActiveVarCount\n",
      "     |  \n",
      "     |  getCalculateVarImportance(...)\n",
      "     |      getCalculateVarImportance() -> retval\n",
      "     |      .   @see setCalculateVarImportance\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getVarImportance(...)\n",
      "     |      getVarImportance() -> retval\n",
      "     |      .   Returns the variable importance array.\n",
      "     |      .       The method returns the variable importance vector, computed at the training stage when\n",
      "     |      .       CalculateVarImportance is set to true. If this flag was set to false, the empty matrix is\n",
      "     |      .       returned.\n",
      "     |  \n",
      "     |  getVotes(...)\n",
      "     |      getVotes(samples, flags[, results]) -> results\n",
      "     |      .   Returns the result of each individual tree in the forest.\n",
      "     |      .       In case the model is a regression problem, the method will return each of the trees'\n",
      "     |      .       results for each of the sample cases. If the model is a classifier, it will return\n",
      "     |      .       a Mat with samples + 1 rows, where the first row gives the class number and the\n",
      "     |      .       following rows return the votes each class had for each sample.\n",
      "     |      .           @param samples Array containing the samples for which votes will be calculated.\n",
      "     |      .           @param results Array where the result of the calculation will be written.\n",
      "     |      .           @param flags Flags for defining the type of RTrees.\n",
      "     |  \n",
      "     |  setActiveVarCount(...)\n",
      "     |      setActiveVarCount(val) -> None\n",
      "     |      .   @copybrief getActiveVarCount @see getActiveVarCount\n",
      "     |  \n",
      "     |  setCalculateVarImportance(...)\n",
      "     |      setCalculateVarImportance(val) -> None\n",
      "     |      .   @copybrief getCalculateVarImportance @see getCalculateVarImportance\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   Creates the empty model.\n",
      "     |      .       Use StatModel::train to train the model, StatModel::train to create and train the model,\n",
      "     |      .       Algorithm::load to load the pre-trained model.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized RTree from a file\n",
      "     |      .        *\n",
      "     |      .        * Use RTree::save to serialize and store an RTree to disk.\n",
      "     |      .        * Load the RTree from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized RTree\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_DTrees:\n",
      "     |  \n",
      "     |  getCVFolds(...)\n",
      "     |      getCVFolds() -> retval\n",
      "     |      .   @see setCVFolds\n",
      "     |  \n",
      "     |  getMaxCategories(...)\n",
      "     |      getMaxCategories() -> retval\n",
      "     |      .   @see setMaxCategories\n",
      "     |  \n",
      "     |  getMaxDepth(...)\n",
      "     |      getMaxDepth() -> retval\n",
      "     |      .   @see setMaxDepth\n",
      "     |  \n",
      "     |  getMinSampleCount(...)\n",
      "     |      getMinSampleCount() -> retval\n",
      "     |      .   @see setMinSampleCount\n",
      "     |  \n",
      "     |  getPriors(...)\n",
      "     |      getPriors() -> retval\n",
      "     |      .   @see setPriors\n",
      "     |  \n",
      "     |  getRegressionAccuracy(...)\n",
      "     |      getRegressionAccuracy() -> retval\n",
      "     |      .   @see setRegressionAccuracy\n",
      "     |  \n",
      "     |  getTruncatePrunedTree(...)\n",
      "     |      getTruncatePrunedTree() -> retval\n",
      "     |      .   @see setTruncatePrunedTree\n",
      "     |  \n",
      "     |  getUse1SERule(...)\n",
      "     |      getUse1SERule() -> retval\n",
      "     |      .   @see setUse1SERule\n",
      "     |  \n",
      "     |  getUseSurrogates(...)\n",
      "     |      getUseSurrogates() -> retval\n",
      "     |      .   @see setUseSurrogates\n",
      "     |  \n",
      "     |  setCVFolds(...)\n",
      "     |      setCVFolds(val) -> None\n",
      "     |      .   @copybrief getCVFolds @see getCVFolds\n",
      "     |  \n",
      "     |  setMaxCategories(...)\n",
      "     |      setMaxCategories(val) -> None\n",
      "     |      .   @copybrief getMaxCategories @see getMaxCategories\n",
      "     |  \n",
      "     |  setMaxDepth(...)\n",
      "     |      setMaxDepth(val) -> None\n",
      "     |      .   @copybrief getMaxDepth @see getMaxDepth\n",
      "     |  \n",
      "     |  setMinSampleCount(...)\n",
      "     |      setMinSampleCount(val) -> None\n",
      "     |      .   @copybrief getMinSampleCount @see getMinSampleCount\n",
      "     |  \n",
      "     |  setPriors(...)\n",
      "     |      setPriors(val) -> None\n",
      "     |      .   @copybrief getPriors @see getPriors\n",
      "     |  \n",
      "     |  setRegressionAccuracy(...)\n",
      "     |      setRegressionAccuracy(val) -> None\n",
      "     |      .   @copybrief getRegressionAccuracy @see getRegressionAccuracy\n",
      "     |  \n",
      "     |  setTruncatePrunedTree(...)\n",
      "     |      setTruncatePrunedTree(val) -> None\n",
      "     |      .   @copybrief getTruncatePrunedTree @see getTruncatePrunedTree\n",
      "     |  \n",
      "     |  setUse1SERule(...)\n",
      "     |      setUse1SERule(val) -> None\n",
      "     |      .   @copybrief getUse1SERule @see getUse1SERule\n",
      "     |  \n",
      "     |  setUseSurrogates(...)\n",
      "     |      setUseSurrogates(val) -> None\n",
      "     |      .   @copybrief getUseSurrogates @see getUseSurrogates\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_SVM(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_SVM\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getC(...)\n",
      "     |      getC() -> retval\n",
      "     |      .   @see setC\n",
      "     |  \n",
      "     |  getClassWeights(...)\n",
      "     |      getClassWeights() -> retval\n",
      "     |      .   @see setClassWeights\n",
      "     |  \n",
      "     |  getCoef0(...)\n",
      "     |      getCoef0() -> retval\n",
      "     |      .   @see setCoef0\n",
      "     |  \n",
      "     |  getDecisionFunction(...)\n",
      "     |      getDecisionFunction(i[, alpha[, svidx]]) -> retval, alpha, svidx\n",
      "     |      .   @brief Retrieves the decision function\n",
      "     |      .   \n",
      "     |      .       @param i the index of the decision function. If the problem solved is regression, 1-class or\n",
      "     |      .           2-class classification, then there will be just one decision function and the index should\n",
      "     |      .           always be 0. Otherwise, in the case of N-class classification, there will be \\f$N(N-1)/2\\f$\n",
      "     |      .           decision functions.\n",
      "     |      .       @param alpha the optional output vector for weights, corresponding to different support vectors.\n",
      "     |      .           In the case of linear %SVM all the alpha's will be 1's.\n",
      "     |      .       @param svidx the optional output vector of indices of support vectors within the matrix of\n",
      "     |      .           support vectors (which can be retrieved by SVM::getSupportVectors). In the case of linear\n",
      "     |      .           %SVM each decision function consists of a single \"compressed\" support vector.\n",
      "     |      .   \n",
      "     |      .       The method returns rho parameter of the decision function, a scalar subtracted from the weighted\n",
      "     |      .       sum of kernel responses.\n",
      "     |  \n",
      "     |  getDegree(...)\n",
      "     |      getDegree() -> retval\n",
      "     |      .   @see setDegree\n",
      "     |  \n",
      "     |  getGamma(...)\n",
      "     |      getGamma() -> retval\n",
      "     |      .   @see setGamma\n",
      "     |  \n",
      "     |  getKernelType(...)\n",
      "     |      getKernelType() -> retval\n",
      "     |      .   Type of a %SVM kernel.\n",
      "     |      .   See SVM::KernelTypes. Default value is SVM::RBF.\n",
      "     |  \n",
      "     |  getNu(...)\n",
      "     |      getNu() -> retval\n",
      "     |      .   @see setNu\n",
      "     |  \n",
      "     |  getP(...)\n",
      "     |      getP() -> retval\n",
      "     |      .   @see setP\n",
      "     |  \n",
      "     |  getSupportVectors(...)\n",
      "     |      getSupportVectors() -> retval\n",
      "     |      .   @brief Retrieves all the support vectors\n",
      "     |      .   \n",
      "     |      .       The method returns all the support vectors as a floating-point matrix, where support vectors are\n",
      "     |      .       stored as matrix rows.\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getType(...)\n",
      "     |      getType() -> retval\n",
      "     |      .   @see setType\n",
      "     |  \n",
      "     |  getUncompressedSupportVectors(...)\n",
      "     |      getUncompressedSupportVectors() -> retval\n",
      "     |      .   @brief Retrieves all the uncompressed support vectors of a linear %SVM\n",
      "     |      .   \n",
      "     |      .       The method returns all the uncompressed support vectors of a linear %SVM that the compressed\n",
      "     |      .       support vector, used for prediction, was derived from. They are returned in a floating-point\n",
      "     |      .       matrix, where the support vectors are stored as matrix rows.\n",
      "     |  \n",
      "     |  setC(...)\n",
      "     |      setC(val) -> None\n",
      "     |      .   @copybrief getC @see getC\n",
      "     |  \n",
      "     |  setClassWeights(...)\n",
      "     |      setClassWeights(val) -> None\n",
      "     |      .   @copybrief getClassWeights @see getClassWeights\n",
      "     |  \n",
      "     |  setCoef0(...)\n",
      "     |      setCoef0(val) -> None\n",
      "     |      .   @copybrief getCoef0 @see getCoef0\n",
      "     |  \n",
      "     |  setDegree(...)\n",
      "     |      setDegree(val) -> None\n",
      "     |      .   @copybrief getDegree @see getDegree\n",
      "     |  \n",
      "     |  setGamma(...)\n",
      "     |      setGamma(val) -> None\n",
      "     |      .   @copybrief getGamma @see getGamma\n",
      "     |  \n",
      "     |  setKernel(...)\n",
      "     |      setKernel(kernelType) -> None\n",
      "     |      .   Initialize with one of predefined kernels.\n",
      "     |      .   See SVM::KernelTypes.\n",
      "     |  \n",
      "     |  setNu(...)\n",
      "     |      setNu(val) -> None\n",
      "     |      .   @copybrief getNu @see getNu\n",
      "     |  \n",
      "     |  setP(...)\n",
      "     |      setP(val) -> None\n",
      "     |      .   @copybrief getP @see getP\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  setType(...)\n",
      "     |      setType(val) -> None\n",
      "     |      .   @copybrief getType @see getType\n",
      "     |  \n",
      "     |  trainAuto(...)\n",
      "     |      trainAuto(samples, layout, responses[, kFold[, Cgrid[, gammaGrid[, pGrid[, nuGrid[, coeffGrid[, degreeGrid[, balanced]]]]]]]]) -> retval\n",
      "     |      .   @brief Trains an %SVM with optimal parameters\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |      .       @param kFold Cross-validation parameter. The training set is divided into kFold subsets. One\n",
      "     |      .           subset is used to test the model, the others form the train set. So, the %SVM algorithm is\n",
      "     |      .       @param Cgrid grid for C\n",
      "     |      .       @param gammaGrid grid for gamma\n",
      "     |      .       @param pGrid grid for p\n",
      "     |      .       @param nuGrid grid for nu\n",
      "     |      .       @param coeffGrid grid for coeff\n",
      "     |      .       @param degreeGrid grid for degree\n",
      "     |      .       @param balanced If true and the problem is 2-class classification then the method creates more\n",
      "     |      .           balanced cross-validation subsets that is proportions between classes in subsets are close\n",
      "     |      .           to such proportion in the whole train dataset.\n",
      "     |      .   \n",
      "     |      .       The method trains the %SVM model automatically by choosing the optimal parameters C, gamma, p,\n",
      "     |      .       nu, coef0, degree. Parameters are considered optimal when the cross-validation\n",
      "     |      .       estimate of the test set error is minimal.\n",
      "     |      .   \n",
      "     |      .       This function only makes use of SVM::getDefaultGrid for parameter optimization and thus only\n",
      "     |      .       offers rudimentary parameter options.\n",
      "     |      .   \n",
      "     |      .       This function works for the classification (SVM::C_SVC or SVM::NU_SVC) as well as for the\n",
      "     |      .       regression (SVM::EPS_SVR or SVM::NU_SVR). If it is SVM::ONE_CLASS, no optimization is made and\n",
      "     |      .       the usual %SVM with parameters specified in params is executed.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   Creates empty model.\n",
      "     |      .       Use StatModel::train to train the model. Since %SVM has several parameters, you may want to\n",
      "     |      .   find the best parameters for your problem, it can be done with SVM::trainAuto.\n",
      "     |  \n",
      "     |  getDefaultGridPtr(...)\n",
      "     |      getDefaultGridPtr(param_id) -> retval\n",
      "     |      .   @brief Generates a grid for %SVM parameters.\n",
      "     |      .   \n",
      "     |      .       @param param_id %SVM parameters IDs that must be one of the SVM::ParamTypes. The grid is\n",
      "     |      .       generated for the parameter with this ID.\n",
      "     |      .   \n",
      "     |      .       The function generates a grid pointer for the specified parameter of the %SVM algorithm.\n",
      "     |      .       The grid may be passed to the function SVM::trainAuto.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath) -> retval\n",
      "     |      .   @brief Loads and creates a serialized svm from a file\n",
      "     |      .        *\n",
      "     |      .        * Use SVM::save to serialize and store an SVM to disk.\n",
      "     |      .        * Load the SVM from this file again, by calling this function with the path to the file.\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized svm\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_SVMSGD(ml_StatModel)\n",
      "     |  Method resolution order:\n",
      "     |      ml_SVMSGD\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getInitialStepSize(...)\n",
      "     |      getInitialStepSize() -> retval\n",
      "     |      .   @see setInitialStepSize\n",
      "     |  \n",
      "     |  getMarginRegularization(...)\n",
      "     |      getMarginRegularization() -> retval\n",
      "     |      .   @see setMarginRegularization\n",
      "     |  \n",
      "     |  getMarginType(...)\n",
      "     |      getMarginType() -> retval\n",
      "     |      .   @see setMarginType\n",
      "     |  \n",
      "     |  getShift(...)\n",
      "     |      getShift() -> retval\n",
      "     |      .   * @return the shift of the trained model (decision function f(x) = weights * x + shift).\n",
      "     |  \n",
      "     |  getStepDecreasingPower(...)\n",
      "     |      getStepDecreasingPower() -> retval\n",
      "     |      .   @see setStepDecreasingPower\n",
      "     |  \n",
      "     |  getSvmsgdType(...)\n",
      "     |      getSvmsgdType() -> retval\n",
      "     |      .   @see setSvmsgdType\n",
      "     |  \n",
      "     |  getTermCriteria(...)\n",
      "     |      getTermCriteria() -> retval\n",
      "     |      .   @see setTermCriteria\n",
      "     |  \n",
      "     |  getWeights(...)\n",
      "     |      getWeights() -> retval\n",
      "     |      .   * @return the weights of the trained model (decision function f(x) = weights * x + shift).\n",
      "     |  \n",
      "     |  setInitialStepSize(...)\n",
      "     |      setInitialStepSize(InitialStepSize) -> None\n",
      "     |      .   @copybrief getInitialStepSize @see getInitialStepSize\n",
      "     |  \n",
      "     |  setMarginRegularization(...)\n",
      "     |      setMarginRegularization(marginRegularization) -> None\n",
      "     |      .   @copybrief getMarginRegularization @see getMarginRegularization\n",
      "     |  \n",
      "     |  setMarginType(...)\n",
      "     |      setMarginType(marginType) -> None\n",
      "     |      .   @copybrief getMarginType @see getMarginType\n",
      "     |  \n",
      "     |  setOptimalParameters(...)\n",
      "     |      setOptimalParameters([, svmsgdType[, marginType]]) -> None\n",
      "     |      .   @brief Function sets optimal parameters values for chosen SVM SGD model.\n",
      "     |      .        * @param svmsgdType is the type of SVMSGD classifier.\n",
      "     |      .        * @param marginType is the type of margin constraint.\n",
      "     |  \n",
      "     |  setStepDecreasingPower(...)\n",
      "     |      setStepDecreasingPower(stepDecreasingPower) -> None\n",
      "     |      .   @copybrief getStepDecreasingPower @see getStepDecreasingPower\n",
      "     |  \n",
      "     |  setSvmsgdType(...)\n",
      "     |      setSvmsgdType(svmsgdType) -> None\n",
      "     |      .   @copybrief getSvmsgdType @see getSvmsgdType\n",
      "     |  \n",
      "     |  setTermCriteria(...)\n",
      "     |      setTermCriteria(val) -> None\n",
      "     |      .   @copybrief getTermCriteria @see getTermCriteria\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create() -> retval\n",
      "     |      .   @brief Creates empty model.\n",
      "     |      .        * Use StatModel::train to train the model. Since %SVMSGD has several parameters, you may want to\n",
      "     |      .        * find the best parameters for your problem or use setOptimalParameters() to set some default parameters.\n",
      "     |  \n",
      "     |  load(...)\n",
      "     |      load(filepath[, nodeName]) -> retval\n",
      "     |      .   @brief Loads and creates a serialized SVMSGD from a file\n",
      "     |      .        *\n",
      "     |      .        * Use SVMSGD::save to serialize and store an SVMSGD to disk.\n",
      "     |      .        * Load the SVMSGD from this file again, by calling this function with the path to the file.\n",
      "     |      .        * Optionally specify the node for the file containing the classifier\n",
      "     |      .        *\n",
      "     |      .        * @param filepath path to serialized SVMSGD\n",
      "     |      .        * @param nodeName name of node containing the classifier\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ml_StatModel:\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_StatModel(Algorithm)\n",
      "     |  Method resolution order:\n",
      "     |      ml_StatModel\n",
      "     |      Algorithm\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  calcError(...)\n",
      "     |      calcError(data, test[, resp]) -> retval, resp\n",
      "     |      .   @brief Computes error on the training or test dataset\n",
      "     |      .   \n",
      "     |      .       @param data the training data\n",
      "     |      .       @param test if true, the error is computed over the test subset of the data, otherwise it's\n",
      "     |      .           computed over the training subset of the data. Please note that if you loaded a completely\n",
      "     |      .           different dataset to evaluate already trained classifier, you will probably want not to set\n",
      "     |      .           the test subset at all with TrainData::setTrainTestSplitRatio and specify test=false, so\n",
      "     |      .           that the error is computed for the whole new set. Yes, this sounds a bit confusing.\n",
      "     |      .       @param resp the optional output responses.\n",
      "     |      .   \n",
      "     |      .       The method uses StatModel::predict to compute the error. For regression models the error is\n",
      "     |      .       computed as RMS, for classifiers - as a percent of missclassified samples (0%-100%).\n",
      "     |  \n",
      "     |  empty(...)\n",
      "     |      empty() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarCount(...)\n",
      "     |      getVarCount() -> retval\n",
      "     |      .   @brief Returns the number of variables in training samples\n",
      "     |  \n",
      "     |  isClassifier(...)\n",
      "     |      isClassifier() -> retval\n",
      "     |      .   @brief Returns true if the model is classifier\n",
      "     |  \n",
      "     |  isTrained(...)\n",
      "     |      isTrained() -> retval\n",
      "     |      .   @brief Returns true if the model is trained\n",
      "     |  \n",
      "     |  predict(...)\n",
      "     |      predict(samples[, results[, flags]]) -> retval, results\n",
      "     |      .   @brief Predicts response(s) for the provided sample(s)\n",
      "     |      .   \n",
      "     |      .       @param samples The input samples, floating-point matrix\n",
      "     |      .       @param results The optional output matrix of results.\n",
      "     |      .       @param flags The optional flags, model-dependent. See cv::ml::StatModel::Flags.\n",
      "     |  \n",
      "     |  train(...)\n",
      "     |      train(trainData[, flags]) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param trainData training data that can be loaded from file using TrainData::loadFromCSV or\n",
      "     |      .           created with TrainData::create.\n",
      "     |      .       @param flags optional flags, depending on the model. Some of the models can be updated with the\n",
      "     |      .           new training samples, not completely overwritten (such as NormalBayesClassifier or ANN_MLP).\n",
      "     |      \n",
      "     |      \n",
      "     |      \n",
      "     |      train(samples, layout, responses) -> retval\n",
      "     |      .   @brief Trains the statistical model\n",
      "     |      .   \n",
      "     |      .       @param samples training samples\n",
      "     |      .       @param layout See ml::SampleTypes.\n",
      "     |      .       @param responses vector of responses associated with the training samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Algorithm:\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      clear() -> None\n",
      "     |      .   @brief Clears the algorithm state\n",
      "     |  \n",
      "     |  getDefaultName(...)\n",
      "     |      getDefaultName() -> retval\n",
      "     |      .   Returns the algorithm string identifier.\n",
      "     |      .   This string is used as top level xml/yml node tag when the object is saved to a file or string.\n",
      "     |  \n",
      "     |  read(...)\n",
      "     |      read(fn) -> None\n",
      "     |      .   @brief Reads algorithm parameters from a file storage\n",
      "     |  \n",
      "     |  save(...)\n",
      "     |      save(filename) -> None\n",
      "     |      .   Saves the algorithm to a file.\n",
      "     |      .   In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).\n",
      "     |  \n",
      "     |  write(...)\n",
      "     |      write(fs[, name]) -> None\n",
      "     |      .   @brief simplified API for language bindings\n",
      "     |      .       * @overload\n",
      "    \n",
      "    class ml_TrainData(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  getCatCount(...)\n",
      "     |      getCatCount(vi) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCatMap(...)\n",
      "     |      getCatMap() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getCatOfs(...)\n",
      "     |      getCatOfs() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getClassLabels(...)\n",
      "     |      getClassLabels() -> retval\n",
      "     |      .   @brief Returns the vector of class labels\n",
      "     |      .   \n",
      "     |      .       The function returns vector of unique labels occurred in the responses.\n",
      "     |  \n",
      "     |  getDefaultSubstValues(...)\n",
      "     |      getDefaultSubstValues() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getLayout(...)\n",
      "     |      getLayout() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getMissing(...)\n",
      "     |      getMissing() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNAllVars(...)\n",
      "     |      getNAllVars() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNSamples(...)\n",
      "     |      getNSamples() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNTestSamples(...)\n",
      "     |      getNTestSamples() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNTrainSamples(...)\n",
      "     |      getNTrainSamples() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNVars(...)\n",
      "     |      getNVars() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getNames(...)\n",
      "     |      getNames(names) -> None\n",
      "     |      .   @brief Returns vector of symbolic names captured in loadFromCSV()\n",
      "     |  \n",
      "     |  getNormCatResponses(...)\n",
      "     |      getNormCatResponses() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getResponseType(...)\n",
      "     |      getResponseType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getResponses(...)\n",
      "     |      getResponses() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSample(...)\n",
      "     |      getSample(varIdx, sidx, buf) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSampleWeights(...)\n",
      "     |      getSampleWeights() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getSamples(...)\n",
      "     |      getSamples() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTestNormCatResponses(...)\n",
      "     |      getTestNormCatResponses() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTestResponses(...)\n",
      "     |      getTestResponses() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTestSampleIdx(...)\n",
      "     |      getTestSampleIdx() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTestSampleWeights(...)\n",
      "     |      getTestSampleWeights() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTestSamples(...)\n",
      "     |      getTestSamples() -> retval\n",
      "     |      .   @brief Returns matrix of test samples\n",
      "     |  \n",
      "     |  getTrainNormCatResponses(...)\n",
      "     |      getTrainNormCatResponses() -> retval\n",
      "     |      .   @brief Returns the vector of normalized categorical responses\n",
      "     |      .   \n",
      "     |      .       The function returns vector of responses. Each response is integer from `0` to `<number of\n",
      "     |      .       classes>-1`. The actual label value can be retrieved then from the class label vector, see\n",
      "     |      .       TrainData::getClassLabels.\n",
      "     |  \n",
      "     |  getTrainResponses(...)\n",
      "     |      getTrainResponses() -> retval\n",
      "     |      .   @brief Returns the vector of responses\n",
      "     |      .   \n",
      "     |      .       The function returns ordered or the original categorical responses. Usually it's used in\n",
      "     |      .       regression algorithms.\n",
      "     |  \n",
      "     |  getTrainSampleIdx(...)\n",
      "     |      getTrainSampleIdx() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTrainSampleWeights(...)\n",
      "     |      getTrainSampleWeights() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getTrainSamples(...)\n",
      "     |      getTrainSamples([, layout[, compressSamples[, compressVars]]]) -> retval\n",
      "     |      .   @brief Returns matrix of train samples\n",
      "     |      .   \n",
      "     |      .       @param layout The requested layout. If it's different from the initial one, the matrix is\n",
      "     |      .           transposed. See ml::SampleTypes.\n",
      "     |      .       @param compressSamples if true, the function returns only the training samples (specified by\n",
      "     |      .           sampleIdx)\n",
      "     |      .       @param compressVars if true, the function returns the shorter training samples, containing only\n",
      "     |      .           the active variables.\n",
      "     |      .   \n",
      "     |      .       In current implementation the function tries to avoid physical data copying and returns the\n",
      "     |      .       matrix stored inside TrainData (unless the transposition or compression is needed).\n",
      "     |  \n",
      "     |  getValues(...)\n",
      "     |      getValues(vi, sidx, values) -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarIdx(...)\n",
      "     |      getVarIdx() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarSymbolFlags(...)\n",
      "     |      getVarSymbolFlags() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  getVarType(...)\n",
      "     |      getVarType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  setTrainTestSplit(...)\n",
      "     |      setTrainTestSplit(count[, shuffle]) -> None\n",
      "     |      .   @brief Splits the training data into the training and test parts\n",
      "     |      .       @sa TrainData::setTrainTestSplitRatio\n",
      "     |  \n",
      "     |  setTrainTestSplitRatio(...)\n",
      "     |      setTrainTestSplitRatio(ratio[, shuffle]) -> None\n",
      "     |      .   @brief Splits the training data into the training and test parts\n",
      "     |      .   \n",
      "     |      .       The function selects a subset of specified relative size and then returns it as the training\n",
      "     |      .       set. If the function is not called, all the data is used for training. Please, note that for\n",
      "     |      .       each of TrainData::getTrain\\* there is corresponding TrainData::getTest\\*, so that the test\n",
      "     |      .       subset can be retrieved and processed as well.\n",
      "     |      .       @sa TrainData::setTrainTestSplit\n",
      "     |  \n",
      "     |  shuffleTrainTest(...)\n",
      "     |      shuffleTrainTest() -> None\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  create(...)\n",
      "     |      create(samples, layout, responses[, varIdx[, sampleIdx[, sampleWeights[, varType]]]]) -> retval\n",
      "     |      .   @brief Creates training data from in-memory arrays.\n",
      "     |      .   \n",
      "     |      .       @param samples matrix of samples. It should have CV_32F type.\n",
      "     |      .       @param layout see ml::SampleTypes.\n",
      "     |      .       @param responses matrix of responses. If the responses are scalar, they should be stored as a\n",
      "     |      .           single row or as a single column. The matrix should have type CV_32F or CV_32S (in the\n",
      "     |      .           former case the responses are considered as ordered by default; in the latter case - as\n",
      "     |      .           categorical)\n",
      "     |      .       @param varIdx vector specifying which variables to use for training. It can be an integer vector\n",
      "     |      .           (CV_32S) containing 0-based variable indices or byte vector (CV_8U) containing a mask of\n",
      "     |      .           active variables.\n",
      "     |      .       @param sampleIdx vector specifying which samples to use for training. It can be an integer\n",
      "     |      .           vector (CV_32S) containing 0-based sample indices or byte vector (CV_8U) containing a mask\n",
      "     |      .           of training samples.\n",
      "     |      .       @param sampleWeights optional vector with weights for each sample. It should have CV_32F type.\n",
      "     |      .       @param varType optional vector of type CV_8U and size `<number_of_variables_in_samples> +\n",
      "     |      .           <number_of_variables_in_responses>`, containing types of each input and output variable. See\n",
      "     |      .           ml::VariableTypes.\n",
      "     |  \n",
      "     |  getSubMatrix(...)\n",
      "     |      getSubMatrix(matrix, idx, layout) -> retval\n",
      "     |      .   @brief Extract from matrix rows/cols specified by passed indexes.\n",
      "     |      .       @param matrix input matrix (supported types: CV_32S, CV_32F, CV_64F)\n",
      "     |      .       @param idx 1D index vector\n",
      "     |      .       @param layout specifies to extract rows (cv::ml::ROW_SAMPLES) or to extract columns (cv::ml::COL_SAMPLES)\n",
      "     |  \n",
      "     |  getSubVector(...)\n",
      "     |      getSubVector(vec, idx) -> retval\n",
      "     |      .   @brief Extract from 1D vector elements specified by passed indexes.\n",
      "     |      .       @param vec input vector (supported types: CV_32S, CV_32F, CV_64F)\n",
      "     |      .       @param idx 1D index vector\n",
      "    \n",
      "    class ocl_Device(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  OpenCLVersion(...)\n",
      "     |      OpenCLVersion() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  OpenCL_C_Version(...)\n",
      "     |      OpenCL_C_Version() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  addressBits(...)\n",
      "     |      addressBits() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  available(...)\n",
      "     |      available() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  compilerAvailable(...)\n",
      "     |      compilerAvailable() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  deviceVersionMajor(...)\n",
      "     |      deviceVersionMajor() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  deviceVersionMinor(...)\n",
      "     |      deviceVersionMinor() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  doubleFPConfig(...)\n",
      "     |      doubleFPConfig() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  driverVersion(...)\n",
      "     |      driverVersion() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  endianLittle(...)\n",
      "     |      endianLittle() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  errorCorrectionSupport(...)\n",
      "     |      errorCorrectionSupport() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  executionCapabilities(...)\n",
      "     |      executionCapabilities() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  extensions(...)\n",
      "     |      extensions() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  globalMemCacheLineSize(...)\n",
      "     |      globalMemCacheLineSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  globalMemCacheSize(...)\n",
      "     |      globalMemCacheSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  globalMemCacheType(...)\n",
      "     |      globalMemCacheType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  globalMemSize(...)\n",
      "     |      globalMemSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  halfFPConfig(...)\n",
      "     |      halfFPConfig() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  hostUnifiedMemory(...)\n",
      "     |      hostUnifiedMemory() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  image2DMaxHeight(...)\n",
      "     |      image2DMaxHeight() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  image2DMaxWidth(...)\n",
      "     |      image2DMaxWidth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  image3DMaxDepth(...)\n",
      "     |      image3DMaxDepth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  image3DMaxHeight(...)\n",
      "     |      image3DMaxHeight() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  image3DMaxWidth(...)\n",
      "     |      image3DMaxWidth() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  imageFromBufferSupport(...)\n",
      "     |      imageFromBufferSupport() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  imageMaxArraySize(...)\n",
      "     |      imageMaxArraySize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  imageMaxBufferSize(...)\n",
      "     |      imageMaxBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  imageSupport(...)\n",
      "     |      imageSupport() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  intelSubgroupsSupport(...)\n",
      "     |      intelSubgroupsSupport() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isAMD(...)\n",
      "     |      isAMD() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isExtensionSupported(...)\n",
      "     |      isExtensionSupported(extensionName) -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isIntel(...)\n",
      "     |      isIntel() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  isNVidia(...)\n",
      "     |      isNVidia() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  linkerAvailable(...)\n",
      "     |      linkerAvailable() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  localMemSize(...)\n",
      "     |      localMemSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  localMemType(...)\n",
      "     |      localMemType() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxClockFrequency(...)\n",
      "     |      maxClockFrequency() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxComputeUnits(...)\n",
      "     |      maxComputeUnits() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxConstantArgs(...)\n",
      "     |      maxConstantArgs() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxConstantBufferSize(...)\n",
      "     |      maxConstantBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxMemAllocSize(...)\n",
      "     |      maxMemAllocSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxParameterSize(...)\n",
      "     |      maxParameterSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxReadImageArgs(...)\n",
      "     |      maxReadImageArgs() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxSamplers(...)\n",
      "     |      maxSamplers() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxWorkGroupSize(...)\n",
      "     |      maxWorkGroupSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxWorkItemDims(...)\n",
      "     |      maxWorkItemDims() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  maxWriteImageArgs(...)\n",
      "     |      maxWriteImageArgs() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  memBaseAddrAlign(...)\n",
      "     |      memBaseAddrAlign() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  name(...)\n",
      "     |      name() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthChar(...)\n",
      "     |      nativeVectorWidthChar() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthDouble(...)\n",
      "     |      nativeVectorWidthDouble() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthFloat(...)\n",
      "     |      nativeVectorWidthFloat() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthHalf(...)\n",
      "     |      nativeVectorWidthHalf() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthInt(...)\n",
      "     |      nativeVectorWidthInt() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthLong(...)\n",
      "     |      nativeVectorWidthLong() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  nativeVectorWidthShort(...)\n",
      "     |      nativeVectorWidthShort() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthChar(...)\n",
      "     |      preferredVectorWidthChar() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthDouble(...)\n",
      "     |      preferredVectorWidthDouble() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthFloat(...)\n",
      "     |      preferredVectorWidthFloat() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthHalf(...)\n",
      "     |      preferredVectorWidthHalf() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthInt(...)\n",
      "     |      preferredVectorWidthInt() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthLong(...)\n",
      "     |      preferredVectorWidthLong() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  preferredVectorWidthShort(...)\n",
      "     |      preferredVectorWidthShort() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  printfBufferSize(...)\n",
      "     |      printfBufferSize() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  profilingTimerResolution(...)\n",
      "     |      profilingTimerResolution() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  singleFPConfig(...)\n",
      "     |      singleFPConfig() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  type(...)\n",
      "     |      type() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  vendorID(...)\n",
      "     |      vendorID() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  vendorName(...)\n",
      "     |      vendorName() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  version(...)\n",
      "     |      version() -> retval\n",
      "     |      .\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  getDefault(...)\n",
      "     |      getDefault() -> retval\n",
      "     |      .\n",
      "\n",
      "FUNCTIONS\n",
      "    AKAZE_create(...)\n",
      "        AKAZE_create([, descriptor_type[, descriptor_size[, descriptor_channels[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]]) -> retval\n",
      "        .   @brief The AKAZE constructor\n",
      "        .   \n",
      "        .       @param descriptor_type Type of the extracted descriptor: DESCRIPTOR_KAZE,\n",
      "        .       DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.\n",
      "        .       @param descriptor_size Size of the descriptor in bits. 0 -\\> Full size\n",
      "        .       @param descriptor_channels Number of channels in the descriptor (1, 2, 3)\n",
      "        .       @param threshold Detector response threshold to accept point\n",
      "        .       @param nOctaves Maximum octave evolution of the image\n",
      "        .       @param nOctaveLayers Default number of sublevels per scale level\n",
      "        .       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n",
      "        .       DIFF_CHARBONNIER\n",
      "    \n",
      "    AgastFeatureDetector_create(...)\n",
      "        AgastFeatureDetector_create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n",
      "        .\n",
      "    \n",
      "    BFMatcher_create(...)\n",
      "        BFMatcher_create([, normType[, crossCheck]]) -> retval\n",
      "        .   @brief Brute-force matcher create method.\n",
      "        .       @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are\n",
      "        .       preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and\n",
      "        .       BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor\n",
      "        .       description).\n",
      "        .       @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k\n",
      "        .       nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with\n",
      "        .       k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the\n",
      "        .       matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent\n",
      "        .       pairs. Such technique usually produces best results with minimal number of outliers when there are\n",
      "        .       enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.\n",
      "    \n",
      "    BRISK_create(...)\n",
      "        BRISK_create([, thresh[, octaves[, patternScale]]]) -> retval\n",
      "        .   @brief The BRISK constructor\n",
      "        .   \n",
      "        .       @param thresh AGAST detection threshold score.\n",
      "        .       @param octaves detection octaves. Use 0 to do single scale.\n",
      "        .       @param patternScale apply this scale to the pattern used for sampling the neighbourhood of a\n",
      "        .       keypoint.\n",
      "        \n",
      "        \n",
      "        \n",
      "        BRISK_create(radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n",
      "        .   @brief The BRISK constructor for a custom pattern\n",
      "        .   \n",
      "        .       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n",
      "        .       keypoint scale 1).\n",
      "        .       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n",
      "        .       size as radiusList..\n",
      "        .       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n",
      "        .       scale 1).\n",
      "        .       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n",
      "        .       keypoint scale 1).\n",
      "        .   @param indexChange index remapping of the bits.\n",
      "        \n",
      "        \n",
      "        \n",
      "        BRISK_create(thresh, octaves, radiusList, numberList[, dMax[, dMin[, indexChange]]]) -> retval\n",
      "        .   @brief The BRISK constructor for a custom pattern, detection threshold and octaves\n",
      "        .   \n",
      "        .       @param thresh AGAST detection threshold score.\n",
      "        .       @param octaves detection octaves. Use 0 to do single scale.\n",
      "        .       @param radiusList defines the radii (in pixels) where the samples around a keypoint are taken (for\n",
      "        .       keypoint scale 1).\n",
      "        .       @param numberList defines the number of sampling points on the sampling circle. Must be the same\n",
      "        .       size as radiusList..\n",
      "        .       @param dMax threshold for the short pairings used for descriptor formation (in pixels for keypoint\n",
      "        .       scale 1).\n",
      "        .       @param dMin threshold for the long pairings used for orientation determination (in pixels for\n",
      "        .       keypoint scale 1).\n",
      "        .   @param indexChange index remapping of the bits.\n",
      "    \n",
      "    CamShift(...)\n",
      "        CamShift(probImage, window, criteria) -> retval, window\n",
      "        .   @brief Finds an object center, size, and orientation.\n",
      "        .   \n",
      "        .   @param probImage Back projection of the object histogram. See calcBackProject.\n",
      "        .   @param window Initial search window.\n",
      "        .   @param criteria Stop criteria for the underlying meanShift.\n",
      "        .   returns\n",
      "        .   (in old interfaces) Number of iterations CAMSHIFT took to converge\n",
      "        .   The function implements the CAMSHIFT object tracking algorithm @cite Bradski98 . First, it finds an\n",
      "        .   object center using meanShift and then adjusts the window size and finds the optimal rotation. The\n",
      "        .   function returns the rotated rectangle structure that includes the object position, size, and\n",
      "        .   orientation. The next position of the search window can be obtained with RotatedRect::boundingRect()\n",
      "        .   \n",
      "        .   See the OpenCV sample camshiftdemo.c that tracks colored objects.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   -   (Python) A sample explaining the camshift tracking algorithm can be found at\n",
      "        .       opencv_source_code/samples/python/camshift.py\n",
      "    \n",
      "    Canny(...)\n",
      "        Canny(image, threshold1, threshold2[, edges[, apertureSize[, L2gradient]]]) -> edges\n",
      "        .   @brief Finds edges in an image using the Canny algorithm @cite Canny86 .\n",
      "        .   \n",
      "        .   The function finds edges in the input image and marks them in the output map edges using the\n",
      "        .   Canny algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The\n",
      "        .   largest value is used to find initial segments of strong edges. See\n",
      "        .   <http://en.wikipedia.org/wiki/Canny_edge_detector>\n",
      "        .   \n",
      "        .   @param image 8-bit input image.\n",
      "        .   @param edges output edge map; single channels 8-bit image, which has the same size as image .\n",
      "        .   @param threshold1 first threshold for the hysteresis procedure.\n",
      "        .   @param threshold2 second threshold for the hysteresis procedure.\n",
      "        .   @param apertureSize aperture size for the Sobel operator.\n",
      "        .   @param L2gradient a flag, indicating whether a more accurate \\f$L_2\\f$ norm\n",
      "        .   \\f$=\\sqrt{(dI/dx)^2 + (dI/dy)^2}\\f$ should be used to calculate the image gradient magnitude (\n",
      "        .   L2gradient=true ), or whether the default \\f$L_1\\f$ norm \\f$=|dI/dx|+|dI/dy|\\f$ is enough (\n",
      "        .   L2gradient=false ).\n",
      "        \n",
      "        \n",
      "        \n",
      "        Canny(dx, dy, threshold1, threshold2[, edges[, L2gradient]]) -> edges\n",
      "        .   \\overload\n",
      "        .   \n",
      "        .   Finds edges in an image using the Canny algorithm with custom image gradient.\n",
      "        .   \n",
      "        .   @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).\n",
      "        .   @param dy 16-bit y derivative of input image (same type as dx).\n",
      "        .   @param edges output edge map; single channels 8-bit image, which has the same size as image .\n",
      "        .   @param threshold1 first threshold for the hysteresis procedure.\n",
      "        .   @param threshold2 second threshold for the hysteresis procedure.\n",
      "        .   @param L2gradient a flag, indicating whether a more accurate \\f$L_2\\f$ norm\n",
      "        .   \\f$=\\sqrt{(dI/dx)^2 + (dI/dy)^2}\\f$ should be used to calculate the image gradient magnitude (\n",
      "        .   L2gradient=true ), or whether the default \\f$L_1\\f$ norm \\f$=|dI/dx|+|dI/dy|\\f$ is enough (\n",
      "        .   L2gradient=false ).\n",
      "    \n",
      "    CascadeClassifier_convert(...)\n",
      "        CascadeClassifier_convert(oldcascade, newcascade) -> retval\n",
      "        .\n",
      "    \n",
      "    DISOpticalFlow_create(...)\n",
      "        DISOpticalFlow_create([, preset]) -> retval\n",
      "        .   @brief Creates an instance of DISOpticalFlow\n",
      "        .   \n",
      "        .       @param preset one of PRESET_ULTRAFAST, PRESET_FAST and PRESET_MEDIUM\n",
      "    \n",
      "    DescriptorMatcher_create(...)\n",
      "        DescriptorMatcher_create(descriptorMatcherType) -> retval\n",
      "        .   @brief Creates a descriptor matcher of a given type with the default parameters (using default\n",
      "        .       constructor).\n",
      "        .   \n",
      "        .       @param descriptorMatcherType Descriptor matcher type. Now the following matcher types are\n",
      "        .       supported:\n",
      "        .       -   `BruteForce` (it uses L2 )\n",
      "        .       -   `BruteForce-L1`\n",
      "        .       -   `BruteForce-Hamming`\n",
      "        .       -   `BruteForce-Hamming(2)`\n",
      "        .       -   `FlannBased`\n",
      "        \n",
      "        \n",
      "        \n",
      "        DescriptorMatcher_create(matcherType) -> retval\n",
      "        .\n",
      "    \n",
      "    EMD(...)\n",
      "        EMD(signature1, signature2, distType[, cost[, lowerBound[, flow]]]) -> retval, lowerBound, flow\n",
      "        .   @brief Computes the \"minimal work\" distance between two weighted point configurations.\n",
      "        .   \n",
      "        .   The function computes the earth mover distance and/or a lower boundary of the distance between the\n",
      "        .   two weighted point configurations. One of the applications described in @cite RubnerSept98,\n",
      "        .   @cite Rubner2000 is multi-dimensional histogram comparison for image retrieval. EMD is a transportation\n",
      "        .   problem that is solved using some modification of a simplex algorithm, thus the complexity is\n",
      "        .   exponential in the worst case, though, on average it is much faster. In the case of a real metric\n",
      "        .   the lower boundary can be calculated even faster (using linear-time algorithm) and it can be used\n",
      "        .   to determine roughly whether the two signatures are far enough so that they cannot relate to the\n",
      "        .   same object.\n",
      "        .   \n",
      "        .   @param signature1 First signature, a \\f$\\texttt{size1}\\times \\texttt{dims}+1\\f$ floating-point matrix.\n",
      "        .   Each row stores the point weight followed by the point coordinates. The matrix is allowed to have\n",
      "        .   a single column (weights only) if the user-defined cost matrix is used. The weights must be\n",
      "        .   non-negative and have at least one non-zero value.\n",
      "        .   @param signature2 Second signature of the same format as signature1 , though the number of rows\n",
      "        .   may be different. The total weights may be different. In this case an extra \"dummy\" point is added\n",
      "        .   to either signature1 or signature2. The weights must be non-negative and have at least one non-zero\n",
      "        .   value.\n",
      "        .   @param distType Used metric. See #DistanceTypes.\n",
      "        .   @param cost User-defined \\f$\\texttt{size1}\\times \\texttt{size2}\\f$ cost matrix. Also, if a cost matrix\n",
      "        .   is used, lower boundary lowerBound cannot be calculated because it needs a metric function.\n",
      "        .   @param lowerBound Optional input/output parameter: lower boundary of a distance between the two\n",
      "        .   signatures that is a distance between mass centers. The lower boundary may not be calculated if\n",
      "        .   the user-defined cost matrix is used, the total weights of point configurations are not equal, or\n",
      "        .   if the signatures consist of weights only (the signature matrices have a single column). You\n",
      "        .   **must** initialize \\*lowerBound . If the calculated distance between mass centers is greater or\n",
      "        .   equal to \\*lowerBound (it means that the signatures are far enough), the function does not\n",
      "        .   calculate EMD. In any case \\*lowerBound is set to the calculated distance between mass centers on\n",
      "        .   return. Thus, if you want to calculate both distance between mass centers and EMD, \\*lowerBound\n",
      "        .   should be set to 0.\n",
      "        .   @param flow Resultant \\f$\\texttt{size1} \\times \\texttt{size2}\\f$ flow matrix: \\f$\\texttt{flow}_{i,j}\\f$ is\n",
      "        .   a flow from \\f$i\\f$ -th point of signature1 to \\f$j\\f$ -th point of signature2 .\n",
      "    \n",
      "    FarnebackOpticalFlow_create(...)\n",
      "        FarnebackOpticalFlow_create([, numLevels[, pyrScale[, fastPyramids[, winSize[, numIters[, polyN[, polySigma[, flags]]]]]]]]) -> retval\n",
      "        .\n",
      "    \n",
      "    FastFeatureDetector_create(...)\n",
      "        FastFeatureDetector_create([, threshold[, nonmaxSuppression[, type]]]) -> retval\n",
      "        .\n",
      "    \n",
      "    FlannBasedMatcher_create(...)\n",
      "        FlannBasedMatcher_create() -> retval\n",
      "        .\n",
      "    \n",
      "    GFTTDetector_create(...)\n",
      "        GFTTDetector_create([, maxCorners[, qualityLevel[, minDistance[, blockSize[, useHarrisDetector[, k]]]]]]) -> retval\n",
      "        .   \n",
      "        \n",
      "        \n",
      "        \n",
      "        GFTTDetector_create(maxCorners, qualityLevel, minDistance, blockSize, gradiantSize[, useHarrisDetector[, k]]) -> retval\n",
      "        .\n",
      "    \n",
      "    GaussianBlur(...)\n",
      "        GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) -> dst\n",
      "        .   @brief Blurs an image using a Gaussian filter.\n",
      "        .   \n",
      "        .   The function convolves the source image with the specified Gaussian kernel. In-place filtering is\n",
      "        .   supported.\n",
      "        .   \n",
      "        .   @param src input image; the image can have any number of channels, which are processed\n",
      "        .   independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "        .   @param dst output image of the same size and type as src.\n",
      "        .   @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be\n",
      "        .   positive and odd. Or, they can be zero's and then they are computed from sigma.\n",
      "        .   @param sigmaX Gaussian kernel standard deviation in X direction.\n",
      "        .   @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be\n",
      "        .   equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,\n",
      "        .   respectively (see #getGaussianKernel for details); to fully control the result regardless of\n",
      "        .   possible future modifications of all this semantics, it is recommended to specify all of ksize,\n",
      "        .   sigmaX, and sigmaY.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   \n",
      "        .   @sa  sepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlur\n",
      "    \n",
      "    HOGDescriptor_getDaimlerPeopleDetector(...)\n",
      "        HOGDescriptor_getDaimlerPeopleDetector() -> retval\n",
      "        .   @brief Returns coefficients of the classifier trained for people detection (for 48x96 windows).\n",
      "    \n",
      "    HOGDescriptor_getDefaultPeopleDetector(...)\n",
      "        HOGDescriptor_getDefaultPeopleDetector() -> retval\n",
      "        .   @brief Returns coefficients of the classifier trained for people detection (for 64x128 windows).\n",
      "    \n",
      "    HoughCircles(...)\n",
      "        HoughCircles(image, method, dp, minDist[, circles[, param1[, param2[, minRadius[, maxRadius]]]]]) -> circles\n",
      "        .   @brief Finds circles in a grayscale image using the Hough transform.\n",
      "        .   \n",
      "        .   The function finds circles in a grayscale image using a modification of the Hough transform.\n",
      "        .   \n",
      "        .   Example: :\n",
      "        .   @include snippets/imgproc_HoughLinesCircles.cpp\n",
      "        .   \n",
      "        .   @note Usually the function detects the centers of circles well. However, it may fail to find correct\n",
      "        .   radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if\n",
      "        .   you know it. Or, you may set maxRadius to a negative number to return centers only without radius\n",
      "        .   search, and find the correct radius using an additional procedure.\n",
      "        .   \n",
      "        .   @param image 8-bit, single-channel, grayscale input image.\n",
      "        .   @param circles Output vector of found circles. Each vector is encoded as  3 or 4 element\n",
      "        .   floating-point vector \\f$(x, y, radius)\\f$ or \\f$(x, y, radius, votes)\\f$ .\n",
      "        .   @param method Detection method, see #HoughModes. Currently, the only implemented method is #HOUGH_GRADIENT\n",
      "        .   @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if\n",
      "        .   dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has\n",
      "        .   half as big width and height.\n",
      "        .   @param minDist Minimum distance between the centers of the detected circles. If the parameter is\n",
      "        .   too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is\n",
      "        .   too large, some circles may be missed.\n",
      "        .   @param param1 First method-specific parameter. In case of #HOUGH_GRADIENT , it is the higher\n",
      "        .   threshold of the two passed to the Canny edge detector (the lower one is twice smaller).\n",
      "        .   @param param2 Second method-specific parameter. In case of #HOUGH_GRADIENT , it is the\n",
      "        .   accumulator threshold for the circle centers at the detection stage. The smaller it is, the more\n",
      "        .   false circles may be detected. Circles, corresponding to the larger accumulator values, will be\n",
      "        .   returned first.\n",
      "        .   @param minRadius Minimum circle radius.\n",
      "        .   @param maxRadius Maximum circle radius. If <= 0, uses the maximum image dimension. If < 0, returns\n",
      "        .   centers without finding the radius.\n",
      "        .   \n",
      "        .   @sa fitEllipse, minEnclosingCircle\n",
      "    \n",
      "    HoughLines(...)\n",
      "        HoughLines(image, rho, theta, threshold[, lines[, srn[, stn[, min_theta[, max_theta]]]]]) -> lines\n",
      "        .   @brief Finds lines in a binary image using the standard Hough transform.\n",
      "        .   \n",
      "        .   The function implements the standard or standard multi-scale Hough transform algorithm for line\n",
      "        .   detection. See <http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm> for a good explanation of Hough\n",
      "        .   transform.\n",
      "        .   \n",
      "        .   @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n",
      "        .   @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector\n",
      "        .   \\f$(\\rho, \\theta)\\f$ or \\f$(\\rho, \\theta, \\textrm{votes})\\f$ . \\f$\\rho\\f$ is the distance from the coordinate origin \\f$(0,0)\\f$ (top-left corner of\n",
      "        .   the image). \\f$\\theta\\f$ is the line rotation angle in radians (\n",
      "        .   \\f$0 \\sim \\textrm{vertical line}, \\pi/2 \\sim \\textrm{horizontal line}\\f$ ).\n",
      "        .   \\f$\\textrm{votes}\\f$ is the value of accumulator.\n",
      "        .   @param rho Distance resolution of the accumulator in pixels.\n",
      "        .   @param theta Angle resolution of the accumulator in radians.\n",
      "        .   @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n",
      "        .   votes ( \\f$>\\texttt{threshold}\\f$ ).\n",
      "        .   @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .\n",
      "        .   The coarse accumulator distance resolution is rho and the accurate accumulator resolution is\n",
      "        .   rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these\n",
      "        .   parameters should be positive.\n",
      "        .   @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.\n",
      "        .   @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.\n",
      "        .   Must fall between 0 and max_theta.\n",
      "        .   @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.\n",
      "        .   Must fall between min_theta and CV_PI.\n",
      "    \n",
      "    HoughLinesP(...)\n",
      "        HoughLinesP(image, rho, theta, threshold[, lines[, minLineLength[, maxLineGap]]]) -> lines\n",
      "        .   @brief Finds line segments in a binary image using the probabilistic Hough transform.\n",
      "        .   \n",
      "        .   The function implements the probabilistic Hough transform algorithm for line detection, described\n",
      "        .   in @cite Matas00\n",
      "        .   \n",
      "        .   See the line detection example below:\n",
      "        .   @include snippets/imgproc_HoughLinesP.cpp\n",
      "        .   This is a sample picture the function parameters have been tuned for:\n",
      "        .   \n",
      "        .   ![image](pics/building.jpg)\n",
      "        .   \n",
      "        .   And this is the output of the above program in case of the probabilistic Hough transform:\n",
      "        .   \n",
      "        .   ![image](pics/houghp.png)\n",
      "        .   \n",
      "        .   @param image 8-bit, single-channel binary source image. The image may be modified by the function.\n",
      "        .   @param lines Output vector of lines. Each line is represented by a 4-element vector\n",
      "        .   \\f$(x_1, y_1, x_2, y_2)\\f$ , where \\f$(x_1,y_1)\\f$ and \\f$(x_2, y_2)\\f$ are the ending points of each detected\n",
      "        .   line segment.\n",
      "        .   @param rho Distance resolution of the accumulator in pixels.\n",
      "        .   @param theta Angle resolution of the accumulator in radians.\n",
      "        .   @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n",
      "        .   votes ( \\f$>\\texttt{threshold}\\f$ ).\n",
      "        .   @param minLineLength Minimum line length. Line segments shorter than that are rejected.\n",
      "        .   @param maxLineGap Maximum allowed gap between points on the same line to link them.\n",
      "        .   \n",
      "        .   @sa LineSegmentDetector\n",
      "    \n",
      "    HoughLinesPointSet(...)\n",
      "        HoughLinesPointSet(_point, lines_max, threshold, min_rho, max_rho, rho_step, min_theta, max_theta, theta_step[, _lines]) -> _lines\n",
      "        .   @brief Finds lines in a set of points using the standard Hough transform.\n",
      "        .   \n",
      "        .   The function finds lines in a set of points using a modification of the Hough transform.\n",
      "        .   @include snippets/imgproc_HoughLinesPointSet.cpp\n",
      "        .   @param _point Input vector of points. Each vector must be encoded as a Point vector \\f$(x,y)\\f$. Type must be CV_32FC2 or CV_32SC2.\n",
      "        .   @param _lines Output vector of found lines. Each vector is encoded as a vector<Vec3d> \\f$(votes, rho, theta)\\f$.\n",
      "        .   The larger the value of 'votes', the higher the reliability of the Hough line.\n",
      "        .   @param lines_max Max count of hough lines.\n",
      "        .   @param threshold Accumulator threshold parameter. Only those lines are returned that get enough\n",
      "        .   votes ( \\f$>\\texttt{threshold}\\f$ )\n",
      "        .   @param min_rho Minimum Distance value of the accumulator in pixels.\n",
      "        .   @param max_rho Maximum Distance value of the accumulator in pixels.\n",
      "        .   @param rho_step Distance resolution of the accumulator in pixels.\n",
      "        .   @param min_theta Minimum angle value of the accumulator in radians.\n",
      "        .   @param max_theta Maximum angle value of the accumulator in radians.\n",
      "        .   @param theta_step Angle resolution of the accumulator in radians.\n",
      "    \n",
      "    HuMoments(...)\n",
      "        HuMoments(m[, hu]) -> hu\n",
      "        .   @overload\n",
      "    \n",
      "    KAZE_create(...)\n",
      "        KAZE_create([, extended[, upright[, threshold[, nOctaves[, nOctaveLayers[, diffusivity]]]]]]) -> retval\n",
      "        .   @brief The KAZE constructor\n",
      "        .   \n",
      "        .       @param extended Set to enable extraction of extended (128-byte) descriptor.\n",
      "        .       @param upright Set to enable use of upright descriptors (non rotation-invariant).\n",
      "        .       @param threshold Detector response threshold to accept point\n",
      "        .       @param nOctaves Maximum octave evolution of the image\n",
      "        .       @param nOctaveLayers Default number of sublevels per scale level\n",
      "        .       @param diffusivity Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or\n",
      "        .       DIFF_CHARBONNIER\n",
      "    \n",
      "    KeyPoint_convert(...)\n",
      "        KeyPoint_convert(keypoints[, keypointIndexes]) -> points2f\n",
      "        .   This method converts vector of keypoints to vector of points or the reverse, where each keypoint is\n",
      "        .       assigned the same size and the same orientation.\n",
      "        .   \n",
      "        .       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n",
      "        .       @param points2f Array of (x,y) coordinates of each keypoint\n",
      "        .       @param keypointIndexes Array of indexes of keypoints to be converted to points. (Acts like a mask to\n",
      "        .       convert only specified keypoints)\n",
      "        \n",
      "        \n",
      "        \n",
      "        KeyPoint_convert(points2f[, size[, response[, octave[, class_id]]]]) -> keypoints\n",
      "        .   @overload\n",
      "        .       @param points2f Array of (x,y) coordinates of each keypoint\n",
      "        .       @param keypoints Keypoints obtained from any feature detection algorithm like SIFT/SURF/ORB\n",
      "        .       @param size keypoint diameter\n",
      "        .       @param response keypoint detector response on the keypoint (that is, strength of the keypoint)\n",
      "        .       @param octave pyramid octave in which the keypoint has been detected\n",
      "        .       @param class_id object id\n",
      "    \n",
      "    KeyPoint_overlap(...)\n",
      "        KeyPoint_overlap(kp1, kp2) -> retval\n",
      "        .   This method computes overlap for pair of keypoints. Overlap is the ratio between area of keypoint\n",
      "        .       regions' intersection and area of keypoint regions' union (considering keypoint region as circle).\n",
      "        .       If they don't overlap, we get zero. If they coincide at same location with same size, we get 1.\n",
      "        .       @param kp1 First keypoint\n",
      "        .       @param kp2 Second keypoint\n",
      "    \n",
      "    LUT(...)\n",
      "        LUT(src, lut[, dst]) -> dst\n",
      "        .   @brief Performs a look-up table transform of an array.\n",
      "        .   \n",
      "        .   The function LUT fills the output array with values from the look-up table. Indices of the entries\n",
      "        .   are taken from the input array. That is, the function processes each element of src as follows:\n",
      "        .   \\f[\\texttt{dst} (I)  \\leftarrow \\texttt{lut(src(I) + d)}\\f]\n",
      "        .   where\n",
      "        .   \\f[d =  \\fork{0}{if \\(\\texttt{src}\\) has depth \\(\\texttt{CV_8U}\\)}{128}{if \\(\\texttt{src}\\) has depth \\(\\texttt{CV_8S}\\)}\\f]\n",
      "        .   @param src input array of 8-bit elements.\n",
      "        .   @param lut look-up table of 256 elements; in case of multi-channel input array, the table should\n",
      "        .   either have a single channel (in this case the same table is used for all channels) or the same\n",
      "        .   number of channels as in the input array.\n",
      "        .   @param dst output array of the same size and number of channels as src, and the same depth as lut.\n",
      "        .   @sa  convertScaleAbs, Mat::convertTo\n",
      "    \n",
      "    Laplacian(...)\n",
      "        Laplacian(src, ddepth[, dst[, ksize[, scale[, delta[, borderType]]]]]) -> dst\n",
      "        .   @brief Calculates the Laplacian of an image.\n",
      "        .   \n",
      "        .   The function calculates the Laplacian of the source image by adding up the second x and y\n",
      "        .   derivatives calculated using the Sobel operator:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} =  \\Delta \\texttt{src} =  \\frac{\\partial^2 \\texttt{src}}{\\partial x^2} +  \\frac{\\partial^2 \\texttt{src}}{\\partial y^2}\\f]\n",
      "        .   \n",
      "        .   This is done when `ksize > 1`. When `ksize == 1`, the Laplacian is computed by filtering the image\n",
      "        .   with the following \\f$3 \\times 3\\f$ aperture:\n",
      "        .   \n",
      "        .   \\f[\\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\\f]\n",
      "        .   \n",
      "        .   @param src Source image.\n",
      "        .   @param dst Destination image of the same size and the same number of channels as src .\n",
      "        .   @param ddepth Desired depth of the destination image.\n",
      "        .   @param ksize Aperture size used to compute the second-derivative filters. See #getDerivKernels for\n",
      "        .   details. The size must be positive and odd.\n",
      "        .   @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is\n",
      "        .   applied. See #getDerivKernels for details.\n",
      "        .   @param delta Optional delta value that is added to the results prior to storing them in dst .\n",
      "        .   @param borderType Pixel extrapolation method, see #BorderTypes\n",
      "        .   @sa  Sobel, Scharr\n",
      "    \n",
      "    MSER_create(...)\n",
      "        MSER_create([, _delta[, _min_area[, _max_area[, _max_variation[, _min_diversity[, _max_evolution[, _area_threshold[, _min_margin[, _edge_blur_size]]]]]]]]]) -> retval\n",
      "        .   @brief Full consturctor for %MSER detector\n",
      "        .   \n",
      "        .       @param _delta it compares \\f$(size_{i}-size_{i-delta})/size_{i-delta}\\f$\n",
      "        .       @param _min_area prune the area which smaller than minArea\n",
      "        .       @param _max_area prune the area which bigger than maxArea\n",
      "        .       @param _max_variation prune the area have similar size to its children\n",
      "        .       @param _min_diversity for color image, trace back to cut off mser with diversity less than min_diversity\n",
      "        .       @param _max_evolution  for color image, the evolution steps\n",
      "        .       @param _area_threshold for color image, the area threshold to cause re-initialize\n",
      "        .       @param _min_margin for color image, ignore too small margin\n",
      "        .       @param _edge_blur_size for color image, the aperture size for edge blur\n",
      "    \n",
      "    Mahalanobis(...)\n",
      "        Mahalanobis(v1, v2, icovar) -> retval\n",
      "        .   @brief Calculates the Mahalanobis distance between two vectors.\n",
      "        .   \n",
      "        .   The function cv::Mahalanobis calculates and returns the weighted distance between two vectors:\n",
      "        .   \\f[d( \\texttt{vec1} , \\texttt{vec2} )= \\sqrt{\\sum_{i,j}{\\texttt{icovar(i,j)}\\cdot(\\texttt{vec1}(I)-\\texttt{vec2}(I))\\cdot(\\texttt{vec1(j)}-\\texttt{vec2(j)})} }\\f]\n",
      "        .   The covariance matrix may be calculated using the #calcCovarMatrix function and then inverted using\n",
      "        .   the invert function (preferably using the #DECOMP_SVD method, as the most accurate).\n",
      "        .   @param v1 first 1D input vector.\n",
      "        .   @param v2 second 1D input vector.\n",
      "        .   @param icovar inverse covariance matrix.\n",
      "    \n",
      "    ORB_create(...)\n",
      "        ORB_create([, nfeatures[, scaleFactor[, nlevels[, edgeThreshold[, firstLevel[, WTA_K[, scoreType[, patchSize[, fastThreshold]]]]]]]]]) -> retval\n",
      "        .   @brief The ORB constructor\n",
      "        .   \n",
      "        .       @param nfeatures The maximum number of features to retain.\n",
      "        .       @param scaleFactor Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical\n",
      "        .       pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor\n",
      "        .       will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor\n",
      "        .       will mean that to cover certain scale range you will need more pyramid levels and so the speed\n",
      "        .       will suffer.\n",
      "        .       @param nlevels The number of pyramid levels. The smallest level will have linear size equal to\n",
      "        .       input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).\n",
      "        .       @param edgeThreshold This is size of the border where the features are not detected. It should\n",
      "        .       roughly match the patchSize parameter.\n",
      "        .       @param firstLevel The level of pyramid to put source image to. Previous layers are filled\n",
      "        .       with upscaled source image.\n",
      "        .       @param WTA_K The number of points that produce each element of the oriented BRIEF descriptor. The\n",
      "        .       default value 2 means the BRIEF where we take a random point pair and compare their brightnesses,\n",
      "        .       so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3\n",
      "        .       random points (of course, those point coordinates are random, but they are generated from the\n",
      "        .       pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel\n",
      "        .       rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such\n",
      "        .       output will occupy 2 bits, and therefore it will need a special variant of Hamming distance,\n",
      "        .       denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each\n",
      "        .       bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).\n",
      "        .       @param scoreType The default HARRIS_SCORE means that Harris algorithm is used to rank features\n",
      "        .       (the score is written to KeyPoint::score and is used to retain best nfeatures features);\n",
      "        .       FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints,\n",
      "        .       but it is a little faster to compute.\n",
      "        .       @param patchSize size of the patch used by the oriented BRIEF descriptor. Of course, on smaller\n",
      "        .       pyramid layers the perceived image area covered by a feature will be larger.\n",
      "        .       @param fastThreshold the fast threshold\n",
      "    \n",
      "    PCABackProject(...)\n",
      "        PCABackProject(data, mean, eigenvectors[, result]) -> result\n",
      "        .   wrap PCA::backProject\n",
      "    \n",
      "    PCACompute(...)\n",
      "        PCACompute(data, mean[, eigenvectors[, maxComponents]]) -> mean, eigenvectors\n",
      "        .   wrap PCA::operator()\n",
      "        \n",
      "        \n",
      "        \n",
      "        PCACompute(data, mean, retainedVariance[, eigenvectors]) -> mean, eigenvectors\n",
      "        .   wrap PCA::operator()\n",
      "    \n",
      "    PCACompute2(...)\n",
      "        PCACompute2(data, mean[, eigenvectors[, eigenvalues[, maxComponents]]]) -> mean, eigenvectors, eigenvalues\n",
      "        .   wrap PCA::operator() and add eigenvalues output parameter\n",
      "        \n",
      "        \n",
      "        \n",
      "        PCACompute2(data, mean, retainedVariance[, eigenvectors[, eigenvalues]]) -> mean, eigenvectors, eigenvalues\n",
      "        .   wrap PCA::operator() and add eigenvalues output parameter\n",
      "    \n",
      "    PCAProject(...)\n",
      "        PCAProject(data, mean, eigenvectors[, result]) -> result\n",
      "        .   wrap PCA::project\n",
      "    \n",
      "    PSNR(...)\n",
      "        PSNR(src1, src2[, R]) -> retval\n",
      "        .   @brief Computes the Peak Signal-to-Noise Ratio (PSNR) image quality metric.\n",
      "        .   \n",
      "        .   This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels (dB),\n",
      "        .   between two input arrays src1 and src2. The arrays must have the same type.\n",
      "        .   \n",
      "        .   The PSNR is calculated as follows:\n",
      "        .   \n",
      "        .   \\f[\n",
      "        .   \\texttt{PSNR} = 10 \\cdot \\log_{10}{\\left( \\frac{R^2}{MSE} \\right) }\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   where R is the maximum integer value of depth (e.g. 255 in the case of CV_8U data)\n",
      "        .   and MSE is the mean squared error between the two arrays.\n",
      "        .   \n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size as src1.\n",
      "        .   @param R the maximum pixel value (255 by default)\n",
      "    \n",
      "    RQDecomp3x3(...)\n",
      "        RQDecomp3x3(src[, mtxR[, mtxQ[, Qx[, Qy[, Qz]]]]]) -> retval, mtxR, mtxQ, Qx, Qy, Qz\n",
      "        .   @brief Computes an RQ decomposition of 3x3 matrices.\n",
      "        .   \n",
      "        .   @param src 3x3 input matrix.\n",
      "        .   @param mtxR Output 3x3 upper-triangular matrix.\n",
      "        .   @param mtxQ Output 3x3 orthogonal matrix.\n",
      "        .   @param Qx Optional output 3x3 rotation matrix around x-axis.\n",
      "        .   @param Qy Optional output 3x3 rotation matrix around y-axis.\n",
      "        .   @param Qz Optional output 3x3 rotation matrix around z-axis.\n",
      "        .   \n",
      "        .   The function computes a RQ decomposition using the given rotations. This function is used in\n",
      "        .   decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera\n",
      "        .   and a rotation matrix.\n",
      "        .   \n",
      "        .   It optionally returns three rotation matrices, one for each axis, and the three Euler angles in\n",
      "        .   degrees (as the return value) that could be used in OpenGL. Note, there is always more than one\n",
      "        .   sequence of rotations about the three principal axes that results in the same orientation of an\n",
      "        .   object, e.g. see @cite Slabaugh . Returned tree rotation matrices and corresponding three Euler angles\n",
      "        .   are only one of the possible solutions.\n",
      "    \n",
      "    Rodrigues(...)\n",
      "        Rodrigues(src[, dst[, jacobian]]) -> dst, jacobian\n",
      "        .   @brief Converts a rotation matrix to a rotation vector or vice versa.\n",
      "        .   \n",
      "        .   @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).\n",
      "        .   @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.\n",
      "        .   @param jacobian Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial\n",
      "        .   derivatives of the output array components with respect to the input array components.\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} \\theta \\leftarrow norm(r) \\\\ r  \\leftarrow r/ \\theta \\\\ R =  \\cos{\\theta} I + (1- \\cos{\\theta} ) r r^T +  \\sin{\\theta} \\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \\end{array}\\f]\n",
      "        .   \n",
      "        .   Inverse transformation can be also done easily, since\n",
      "        .   \n",
      "        .   \\f[\\sin ( \\theta ) \\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \\frac{R - R^T}{2}\\f]\n",
      "        .   \n",
      "        .   A rotation vector is a convenient and most compact representation of a rotation matrix (since any\n",
      "        .   rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry\n",
      "        .   optimization procedures like calibrateCamera, stereoCalibrate, or solvePnP .\n",
      "    \n",
      "    SVBackSubst(...)\n",
      "        SVBackSubst(w, u, vt, rhs[, dst]) -> dst\n",
      "        .   wrap SVD::backSubst\n",
      "    \n",
      "    SVDecomp(...)\n",
      "        SVDecomp(src[, w[, u[, vt[, flags]]]]) -> w, u, vt\n",
      "        .   wrap SVD::compute\n",
      "    \n",
      "    Scharr(...)\n",
      "        Scharr(src, ddepth, dx, dy[, dst[, scale[, delta[, borderType]]]]) -> dst\n",
      "        .   @brief Calculates the first x- or y- image derivative using Scharr operator.\n",
      "        .   \n",
      "        .   The function computes the first x- or y- spatial image derivative using the Scharr operator. The\n",
      "        .   call\n",
      "        .   \n",
      "        .   \\f[\\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\\f]\n",
      "        .   \n",
      "        .   is equivalent to\n",
      "        .   \n",
      "        .   \\f[\\texttt{Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .\\f]\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image of the same size and the same number of channels as src.\n",
      "        .   @param ddepth output image depth, see @ref filter_depths \"combinations\"\n",
      "        .   @param dx order of the derivative x.\n",
      "        .   @param dy order of the derivative y.\n",
      "        .   @param scale optional scale factor for the computed derivative values; by default, no scaling is\n",
      "        .   applied (see #getDerivKernels for details).\n",
      "        .   @param delta optional delta value that is added to the results prior to storing them in dst.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   @sa  cartToPolar\n",
      "    \n",
      "    SimpleBlobDetector_create(...)\n",
      "        SimpleBlobDetector_create([, parameters]) -> retval\n",
      "        .\n",
      "    \n",
      "    Sobel(...)\n",
      "        Sobel(src, ddepth, dx, dy[, dst[, ksize[, scale[, delta[, borderType]]]]]) -> dst\n",
      "        .   @brief Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.\n",
      "        .   \n",
      "        .   In all cases except one, the \\f$\\texttt{ksize} \\times \\texttt{ksize}\\f$ separable kernel is used to\n",
      "        .   calculate the derivative. When \\f$\\texttt{ksize = 1}\\f$, the \\f$3 \\times 1\\f$ or \\f$1 \\times 3\\f$\n",
      "        .   kernel is used (that is, no Gaussian smoothing is done). `ksize = 1` can only be used for the first\n",
      "        .   or the second x- or y- derivatives.\n",
      "        .   \n",
      "        .   There is also the special value `ksize = #FILTER_SCHARR (-1)` that corresponds to the \\f$3\\times3\\f$ Scharr\n",
      "        .   filter that may give more accurate results than the \\f$3\\times3\\f$ Sobel. The Scharr aperture is\n",
      "        .   \n",
      "        .   \\f[\\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\\f]\n",
      "        .   \n",
      "        .   for the x-derivative, or transposed for the y-derivative.\n",
      "        .   \n",
      "        .   The function calculates an image derivative by convolving the image with the appropriate kernel:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} =  \\frac{\\partial^{xorder+yorder} \\texttt{src}}{\\partial x^{xorder} \\partial y^{yorder}}\\f]\n",
      "        .   \n",
      "        .   The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less\n",
      "        .   resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)\n",
      "        .   or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first\n",
      "        .   case corresponds to a kernel of:\n",
      "        .   \n",
      "        .   \\f[\\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\\f]\n",
      "        .   \n",
      "        .   The second case corresponds to a kernel of:\n",
      "        .   \n",
      "        .   \\f[\\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\\f]\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image of the same size and the same number of channels as src .\n",
      "        .   @param ddepth output image depth, see @ref filter_depths \"combinations\"; in the case of\n",
      "        .       8-bit input images it will result in truncated derivatives.\n",
      "        .   @param dx order of the derivative x.\n",
      "        .   @param dy order of the derivative y.\n",
      "        .   @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n",
      "        .   @param scale optional scale factor for the computed derivative values; by default, no scaling is\n",
      "        .   applied (see #getDerivKernels for details).\n",
      "        .   @param delta optional delta value that is added to the results prior to storing them in dst.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   @sa  Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar\n",
      "    \n",
      "    SparsePyrLKOpticalFlow_create(...)\n",
      "        SparsePyrLKOpticalFlow_create([, winSize[, maxLevel[, crit[, flags[, minEigThreshold]]]]]) -> retval\n",
      "        .\n",
      "    \n",
      "    StereoBM_create(...)\n",
      "        StereoBM_create([, numDisparities[, blockSize]]) -> retval\n",
      "        .   @brief Creates StereoBM object\n",
      "        .   \n",
      "        .       @param numDisparities the disparity search range. For each pixel algorithm will find the best\n",
      "        .       disparity from 0 (default minimum disparity) to numDisparities. The search range can then be\n",
      "        .       shifted by changing the minimum disparity.\n",
      "        .       @param blockSize the linear size of the blocks compared by the algorithm. The size should be odd\n",
      "        .       (as the block is centered at the current pixel). Larger block size implies smoother, though less\n",
      "        .       accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher\n",
      "        .       chance for algorithm to find a wrong correspondence.\n",
      "        .   \n",
      "        .       The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for\n",
      "        .       a specific stereo pair.\n",
      "    \n",
      "    StereoSGBM_create(...)\n",
      "        StereoSGBM_create([, minDisparity[, numDisparities[, blockSize[, P1[, P2[, disp12MaxDiff[, preFilterCap[, uniquenessRatio[, speckleWindowSize[, speckleRange[, mode]]]]]]]]]]]) -> retval\n",
      "        .   @brief Creates StereoSGBM object\n",
      "        .   \n",
      "        .       @param minDisparity Minimum possible disparity value. Normally, it is zero but sometimes\n",
      "        .       rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.\n",
      "        .       @param numDisparities Maximum disparity minus minimum disparity. The value is always greater than\n",
      "        .       zero. In the current implementation, this parameter must be divisible by 16.\n",
      "        .       @param blockSize Matched block size. It must be an odd number \\>=1 . Normally, it should be\n",
      "        .       somewhere in the 3..11 range.\n",
      "        .       @param P1 The first parameter controlling the disparity smoothness. See below.\n",
      "        .       @param P2 The second parameter controlling the disparity smoothness. The larger the values are,\n",
      "        .       the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1\n",
      "        .       between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor\n",
      "        .       pixels. The algorithm requires P2 \\> P1 . See stereo_match.cpp sample where some reasonably good\n",
      "        .       P1 and P2 values are shown (like 8\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize and\n",
      "        .       32\\*number_of_image_channels\\*SADWindowSize\\*SADWindowSize , respectively).\n",
      "        .       @param disp12MaxDiff Maximum allowed difference (in integer pixel units) in the left-right\n",
      "        .       disparity check. Set it to a non-positive value to disable the check.\n",
      "        .       @param preFilterCap Truncation value for the prefiltered image pixels. The algorithm first\n",
      "        .       computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval.\n",
      "        .       The result values are passed to the Birchfield-Tomasi pixel cost function.\n",
      "        .       @param uniquenessRatio Margin in percentage by which the best (minimum) computed cost function\n",
      "        .       value should \"win\" the second best value to consider the found match correct. Normally, a value\n",
      "        .       within the 5-15 range is good enough.\n",
      "        .       @param speckleWindowSize Maximum size of smooth disparity regions to consider their noise speckles\n",
      "        .       and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the\n",
      "        .       50-200 range.\n",
      "        .       @param speckleRange Maximum disparity variation within each connected component. If you do speckle\n",
      "        .       filtering, set the parameter to a positive value, it will be implicitly multiplied by 16.\n",
      "        .       Normally, 1 or 2 is good enough.\n",
      "        .       @param mode Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming\n",
      "        .       algorithm. It will consume O(W\\*H\\*numDisparities) bytes, which is large for 640x480 stereo and\n",
      "        .       huge for HD-size pictures. By default, it is set to false .\n",
      "        .   \n",
      "        .       The first constructor initializes StereoSGBM with all the default parameters. So, you only have to\n",
      "        .       set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter\n",
      "        .       to a custom value.\n",
      "    \n",
      "    Stitcher_create(...)\n",
      "        Stitcher_create([, mode]) -> retval\n",
      "        .   @brief Creates a Stitcher configured in one of the stitching modes.\n",
      "        .   \n",
      "        .       @param mode Scenario for stitcher operation. This is usually determined by source of images\n",
      "        .       to stitch and their transformation. Default parameters will be chosen for operation in given\n",
      "        .       scenario.\n",
      "        .       @return Stitcher class instance.\n",
      "    \n",
      "    UMat_context(...)\n",
      "        UMat_context() -> retval\n",
      "        .\n",
      "    \n",
      "    UMat_queue(...)\n",
      "        UMat_queue() -> retval\n",
      "        .\n",
      "    \n",
      "    VariationalRefinement_create(...)\n",
      "        VariationalRefinement_create() -> retval\n",
      "        .   @brief Creates an instance of VariationalRefinement\n",
      "    \n",
      "    VideoWriter_fourcc(...)\n",
      "        VideoWriter_fourcc(c1, c2, c3, c4) -> retval\n",
      "        .   @brief Concatenates 4 chars to a fourcc code\n",
      "        .   \n",
      "        .       @return a fourcc code\n",
      "        .   \n",
      "        .       This static method constructs the fourcc code of the codec to be used in the constructor\n",
      "        .       VideoWriter::VideoWriter or VideoWriter::open.\n",
      "    \n",
      "    absdiff(...)\n",
      "        absdiff(src1, src2[, dst]) -> dst\n",
      "        .   @brief Calculates the per-element absolute difference between two arrays or between an array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::absdiff calculates:\n",
      "        .   *   Absolute difference between two arrays when they have the same\n",
      "        .       size and type:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} (| \\texttt{src1}(I) -  \\texttt{src2}(I)|)\\f]\n",
      "        .   *   Absolute difference between an array and a scalar when the second\n",
      "        .       array is constructed from Scalar or has as many elements as the\n",
      "        .       number of channels in `src1`:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} (| \\texttt{src1}(I) -  \\texttt{src2} |)\\f]\n",
      "        .   *   Absolute difference between a scalar and an array when the first\n",
      "        .       array is constructed from Scalar or has as many elements as the\n",
      "        .       number of channels in `src2`:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} (| \\texttt{src1} -  \\texttt{src2}(I) |)\\f]\n",
      "        .       where I is a multi-dimensional index of array elements. In case of\n",
      "        .       multi-channel arrays, each channel is processed independently.\n",
      "        .   @note Saturation is not applied when the arrays have the depth CV_32S.\n",
      "        .   You may even get a negative value in the case of overflow.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array that has the same size and type as input arrays.\n",
      "        .   @sa cv::abs(const Mat&)\n",
      "    \n",
      "    accumulate(...)\n",
      "        accumulate(src, dst[, mask]) -> dst\n",
      "        .   @brief Adds an image to the accumulator image.\n",
      "        .   \n",
      "        .   The function adds src or some of its elements to dst :\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y)  \\leftarrow \\texttt{dst} (x,y) +  \\texttt{src} (x,y)  \\quad \\text{if} \\quad \\texttt{mask} (x,y)  \\ne 0\\f]\n",
      "        .   \n",
      "        .   The function supports multi-channel images. Each channel is processed independently.\n",
      "        .   \n",
      "        .   The function cv::accumulate can be used, for example, to collect statistics of a scene background\n",
      "        .   viewed by a still camera and for the further foreground-background segmentation.\n",
      "        .   \n",
      "        .   @param src Input image of type CV_8UC(n), CV_16UC(n), CV_32FC(n) or CV_64FC(n), where n is a positive integer.\n",
      "        .   @param dst %Accumulator image with the same number of channels as input image, and a depth of CV_32F or CV_64F.\n",
      "        .   @param mask Optional operation mask.\n",
      "        .   \n",
      "        .   @sa  accumulateSquare, accumulateProduct, accumulateWeighted\n",
      "    \n",
      "    accumulateProduct(...)\n",
      "        accumulateProduct(src1, src2, dst[, mask]) -> dst\n",
      "        .   @brief Adds the per-element product of two input images to the accumulator image.\n",
      "        .   \n",
      "        .   The function adds the product of two images or their selected regions to the accumulator dst :\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y)  \\leftarrow \\texttt{dst} (x,y) +  \\texttt{src1} (x,y)  \\cdot \\texttt{src2} (x,y)  \\quad \\text{if} \\quad \\texttt{mask} (x,y)  \\ne 0\\f]\n",
      "        .   \n",
      "        .   The function supports multi-channel images. Each channel is processed independently.\n",
      "        .   \n",
      "        .   @param src1 First input image, 1- or 3-channel, 8-bit or 32-bit floating point.\n",
      "        .   @param src2 Second input image of the same type and the same size as src1 .\n",
      "        .   @param dst %Accumulator image with the same number of channels as input images, 32-bit or 64-bit\n",
      "        .   floating-point.\n",
      "        .   @param mask Optional operation mask.\n",
      "        .   \n",
      "        .   @sa  accumulate, accumulateSquare, accumulateWeighted\n",
      "    \n",
      "    accumulateSquare(...)\n",
      "        accumulateSquare(src, dst[, mask]) -> dst\n",
      "        .   @brief Adds the square of a source image to the accumulator image.\n",
      "        .   \n",
      "        .   The function adds the input image src or its selected region, raised to a power of 2, to the\n",
      "        .   accumulator dst :\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y)  \\leftarrow \\texttt{dst} (x,y) +  \\texttt{src} (x,y)^2  \\quad \\text{if} \\quad \\texttt{mask} (x,y)  \\ne 0\\f]\n",
      "        .   \n",
      "        .   The function supports multi-channel images. Each channel is processed independently.\n",
      "        .   \n",
      "        .   @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.\n",
      "        .   @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit\n",
      "        .   floating-point.\n",
      "        .   @param mask Optional operation mask.\n",
      "        .   \n",
      "        .   @sa  accumulateSquare, accumulateProduct, accumulateWeighted\n",
      "    \n",
      "    accumulateWeighted(...)\n",
      "        accumulateWeighted(src, dst, alpha[, mask]) -> dst\n",
      "        .   @brief Updates a running average.\n",
      "        .   \n",
      "        .   The function calculates the weighted sum of the input image src and the accumulator dst so that dst\n",
      "        .   becomes a running average of a frame sequence:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y)  \\leftarrow (1- \\texttt{alpha} )  \\cdot \\texttt{dst} (x,y) +  \\texttt{alpha} \\cdot \\texttt{src} (x,y)  \\quad \\text{if} \\quad \\texttt{mask} (x,y)  \\ne 0\\f]\n",
      "        .   \n",
      "        .   That is, alpha regulates the update speed (how fast the accumulator \"forgets\" about earlier images).\n",
      "        .   The function supports multi-channel images. Each channel is processed independently.\n",
      "        .   \n",
      "        .   @param src Input image as 1- or 3-channel, 8-bit or 32-bit floating point.\n",
      "        .   @param dst %Accumulator image with the same number of channels as input image, 32-bit or 64-bit\n",
      "        .   floating-point.\n",
      "        .   @param alpha Weight of the input image.\n",
      "        .   @param mask Optional operation mask.\n",
      "        .   \n",
      "        .   @sa  accumulate, accumulateSquare, accumulateProduct\n",
      "    \n",
      "    adaptiveThreshold(...)\n",
      "        adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C[, dst]) -> dst\n",
      "        .   @brief Applies an adaptive threshold to an array.\n",
      "        .   \n",
      "        .   The function transforms a grayscale image to a binary image according to the formulae:\n",
      "        .   -   **THRESH_BINARY**\n",
      "        .       \\f[dst(x,y) =  \\fork{\\texttt{maxValue}}{if \\(src(x,y) > T(x,y)\\)}{0}{otherwise}\\f]\n",
      "        .   -   **THRESH_BINARY_INV**\n",
      "        .       \\f[dst(x,y) =  \\fork{0}{if \\(src(x,y) > T(x,y)\\)}{\\texttt{maxValue}}{otherwise}\\f]\n",
      "        .   where \\f$T(x,y)\\f$ is a threshold calculated individually for each pixel (see adaptiveMethod parameter).\n",
      "        .   \n",
      "        .   The function can process the image in-place.\n",
      "        .   \n",
      "        .   @param src Source 8-bit single-channel image.\n",
      "        .   @param dst Destination image of the same size and the same type as src.\n",
      "        .   @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied\n",
      "        .   @param adaptiveMethod Adaptive thresholding algorithm to use, see #AdaptiveThresholdTypes.\n",
      "        .   The #BORDER_REPLICATE | #BORDER_ISOLATED is used to process boundaries.\n",
      "        .   @param thresholdType Thresholding type that must be either #THRESH_BINARY or #THRESH_BINARY_INV,\n",
      "        .   see #ThresholdTypes.\n",
      "        .   @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the\n",
      "        .   pixel: 3, 5, 7, and so on.\n",
      "        .   @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it\n",
      "        .   is positive but may be zero or negative as well.\n",
      "        .   \n",
      "        .   @sa  threshold, blur, GaussianBlur\n",
      "    \n",
      "    add(...)\n",
      "        add(src1, src2[, dst[, mask[, dtype]]]) -> dst\n",
      "        .   @brief Calculates the per-element sum of two arrays or an array and a scalar.\n",
      "        .   \n",
      "        .   The function add calculates:\n",
      "        .   - Sum of two arrays when both input arrays have the same size and the same number of channels:\n",
      "        .   \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1}(I) +  \\texttt{src2}(I)) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   - Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of\n",
      "        .   elements as `src1.channels()`:\n",
      "        .   \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1}(I) +  \\texttt{src2} ) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   - Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of\n",
      "        .   elements as `src2.channels()`:\n",
      "        .   \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1} +  \\texttt{src2}(I) ) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   where `I` is a multi-dimensional index of array elements. In case of multi-channel arrays, each\n",
      "        .   channel is processed independently.\n",
      "        .   \n",
      "        .   The first function in the list above can be replaced with matrix expressions:\n",
      "        .   @code{.cpp}\n",
      "        .       dst = src1 + src2;\n",
      "        .       dst += src1; // equivalent to add(dst, src1, dst);\n",
      "        .   @endcode\n",
      "        .   The input arrays and the output array can all have the same or different depths. For example, you\n",
      "        .   can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit\n",
      "        .   floating-point array. Depth of the output array is determined by the dtype parameter. In the second\n",
      "        .   and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can\n",
      "        .   be set to the default -1. In this case, the output array will have the same depth as the input\n",
      "        .   array, be it src1, src2 or both.\n",
      "        .   @note Saturation is not applied when the output array has the depth CV_32S. You may even get\n",
      "        .   result of an incorrect sign in the case of overflow.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array that has the same size and number of channels as the input array(s); the\n",
      "        .   depth is defined by dtype or src1/src2.\n",
      "        .   @param mask optional operation mask - 8-bit single channel array, that specifies elements of the\n",
      "        .   output array to be changed.\n",
      "        .   @param dtype optional depth of the output array (see the discussion below).\n",
      "        .   @sa subtract, addWeighted, scaleAdd, Mat::convertTo\n",
      "    \n",
      "    addText(...)\n",
      "        addText(img, text, org, nameFont[, pointSize[, color[, weight[, style[, spacing]]]]]) -> None\n",
      "        .   @brief Draws a text on the image.\n",
      "        .   \n",
      "        .   @param img 8-bit 3-channel image where the text should be drawn.\n",
      "        .   @param text Text to write on an image.\n",
      "        .   @param org Point(x,y) where the text should start on an image.\n",
      "        .   @param nameFont Name of the font. The name should match the name of a system font (such as\n",
      "        .   *Times*). If the font is not found, a default one is used.\n",
      "        .   @param pointSize Size of the font. If not specified, equal zero or negative, the point size of the\n",
      "        .   font is set to a system-dependent default value. Generally, this is 12 points.\n",
      "        .   @param color Color of the font in BGRA where A = 255 is fully transparent.\n",
      "        .   @param weight Font weight. Available operation flags are : cv::QtFontWeights You can also specify a positive integer for better control.\n",
      "        .   @param style Font style. Available operation flags are : cv::QtFontStyles\n",
      "        .   @param spacing Spacing between characters. It can be negative or positive.\n",
      "    \n",
      "    addWeighted(...)\n",
      "        addWeighted(src1, alpha, src2, beta, gamma[, dst[, dtype]]) -> dst\n",
      "        .   @brief Calculates the weighted sum of two arrays.\n",
      "        .   \n",
      "        .   The function addWeighted calculates the weighted sum of two arrays as follows:\n",
      "        .   \\f[\\texttt{dst} (I)= \\texttt{saturate} ( \\texttt{src1} (I)* \\texttt{alpha} +  \\texttt{src2} (I)* \\texttt{beta} +  \\texttt{gamma} )\\f]\n",
      "        .   where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each\n",
      "        .   channel is processed independently.\n",
      "        .   The function can be replaced with a matrix expression:\n",
      "        .   @code{.cpp}\n",
      "        .       dst = src1*alpha + src2*beta + gamma;\n",
      "        .   @endcode\n",
      "        .   @note Saturation is not applied when the output array has the depth CV_32S. You may even get\n",
      "        .   result of an incorrect sign in the case of overflow.\n",
      "        .   @param src1 first input array.\n",
      "        .   @param alpha weight of the first array elements.\n",
      "        .   @param src2 second input array of the same size and channel number as src1.\n",
      "        .   @param beta weight of the second array elements.\n",
      "        .   @param gamma scalar added to each sum.\n",
      "        .   @param dst output array that has the same size and number of channels as the input arrays.\n",
      "        .   @param dtype optional depth of the output array; when both input arrays have the same depth, dtype\n",
      "        .   can be set to -1, which will be equivalent to src1.depth().\n",
      "        .   @sa  add, subtract, scaleAdd, Mat::convertTo\n",
      "    \n",
      "    applyColorMap(...)\n",
      "        applyColorMap(src, colormap[, dst]) -> dst\n",
      "        .   @brief Applies a GNU Octave/MATLAB equivalent colormap on a given image.\n",
      "        .   \n",
      "        .   @param src The source image, grayscale or colored of type CV_8UC1 or CV_8UC3.\n",
      "        .   @param dst The result is the colormapped source image. Note: Mat::create is called on dst.\n",
      "        .   @param colormap The colormap to apply, see #ColormapTypes\n",
      "        \n",
      "        \n",
      "        \n",
      "        applyColorMap(src, userColor[, dst]) -> dst\n",
      "        .   @brief Applies a user colormap on a given image.\n",
      "        .   \n",
      "        .   @param src The source image, grayscale or colored of type CV_8UC1 or CV_8UC3.\n",
      "        .   @param dst The result is the colormapped source image. Note: Mat::create is called on dst.\n",
      "        .   @param userColor The colormap to apply of type CV_8UC1 or CV_8UC3 and size 256\n",
      "    \n",
      "    approxPolyDP(...)\n",
      "        approxPolyDP(curve, epsilon, closed[, approxCurve]) -> approxCurve\n",
      "        .   @brief Approximates a polygonal curve(s) with the specified precision.\n",
      "        .   \n",
      "        .   The function cv::approxPolyDP approximates a curve or a polygon with another curve/polygon with less\n",
      "        .   vertices so that the distance between them is less or equal to the specified precision. It uses the\n",
      "        .   Douglas-Peucker algorithm <http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm>\n",
      "        .   \n",
      "        .   @param curve Input vector of a 2D point stored in std::vector or Mat\n",
      "        .   @param approxCurve Result of the approximation. The type should match the type of the input curve.\n",
      "        .   @param epsilon Parameter specifying the approximation accuracy. This is the maximum distance\n",
      "        .   between the original curve and its approximation.\n",
      "        .   @param closed If true, the approximated curve is closed (its first and last vertices are\n",
      "        .   connected). Otherwise, it is not closed.\n",
      "    \n",
      "    arcLength(...)\n",
      "        arcLength(curve, closed) -> retval\n",
      "        .   @brief Calculates a contour perimeter or a curve length.\n",
      "        .   \n",
      "        .   The function computes a curve length or a closed contour perimeter.\n",
      "        .   \n",
      "        .   @param curve Input vector of 2D points, stored in std::vector or Mat.\n",
      "        .   @param closed Flag indicating whether the curve is closed or not.\n",
      "    \n",
      "    arrowedLine(...)\n",
      "        arrowedLine(img, pt1, pt2, color[, thickness[, line_type[, shift[, tipLength]]]]) -> img\n",
      "        .   @brief Draws a arrow segment pointing from the first point to the second one.\n",
      "        .   \n",
      "        .   The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also #line.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param pt1 The point the arrow starts from.\n",
      "        .   @param pt2 The point the arrow points to.\n",
      "        .   @param color Line color.\n",
      "        .   @param thickness Line thickness.\n",
      "        .   @param line_type Type of the line. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the point coordinates.\n",
      "        .   @param tipLength The length of the arrow tip in relation to the arrow length\n",
      "    \n",
      "    batchDistance(...)\n",
      "        batchDistance(src1, src2, dtype[, dist[, nidx[, normType[, K[, mask[, update[, crosscheck]]]]]]]) -> dist, nidx\n",
      "        .   @brief naive nearest neighbor finder\n",
      "        .   \n",
      "        .   see http://en.wikipedia.org/wiki/Nearest_neighbor_search\n",
      "        .   @todo document\n",
      "    \n",
      "    bilateralFilter(...)\n",
      "        bilateralFilter(src, d, sigmaColor, sigmaSpace[, dst[, borderType]]) -> dst\n",
      "        .   @brief Applies the bilateral filter to an image.\n",
      "        .   \n",
      "        .   The function applies bilateral filtering to the input image, as described in\n",
      "        .   http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html\n",
      "        .   bilateralFilter can reduce unwanted noise very well while keeping edges fairly sharp. However, it is\n",
      "        .   very slow compared to most filters.\n",
      "        .   \n",
      "        .   _Sigma values_: For simplicity, you can set the 2 sigma values to be the same. If they are small (\\<\n",
      "        .   10), the filter will not have much effect, whereas if they are large (\\> 150), they will have a very\n",
      "        .   strong effect, making the image look \"cartoonish\".\n",
      "        .   \n",
      "        .   _Filter size_: Large filters (d \\> 5) are very slow, so it is recommended to use d=5 for real-time\n",
      "        .   applications, and perhaps d=9 for offline applications that need heavy noise filtering.\n",
      "        .   \n",
      "        .   This filter does not work inplace.\n",
      "        .   @param src Source 8-bit or floating-point, 1-channel or 3-channel image.\n",
      "        .   @param dst Destination image of the same size and type as src .\n",
      "        .   @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,\n",
      "        .   it is computed from sigmaSpace.\n",
      "        .   @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that\n",
      "        .   farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting\n",
      "        .   in larger areas of semi-equal color.\n",
      "        .   @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that\n",
      "        .   farther pixels will influence each other as long as their colors are close enough (see sigmaColor\n",
      "        .   ). When d\\>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is\n",
      "        .   proportional to sigmaSpace.\n",
      "        .   @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes\n",
      "    \n",
      "    bitwise_and(...)\n",
      "        bitwise_and(src1, src2[, dst[, mask]]) -> dst\n",
      "        .   @brief computes bitwise conjunction of the two arrays (dst = src1 & src2)\n",
      "        .   Calculates the per-element bit-wise conjunction of two arrays or an\n",
      "        .   array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::bitwise_and calculates the per-element bit-wise logical conjunction for:\n",
      "        .   *   Two arrays when src1 and src2 have the same size:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\wedge \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   An array and a scalar when src2 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src1.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\wedge \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   A scalar and an array when src1 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src2.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1}  \\wedge \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   In case of floating-point arrays, their machine-specific bit\n",
      "        .   representations (usually IEEE754-compliant) are used for the operation.\n",
      "        .   In case of multi-channel arrays, each channel is processed\n",
      "        .   independently. In the second and third cases above, the scalar is first\n",
      "        .   converted to the array type.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array that has the same size and type as the input\n",
      "        .   arrays.\n",
      "        .   @param mask optional operation mask, 8-bit single channel array, that\n",
      "        .   specifies elements of the output array to be changed.\n",
      "    \n",
      "    bitwise_not(...)\n",
      "        bitwise_not(src[, dst[, mask]]) -> dst\n",
      "        .   @brief  Inverts every bit of an array.\n",
      "        .   \n",
      "        .   The function cv::bitwise_not calculates per-element bit-wise inversion of the input\n",
      "        .   array:\n",
      "        .   \\f[\\texttt{dst} (I) =  \\neg \\texttt{src} (I)\\f]\n",
      "        .   In case of a floating-point input array, its machine-specific bit\n",
      "        .   representation (usually IEEE754-compliant) is used for the operation. In\n",
      "        .   case of multi-channel arrays, each channel is processed independently.\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array that has the same size and type as the input\n",
      "        .   array.\n",
      "        .   @param mask optional operation mask, 8-bit single channel array, that\n",
      "        .   specifies elements of the output array to be changed.\n",
      "    \n",
      "    bitwise_or(...)\n",
      "        bitwise_or(src1, src2[, dst[, mask]]) -> dst\n",
      "        .   @brief Calculates the per-element bit-wise disjunction of two arrays or an\n",
      "        .   array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::bitwise_or calculates the per-element bit-wise logical disjunction for:\n",
      "        .   *   Two arrays when src1 and src2 have the same size:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\vee \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   An array and a scalar when src2 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src1.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\vee \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   A scalar and an array when src1 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src2.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1}  \\vee \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   In case of floating-point arrays, their machine-specific bit\n",
      "        .   representations (usually IEEE754-compliant) are used for the operation.\n",
      "        .   In case of multi-channel arrays, each channel is processed\n",
      "        .   independently. In the second and third cases above, the scalar is first\n",
      "        .   converted to the array type.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array that has the same size and type as the input\n",
      "        .   arrays.\n",
      "        .   @param mask optional operation mask, 8-bit single channel array, that\n",
      "        .   specifies elements of the output array to be changed.\n",
      "    \n",
      "    bitwise_xor(...)\n",
      "        bitwise_xor(src1, src2[, dst[, mask]]) -> dst\n",
      "        .   @brief Calculates the per-element bit-wise \"exclusive or\" operation on two\n",
      "        .   arrays or an array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::bitwise_xor calculates the per-element bit-wise logical \"exclusive-or\"\n",
      "        .   operation for:\n",
      "        .   *   Two arrays when src1 and src2 have the same size:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\oplus \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   An array and a scalar when src2 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src1.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\oplus \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   *   A scalar and an array when src1 is constructed from Scalar or has\n",
      "        .       the same number of elements as `src2.channels()`:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1}  \\oplus \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\f]\n",
      "        .   In case of floating-point arrays, their machine-specific bit\n",
      "        .   representations (usually IEEE754-compliant) are used for the operation.\n",
      "        .   In case of multi-channel arrays, each channel is processed\n",
      "        .   independently. In the 2nd and 3rd cases above, the scalar is first\n",
      "        .   converted to the array type.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array that has the same size and type as the input\n",
      "        .   arrays.\n",
      "        .   @param mask optional operation mask, 8-bit single channel array, that\n",
      "        .   specifies elements of the output array to be changed.\n",
      "    \n",
      "    blur(...)\n",
      "        blur(src, ksize[, dst[, anchor[, borderType]]]) -> dst\n",
      "        .   @brief Blurs an image using the normalized box filter.\n",
      "        .   \n",
      "        .   The function smooths an image using the kernel:\n",
      "        .   \n",
      "        .   \\f[\\texttt{K} =  \\frac{1}{\\texttt{ksize.width*ksize.height}} \\begin{bmatrix} 1 & 1 & 1 &  \\cdots & 1 & 1  \\\\ 1 & 1 & 1 &  \\cdots & 1 & 1  \\\\ \\hdotsfor{6} \\\\ 1 & 1 & 1 &  \\cdots & 1 & 1  \\\\ \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   The call `blur(src, dst, ksize, anchor, borderType)` is equivalent to `boxFilter(src, dst, src.type(),\n",
      "        .   anchor, true, borderType)`.\n",
      "        .   \n",
      "        .   @param src input image; it can have any number of channels, which are processed independently, but\n",
      "        .   the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "        .   @param dst output image of the same size and type as src.\n",
      "        .   @param ksize blurring kernel size.\n",
      "        .   @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n",
      "        .   center.\n",
      "        .   @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes\n",
      "        .   @sa  boxFilter, bilateralFilter, GaussianBlur, medianBlur\n",
      "    \n",
      "    borderInterpolate(...)\n",
      "        borderInterpolate(p, len, borderType) -> retval\n",
      "        .   @brief Computes the source location of an extrapolated pixel.\n",
      "        .   \n",
      "        .   The function computes and returns the coordinate of a donor pixel corresponding to the specified\n",
      "        .   extrapolated pixel when using the specified extrapolation border mode. For example, if you use\n",
      "        .   cv::BORDER_WRAP mode in the horizontal direction, cv::BORDER_REFLECT_101 in the vertical direction and\n",
      "        .   want to compute value of the \"virtual\" pixel Point(-5, 100) in a floating-point image img , it\n",
      "        .   looks like:\n",
      "        .   @code{.cpp}\n",
      "        .       float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),\n",
      "        .                                 borderInterpolate(-5, img.cols, cv::BORDER_WRAP));\n",
      "        .   @endcode\n",
      "        .   Normally, the function is not called directly. It is used inside filtering functions and also in\n",
      "        .   copyMakeBorder.\n",
      "        .   @param p 0-based coordinate of the extrapolated pixel along one of the axes, likely \\<0 or \\>= len\n",
      "        .   @param len Length of the array along the corresponding axis.\n",
      "        .   @param borderType Border type, one of the #BorderTypes, except for #BORDER_TRANSPARENT and\n",
      "        .   #BORDER_ISOLATED . When borderType==#BORDER_CONSTANT , the function always returns -1, regardless\n",
      "        .   of p and len.\n",
      "        .   \n",
      "        .   @sa copyMakeBorder\n",
      "    \n",
      "    boundingRect(...)\n",
      "        boundingRect(array) -> retval\n",
      "        .   @brief Calculates the up-right bounding rectangle of a point set or non-zero pixels of gray-scale image.\n",
      "        .   \n",
      "        .   The function calculates and returns the minimal up-right bounding rectangle for the specified point set or\n",
      "        .   non-zero pixels of gray-scale image.\n",
      "        .   \n",
      "        .   @param array Input gray-scale image or 2D point set, stored in std::vector or Mat.\n",
      "    \n",
      "    boxFilter(...)\n",
      "        boxFilter(src, ddepth, ksize[, dst[, anchor[, normalize[, borderType]]]]) -> dst\n",
      "        .   @brief Blurs an image using the box filter.\n",
      "        .   \n",
      "        .   The function smooths an image using the kernel:\n",
      "        .   \n",
      "        .   \\f[\\texttt{K} =  \\alpha \\begin{bmatrix} 1 & 1 & 1 &  \\cdots & 1 & 1  \\\\ 1 & 1 & 1 &  \\cdots & 1 & 1  \\\\ \\hdotsfor{6} \\\\ 1 & 1 & 1 &  \\cdots & 1 & 1 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \n",
      "        .   \\f[\\alpha = \\fork{\\frac{1}{\\texttt{ksize.width*ksize.height}}}{when \\texttt{normalize=true}}{1}{otherwise}\\f]\n",
      "        .   \n",
      "        .   Unnormalized box filter is useful for computing various integral characteristics over each pixel\n",
      "        .   neighborhood, such as covariance matrices of image derivatives (used in dense optical flow\n",
      "        .   algorithms, and so on). If you need to compute pixel sums over variable-size windows, use #integral.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image of the same size and type as src.\n",
      "        .   @param ddepth the output image depth (-1 to use src.depth()).\n",
      "        .   @param ksize blurring kernel size.\n",
      "        .   @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel\n",
      "        .   center.\n",
      "        .   @param normalize flag, specifying whether the kernel is normalized by its area or not.\n",
      "        .   @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes\n",
      "        .   @sa  blur, bilateralFilter, GaussianBlur, medianBlur, integral\n",
      "    \n",
      "    boxPoints(...)\n",
      "        boxPoints(box[, points]) -> points\n",
      "        .   @brief Finds the four vertices of a rotated rect. Useful to draw the rotated rectangle.\n",
      "        .   \n",
      "        .   The function finds the four vertices of a rotated rectangle. This function is useful to draw the\n",
      "        .   rectangle. In C++, instead of using this function, you can directly use RotatedRect::points method. Please\n",
      "        .   visit the @ref tutorial_bounding_rotated_ellipses \"tutorial on Creating Bounding rotated boxes and ellipses for contours\" for more information.\n",
      "        .   \n",
      "        .   @param box The input rotated rectangle. It may be the output of\n",
      "        .   @param points The output array of four vertices of rectangles.\n",
      "    \n",
      "    buildOpticalFlowPyramid(...)\n",
      "        buildOpticalFlowPyramid(img, winSize, maxLevel[, pyramid[, withDerivatives[, pyrBorder[, derivBorder[, tryReuseInputImage]]]]]) -> retval, pyramid\n",
      "        .   @brief Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.\n",
      "        .   \n",
      "        .   @param img 8-bit input image.\n",
      "        .   @param pyramid output pyramid.\n",
      "        .   @param winSize window size of optical flow algorithm. Must be not less than winSize argument of\n",
      "        .   calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.\n",
      "        .   @param maxLevel 0-based maximal pyramid level number.\n",
      "        .   @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is\n",
      "        .   constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.\n",
      "        .   @param pyrBorder the border mode for pyramid layers.\n",
      "        .   @param derivBorder the border mode for gradients.\n",
      "        .   @param tryReuseInputImage put ROI of input image into the pyramid if possible. You can pass false\n",
      "        .   to force data copying.\n",
      "        .   @return number of levels in constructed pyramid. Can be less than maxLevel.\n",
      "    \n",
      "    calcBackProject(...)\n",
      "        calcBackProject(images, channels, hist, ranges, scale[, dst]) -> dst\n",
      "        .   @overload\n",
      "    \n",
      "    calcCovarMatrix(...)\n",
      "        calcCovarMatrix(samples, mean, flags[, covar[, ctype]]) -> covar, mean\n",
      "        .   @overload\n",
      "        .   @note use #COVAR_ROWS or #COVAR_COLS flag\n",
      "        .   @param samples samples stored as rows/columns of a single matrix.\n",
      "        .   @param covar output covariance matrix of the type ctype and square size.\n",
      "        .   @param mean input or output (depending on the flags) array as the average value of the input vectors.\n",
      "        .   @param flags operation flags as a combination of #CovarFlags\n",
      "        .   @param ctype type of the matrixl; it equals 'CV_64F' by default.\n",
      "    \n",
      "    calcHist(...)\n",
      "        calcHist(images, channels, mask, histSize, ranges[, hist[, accumulate]]) -> hist\n",
      "        .   @overload\n",
      "    \n",
      "    calcOpticalFlowFarneback(...)\n",
      "        calcOpticalFlowFarneback(prev, next, flow, pyr_scale, levels, winsize, iterations, poly_n, poly_sigma, flags) -> flow\n",
      "        .   @brief Computes a dense optical flow using the Gunnar Farneback's algorithm.\n",
      "        .   \n",
      "        .   @param prev first 8-bit single-channel input image.\n",
      "        .   @param next second input image of the same size and the same type as prev.\n",
      "        .   @param flow computed flow image that has the same size as prev and type CV_32FC2.\n",
      "        .   @param pyr_scale parameter, specifying the image scale (\\<1) to build pyramids for each image;\n",
      "        .   pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous\n",
      "        .   one.\n",
      "        .   @param levels number of pyramid layers including the initial image; levels=1 means that no extra\n",
      "        .   layers are created and only the original images are used.\n",
      "        .   @param winsize averaging window size; larger values increase the algorithm robustness to image\n",
      "        .   noise and give more chances for fast motion detection, but yield more blurred motion field.\n",
      "        .   @param iterations number of iterations the algorithm does at each pyramid level.\n",
      "        .   @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel;\n",
      "        .   larger values mean that the image will be approximated with smoother surfaces, yielding more\n",
      "        .   robust algorithm and more blurred motion field, typically poly_n =5 or 7.\n",
      "        .   @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a\n",
      "        .   basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a\n",
      "        .   good value would be poly_sigma=1.5.\n",
      "        .   @param flags operation flags that can be a combination of the following:\n",
      "        .    -   **OPTFLOW_USE_INITIAL_FLOW** uses the input flow as an initial flow approximation.\n",
      "        .    -   **OPTFLOW_FARNEBACK_GAUSSIAN** uses the Gaussian \\f$\\texttt{winsize}\\times\\texttt{winsize}\\f$\n",
      "        .        filter instead of a box filter of the same size for optical flow estimation; usually, this\n",
      "        .        option gives z more accurate flow than with a box filter, at the cost of lower speed;\n",
      "        .        normally, winsize for a Gaussian window should be set to a larger value to achieve the same\n",
      "        .        level of robustness.\n",
      "        .   \n",
      "        .   The function finds an optical flow for each prev pixel using the @cite Farneback2003 algorithm so that\n",
      "        .   \n",
      "        .   \\f[\\texttt{prev} (y,x)  \\sim \\texttt{next} ( y + \\texttt{flow} (y,x)[1],  x + \\texttt{flow} (y,x)[0])\\f]\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   -   An example using the optical flow algorithm described by Gunnar Farneback can be found at\n",
      "        .       opencv_source_code/samples/cpp/fback.cpp\n",
      "        .   -   (Python) An example using the optical flow algorithm described by Gunnar Farneback can be\n",
      "        .       found at opencv_source_code/samples/python/opt_flow.py\n",
      "    \n",
      "    calcOpticalFlowPyrLK(...)\n",
      "        calcOpticalFlowPyrLK(prevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]) -> nextPts, status, err\n",
      "        .   @brief Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with\n",
      "        .   pyramids.\n",
      "        .   \n",
      "        .   @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
      "        .   @param nextImg second input image or pyramid of the same size and the same type as prevImg.\n",
      "        .   @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be\n",
      "        .   single-precision floating-point numbers.\n",
      "        .   @param nextPts output vector of 2D points (with single-precision floating-point coordinates)\n",
      "        .   containing the calculated new positions of input features in the second image; when\n",
      "        .   OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n",
      "        .   @param status output status vector (of unsigned chars); each element of the vector is set to 1 if\n",
      "        .   the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
      "        .   @param err output vector of errors; each element of the vector is set to an error for the\n",
      "        .   corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't\n",
      "        .   found then the error is not defined (use the status parameter to find such cases).\n",
      "        .   @param winSize size of the search window at each pyramid level.\n",
      "        .   @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single\n",
      "        .   level), if set to 1, two levels are used, and so on; if pyramids are passed to input then\n",
      "        .   algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
      "        .   @param criteria parameter, specifying the termination criteria of the iterative search algorithm\n",
      "        .   (after the specified maximum number of iterations criteria.maxCount or when the search window\n",
      "        .   moves by less than criteria.epsilon.\n",
      "        .   @param flags operation flags:\n",
      "        .    -   **OPTFLOW_USE_INITIAL_FLOW** uses initial estimations, stored in nextPts; if the flag is\n",
      "        .        not set, then prevPts is copied to nextPts and is considered the initial estimate.\n",
      "        .    -   **OPTFLOW_LK_GET_MIN_EIGENVALS** use minimum eigen values as an error measure (see\n",
      "        .        minEigThreshold description); if the flag is not set, then L1 distance between patches\n",
      "        .        around the original and a moved point, divided by number of pixels in a window, is used as a\n",
      "        .        error measure.\n",
      "        .   @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of\n",
      "        .   optical flow equations (this matrix is called a spatial gradient matrix in @cite Bouguet00), divided\n",
      "        .   by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding\n",
      "        .   feature is filtered out and its flow is not processed, so it allows to remove bad points and get a\n",
      "        .   performance boost.\n",
      "        .   \n",
      "        .   The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See\n",
      "        .   @cite Bouguet00 . The function is parallelized with the TBB library.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   -   An example using the Lucas-Kanade optical flow algorithm can be found at\n",
      "        .       opencv_source_code/samples/cpp/lkdemo.cpp\n",
      "        .   -   (Python) An example using the Lucas-Kanade optical flow algorithm can be found at\n",
      "        .       opencv_source_code/samples/python/lk_track.py\n",
      "        .   -   (Python) An example using the Lucas-Kanade tracker for homography matching can be found at\n",
      "        .       opencv_source_code/samples/python/lk_homography.py\n",
      "    \n",
      "    calibrateCamera(...)\n",
      "        calibrateCamera(objectPoints, imagePoints, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, flags[, criteria]]]]) -> retval, cameraMatrix, distCoeffs, rvecs, tvecs\n",
      "        .   @overload\n",
      "    \n",
      "    calibrateCameraExtended(...)\n",
      "        calibrateCameraExtended(objectPoints, imagePoints, imageSize, cameraMatrix, distCoeffs[, rvecs[, tvecs[, stdDeviationsIntrinsics[, stdDeviationsExtrinsics[, perViewErrors[, flags[, criteria]]]]]]]) -> retval, cameraMatrix, distCoeffs, rvecs, tvecs, stdDeviationsIntrinsics, stdDeviationsExtrinsics, perViewErrors\n",
      "        .   @brief Finds the camera intrinsic and extrinsic parameters from several views of a calibration pattern.\n",
      "        .   \n",
      "        .   @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in\n",
      "        .   the calibration pattern coordinate space (e.g. std::vector<std::vector<cv::Vec3f>>). The outer\n",
      "        .   vector contains as many elements as the number of the pattern views. If the same calibration pattern\n",
      "        .   is shown in each view and it is fully visible, all the vectors will be the same. Although, it is\n",
      "        .   possible to use partially occluded patterns, or even different patterns in different views. Then,\n",
      "        .   the vectors will be different. The points are 3D, but since they are in a pattern coordinate system,\n",
      "        .   then, if the rig is planar, it may make sense to put the model to a XY coordinate plane so that\n",
      "        .   Z-coordinate of each input object point is 0.\n",
      "        .   In the old interface all the vectors of object points from different views are concatenated\n",
      "        .   together.\n",
      "        .   @param imagePoints In the new interface it is a vector of vectors of the projections of calibration\n",
      "        .   pattern points (e.g. std::vector<std::vector<cv::Vec2f>>). imagePoints.size() and\n",
      "        .   objectPoints.size() and imagePoints[i].size() must be equal to objectPoints[i].size() for each i.\n",
      "        .   In the old interface all the vectors of object points from different views are concatenated\n",
      "        .   together.\n",
      "        .   @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n",
      "        .   @param cameraMatrix Output 3x3 floating-point camera matrix\n",
      "        .   \\f$A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ . If CV\\_CALIB\\_USE\\_INTRINSIC\\_GUESS\n",
      "        .   and/or CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be\n",
      "        .   initialized before calling the function.\n",
      "        .   @param distCoeffs Output vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements.\n",
      "        .   @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view\n",
      "        .   (e.g. std::vector<cv::Mat>>). That is, each k-th rotation vector together with the corresponding\n",
      "        .   k-th translation vector (see the next output parameter description) brings the calibration pattern\n",
      "        .   from the model coordinate space (in which object points are specified) to the world coordinate\n",
      "        .   space, that is, a real position of the calibration pattern in the k-th pattern view (k=0.. *M* -1).\n",
      "        .   @param tvecs Output vector of translation vectors estimated for each pattern view.\n",
      "        .   @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic parameters.\n",
      "        .    Order of deviations values:\n",
      "        .   \\f$(f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3,\n",
      "        .    s_4, \\tau_x, \\tau_y)\\f$ If one of parameters is not estimated, it's deviation is equals to zero.\n",
      "        .   @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic parameters.\n",
      "        .    Order of deviations values: \\f$(R_1, T_1, \\dotsc , R_M, T_M)\\f$ where M is number of pattern views,\n",
      "        .    \\f$R_i, T_i\\f$ are concatenated 1x3 vectors.\n",
      "        .    @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n",
      "        .   @param flags Different flags that may be zero or a combination of the following values:\n",
      "        .   -   **CALIB_USE_INTRINSIC_GUESS** cameraMatrix contains valid initial values of\n",
      "        .   fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image\n",
      "        .   center ( imageSize is used), and focal distances are computed in a least-squares fashion.\n",
      "        .   Note, that if intrinsic parameters are known, there is no need to use this function just to\n",
      "        .   estimate extrinsic parameters. Use solvePnP instead.\n",
      "        .   -   **CALIB_FIX_PRINCIPAL_POINT** The principal point is not changed during the global\n",
      "        .   optimization. It stays at the center or at a different location specified when\n",
      "        .   CALIB_USE_INTRINSIC_GUESS is set too.\n",
      "        .   -   **CALIB_FIX_ASPECT_RATIO** The functions considers only fy as a free parameter. The\n",
      "        .   ratio fx/fy stays the same as in the input cameraMatrix . When\n",
      "        .   CALIB_USE_INTRINSIC_GUESS is not set, the actual input values of fx and fy are\n",
      "        .   ignored, only their ratio is computed and used further.\n",
      "        .   -   **CALIB_ZERO_TANGENT_DIST** Tangential distortion coefficients \\f$(p_1, p_2)\\f$ are set\n",
      "        .   to zeros and stay zero.\n",
      "        .   -   **CALIB_FIX_K1,...,CALIB_FIX_K6** The corresponding radial distortion\n",
      "        .   coefficient is not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is\n",
      "        .   set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   -   **CALIB_RATIONAL_MODEL** Coefficients k4, k5, and k6 are enabled. To provide the\n",
      "        .   backward compatibility, this extra flag should be explicitly specified to make the\n",
      "        .   calibration function use the rational model and return 8 coefficients. If the flag is not\n",
      "        .   set, the function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_THIN_PRISM_MODEL** Coefficients s1, s2, s3 and s4 are enabled. To provide the\n",
      "        .   backward compatibility, this extra flag should be explicitly specified to make the\n",
      "        .   calibration function use the thin prism model and return 12 coefficients. If the flag is not\n",
      "        .   set, the function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_FIX_S1_S2_S3_S4** The thin prism distortion coefficients are not changed during\n",
      "        .   the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n",
      "        .   supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   -   **CALIB_TILTED_MODEL** Coefficients tauX and tauY are enabled. To provide the\n",
      "        .   backward compatibility, this extra flag should be explicitly specified to make the\n",
      "        .   calibration function use the tilted sensor model and return 14 coefficients. If the flag is not\n",
      "        .   set, the function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_FIX_TAUX_TAUY** The coefficients of the tilted sensor model are not changed during\n",
      "        .   the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n",
      "        .   supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   @param criteria Termination criteria for the iterative optimization algorithm.\n",
      "        .   \n",
      "        .   @return the overall RMS re-projection error.\n",
      "        .   \n",
      "        .   The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n",
      "        .   views. The algorithm is based on @cite Zhang2000 and @cite BouguetMCT . The coordinates of 3D object\n",
      "        .   points and their corresponding 2D projections in each view must be specified. That may be achieved\n",
      "        .   by using an object with a known geometry and easily detectable feature points. Such an object is\n",
      "        .   called a calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as\n",
      "        .   a calibration rig (see findChessboardCorners ). Currently, initialization of intrinsic parameters\n",
      "        .   (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration\n",
      "        .   patterns (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also\n",
      "        .   be used as long as initial cameraMatrix is provided.\n",
      "        .   \n",
      "        .   The algorithm performs the following steps:\n",
      "        .   \n",
      "        .   -   Compute the initial intrinsic parameters (the option only available for planar calibration\n",
      "        .       patterns) or read them from the input parameters. The distortion coefficients are all set to\n",
      "        .       zeros initially unless some of CALIB_FIX_K? are specified.\n",
      "        .   \n",
      "        .   -   Estimate the initial camera pose as if the intrinsic parameters have been already known. This is\n",
      "        .       done using solvePnP .\n",
      "        .   \n",
      "        .   -   Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error,\n",
      "        .       that is, the total sum of squared distances between the observed feature points imagePoints and\n",
      "        .       the projected (using the current estimates for camera parameters and the poses) object points\n",
      "        .       objectPoints. See projectPoints for details.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      If you use a non-square (=non-NxN) grid and findChessboardCorners for calibration, and\n",
      "        .       calibrateCamera returns bad values (zero distortion coefficients, an image center very far from\n",
      "        .       (w/2-0.5,h/2-0.5), and/or large differences between \\f$f_x\\f$ and \\f$f_y\\f$ (ratios of 10:1 or more)),\n",
      "        .       then you have probably used patternSize=cvSize(rows,cols) instead of using\n",
      "        .       patternSize=cvSize(cols,rows) in findChessboardCorners .\n",
      "        .   \n",
      "        .   @sa\n",
      "        .      calibrateCameraRO, findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort\n",
      "    \n",
      "    calibrateCameraRO(...)\n",
      "        calibrateCameraRO(objectPoints, imagePoints, imageSize, iFixedPoint, cameraMatrix, distCoeffs[, rvecs[, tvecs[, newObjPoints[, flags[, criteria]]]]]) -> retval, cameraMatrix, distCoeffs, rvecs, tvecs, newObjPoints\n",
      "        .   @overload\n",
      "    \n",
      "    calibrateCameraROExtended(...)\n",
      "        calibrateCameraROExtended(objectPoints, imagePoints, imageSize, iFixedPoint, cameraMatrix, distCoeffs[, rvecs[, tvecs[, newObjPoints[, stdDeviationsIntrinsics[, stdDeviationsExtrinsics[, stdDeviationsObjPoints[, perViewErrors[, flags[, criteria]]]]]]]]]) -> retval, cameraMatrix, distCoeffs, rvecs, tvecs, newObjPoints, stdDeviationsIntrinsics, stdDeviationsExtrinsics, stdDeviationsObjPoints, perViewErrors\n",
      "        .   @brief Finds the camera intrinsic and extrinsic parameters from several views of a calibration pattern.\n",
      "        .   \n",
      "        .   This function is an extension of calibrateCamera() with the method of releasing object which was\n",
      "        .   proposed in @cite strobl2011iccv. In many common cases with inaccurate, unmeasured, roughly planar\n",
      "        .   targets (calibration plates), this method can dramatically improve the precision of the estimated\n",
      "        .   camera parameters. Both the object-releasing method and standard method are supported by this\n",
      "        .   function. Use the parameter **iFixedPoint** for method selection. In the internal implementation,\n",
      "        .   calibrateCamera() is a wrapper for this function.\n",
      "        .   \n",
      "        .   @param objectPoints Vector of vectors of calibration pattern points in the calibration pattern\n",
      "        .   coordinate space. See calibrateCamera() for details. If the method of releasing object to be used,\n",
      "        .   the identical calibration board must be used in each view and it must be fully visible, and all\n",
      "        .   objectPoints[i] must be the same and all points should be roughly close to a plane. **The calibration\n",
      "        .   target has to be rigid, or at least static if the camera (rather than the calibration target) is\n",
      "        .   shifted for grabbing images.**\n",
      "        .   @param imagePoints Vector of vectors of the projections of calibration pattern points. See\n",
      "        .   calibrateCamera() for details.\n",
      "        .   @param imageSize Size of the image used only to initialize the intrinsic camera matrix.\n",
      "        .   @param iFixedPoint The index of the 3D object point in objectPoints[0] to be fixed. It also acts as\n",
      "        .   a switch for calibration method selection. If object-releasing method to be used, pass in the\n",
      "        .   parameter in the range of [1, objectPoints[0].size()-2], otherwise a value out of this range will\n",
      "        .   make standard calibration method selected. Usually the top-right corner point of the calibration\n",
      "        .   board grid is recommended to be fixed when object-releasing method being utilized. According to\n",
      "        .   \\cite strobl2011iccv, two other points are also fixed. In this implementation, objectPoints[0].front\n",
      "        .   and objectPoints[0].back.z are used. With object-releasing method, accurate rvecs, tvecs and\n",
      "        .   newObjPoints are only possible if coordinates of these three fixed points are accurate enough.\n",
      "        .   @param cameraMatrix Output 3x3 floating-point camera matrix. See calibrateCamera() for details.\n",
      "        .   @param distCoeffs Output vector of distortion coefficients. See calibrateCamera() for details.\n",
      "        .   @param rvecs Output vector of rotation vectors estimated for each pattern view. See calibrateCamera()\n",
      "        .   for details.\n",
      "        .   @param tvecs Output vector of translation vectors estimated for each pattern view.\n",
      "        .   @param newObjPoints The updated output vector of calibration pattern points. The coordinates might\n",
      "        .   be scaled based on three fixed points. The returned coordinates are accurate only if the above\n",
      "        .   mentioned three fixed points are accurate. If not needed, noArray() can be passed in. This parameter\n",
      "        .   is ignored with standard calibration method.\n",
      "        .   @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic parameters.\n",
      "        .   See calibrateCamera() for details.\n",
      "        .   @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic parameters.\n",
      "        .   See calibrateCamera() for details.\n",
      "        .   @param stdDeviationsObjPoints Output vector of standard deviations estimated for refined coordinates\n",
      "        .   of calibration pattern points. It has the same size and order as objectPoints[0] vector. This\n",
      "        .   parameter is ignored with standard calibration method.\n",
      "        .    @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n",
      "        .   @param flags Different flags that may be zero or a combination of some predefined values. See\n",
      "        .   calibrateCamera() for details. If the method of releasing object is used, the calibration time may\n",
      "        .   be much longer. CALIB_USE_QR or CALIB_USE_LU could be used for faster calibration with potentially\n",
      "        .   less precise and less stable in some rare cases.\n",
      "        .   @param criteria Termination criteria for the iterative optimization algorithm.\n",
      "        .   \n",
      "        .   @return the overall RMS re-projection error.\n",
      "        .   \n",
      "        .   The function estimates the intrinsic camera parameters and extrinsic parameters for each of the\n",
      "        .   views. The algorithm is based on @cite Zhang2000, @cite BouguetMCT and @cite strobl2011iccv. See\n",
      "        .   calibrateCamera() for other detailed explanations.\n",
      "        .   @sa\n",
      "        .      calibrateCamera, findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort\n",
      "    \n",
      "    calibrateHandEye(...)\n",
      "        calibrateHandEye(R_gripper2base, t_gripper2base, R_target2cam, t_target2cam[, R_cam2gripper[, t_cam2gripper[, method]]]) -> R_cam2gripper, t_cam2gripper\n",
      "        .   @brief Computes Hand-Eye calibration: \\f$_{}^{g}\\textrm{T}_c\\f$\n",
      "        .   \n",
      "        .   @param[in] R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the gripper frame to the robot base frame (\\f$_{}^{b}\\textrm{T}_g\\f$).\n",
      "        .   This is a vector (`vector<Mat>`) that contains the rotation matrices for all the transformations\n",
      "        .   from gripper frame to robot base frame.\n",
      "        .   @param[in] t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the gripper frame to the robot base frame (\\f$_{}^{b}\\textrm{T}_g\\f$).\n",
      "        .   This is a vector (`vector<Mat>`) that contains the translation vectors for all the transformations\n",
      "        .   from gripper frame to robot base frame.\n",
      "        .   @param[in] R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the target frame to the camera frame (\\f$_{}^{c}\\textrm{T}_t\\f$).\n",
      "        .   This is a vector (`vector<Mat>`) that contains the rotation matrices for all the transformations\n",
      "        .   from calibration target frame to camera frame.\n",
      "        .   @param[in] t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the target frame to the camera frame (\\f$_{}^{c}\\textrm{T}_t\\f$).\n",
      "        .   This is a vector (`vector<Mat>`) that contains the translation vectors for all the transformations\n",
      "        .   from calibration target frame to camera frame.\n",
      "        .   @param[out] R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the camera frame to the gripper frame (\\f$_{}^{g}\\textrm{T}_c\\f$).\n",
      "        .   @param[out] t_cam2gripper Estimated translation part extracted from the homogeneous matrix that transforms a point\n",
      "        .   expressed in the camera frame to the gripper frame (\\f$_{}^{g}\\textrm{T}_c\\f$).\n",
      "        .   @param[in] method One of the implemented Hand-Eye calibration method, see cv::HandEyeCalibrationMethod\n",
      "        .   \n",
      "        .   The function performs the Hand-Eye calibration using various methods. One approach consists in estimating the\n",
      "        .   rotation then the translation (separable solutions) and the following methods are implemented:\n",
      "        .     - R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration \\cite Tsai89\n",
      "        .     - F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group \\cite Park94\n",
      "        .     - R. Horaud, F. Dornaika Hand-Eye Calibration \\cite Horaud95\n",
      "        .   \n",
      "        .   Another approach consists in estimating simultaneously the rotation and the translation (simultaneous solutions),\n",
      "        .   with the following implemented method:\n",
      "        .     - N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration \\cite Andreff99\n",
      "        .     - K. Daniilidis Hand-Eye Calibration Using Dual Quaternions \\cite Daniilidis98\n",
      "        .   \n",
      "        .   The following picture describes the Hand-Eye calibration problem where the transformation between a camera (\"eye\")\n",
      "        .   mounted on a robot gripper (\"hand\") has to be estimated.\n",
      "        .   \n",
      "        .   ![](pics/hand-eye_figure.png)\n",
      "        .   \n",
      "        .   The calibration procedure is the following:\n",
      "        .     - a static calibration pattern is used to estimate the transformation between the target frame\n",
      "        .     and the camera frame\n",
      "        .     - the robot gripper is moved in order to acquire several poses\n",
      "        .     - for each pose, the homogeneous transformation between the gripper frame and the robot base frame is recorded using for\n",
      "        .     instance the robot kinematics\n",
      "        .   \\f[\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_b\\\\\n",
      "        .       Y_b\\\\\n",
      "        .       Z_b\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .       =\n",
      "        .       \\begin{bmatrix}\n",
      "        .       _{}^{b}\\textrm{R}_g & _{}^{b}\\textrm{t}_g \\\\\n",
      "        .       0_{1 \\times 3} & 1\n",
      "        .       \\end{bmatrix}\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_g\\\\\n",
      "        .       Y_g\\\\\n",
      "        .       Z_g\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .     - for each pose, the homogeneous transformation between the calibration target frame and the camera frame is recorded using\n",
      "        .     for instance a pose estimation method (PnP) from 2D-3D point correspondences\n",
      "        .   \\f[\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_c\\\\\n",
      "        .       Y_c\\\\\n",
      "        .       Z_c\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .       =\n",
      "        .       \\begin{bmatrix}\n",
      "        .       _{}^{c}\\textrm{R}_t & _{}^{c}\\textrm{t}_t \\\\\n",
      "        .       0_{1 \\times 3} & 1\n",
      "        .       \\end{bmatrix}\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_t\\\\\n",
      "        .       Y_t\\\\\n",
      "        .       Z_t\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   The Hand-Eye calibration procedure returns the following homogeneous transformation\n",
      "        .   \\f[\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_g\\\\\n",
      "        .       Y_g\\\\\n",
      "        .       Z_g\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .       =\n",
      "        .       \\begin{bmatrix}\n",
      "        .       _{}^{g}\\textrm{R}_c & _{}^{g}\\textrm{t}_c \\\\\n",
      "        .       0_{1 \\times 3} & 1\n",
      "        .       \\end{bmatrix}\n",
      "        .       \\begin{bmatrix}\n",
      "        .       X_c\\\\\n",
      "        .       Y_c\\\\\n",
      "        .       Z_c\\\\\n",
      "        .       1\n",
      "        .       \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   This problem is also known as solving the \\f$\\mathbf{A}\\mathbf{X}=\\mathbf{X}\\mathbf{B}\\f$ equation:\n",
      "        .   \\f[\n",
      "        .       \\begin{align*}\n",
      "        .       ^{b}{\\textrm{T}_g}^{(1)} \\hspace{0.2em} ^{g}\\textrm{T}_c \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(1)} &=\n",
      "        .       \\hspace{0.1em} ^{b}{\\textrm{T}_g}^{(2)} \\hspace{0.2em} ^{g}\\textrm{T}_c \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(2)} \\\\\n",
      "        .   \n",
      "        .       (^{b}{\\textrm{T}_g}^{(2)})^{-1} \\hspace{0.2em} ^{b}{\\textrm{T}_g}^{(1)} \\hspace{0.2em} ^{g}\\textrm{T}_c &=\n",
      "        .       \\hspace{0.1em} ^{g}\\textrm{T}_c \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(2)} (^{c}{\\textrm{T}_t}^{(1)})^{-1} \\\\\n",
      "        .   \n",
      "        .       \\textrm{A}_i \\textrm{X} &= \\textrm{X} \\textrm{B}_i \\\\\n",
      "        .       \\end{align*}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   \\note\n",
      "        .   Additional information can be found on this [website](http://campar.in.tum.de/Chair/HandEyeCalibration).\n",
      "        .   \\note\n",
      "        .   A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye transformation.\n",
      "        .   So at least 3 different poses are required, but it is strongly recommended to use many more poses.\n",
      "    \n",
      "    calibrationMatrixValues(...)\n",
      "        calibrationMatrixValues(cameraMatrix, imageSize, apertureWidth, apertureHeight) -> fovx, fovy, focalLength, principalPoint, aspectRatio\n",
      "        .   @brief Computes useful camera characteristics from the camera matrix.\n",
      "        .   \n",
      "        .   @param cameraMatrix Input camera matrix that can be estimated by calibrateCamera or\n",
      "        .   stereoCalibrate .\n",
      "        .   @param imageSize Input image size in pixels.\n",
      "        .   @param apertureWidth Physical width in mm of the sensor.\n",
      "        .   @param apertureHeight Physical height in mm of the sensor.\n",
      "        .   @param fovx Output field of view in degrees along the horizontal sensor axis.\n",
      "        .   @param fovy Output field of view in degrees along the vertical sensor axis.\n",
      "        .   @param focalLength Focal length of the lens in mm.\n",
      "        .   @param principalPoint Principal point in mm.\n",
      "        .   @param aspectRatio \\f$f_y/f_x\\f$\n",
      "        .   \n",
      "        .   The function computes various useful camera characteristics from the previously estimated camera\n",
      "        .   matrix.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      Do keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for\n",
      "        .       the chessboard pitch (it can thus be any value).\n",
      "    \n",
      "    cartToPolar(...)\n",
      "        cartToPolar(x, y[, magnitude[, angle[, angleInDegrees]]]) -> magnitude, angle\n",
      "        .   @brief Calculates the magnitude and angle of 2D vectors.\n",
      "        .   \n",
      "        .   The function cv::cartToPolar calculates either the magnitude, angle, or both\n",
      "        .   for every 2D vector (x(I),y(I)):\n",
      "        .   \\f[\\begin{array}{l} \\texttt{magnitude} (I)= \\sqrt{\\texttt{x}(I)^2+\\texttt{y}(I)^2} , \\\\ \\texttt{angle} (I)= \\texttt{atan2} ( \\texttt{y} (I), \\texttt{x} (I))[ \\cdot180 / \\pi ] \\end{array}\\f]\n",
      "        .   \n",
      "        .   The angles are calculated with accuracy about 0.3 degrees. For the point\n",
      "        .   (0,0), the angle is set to 0.\n",
      "        .   @param x array of x-coordinates; this must be a single-precision or\n",
      "        .   double-precision floating-point array.\n",
      "        .   @param y array of y-coordinates, that must have the same size and same type as x.\n",
      "        .   @param magnitude output array of magnitudes of the same size and type as x.\n",
      "        .   @param angle output array of angles that has the same size and type as\n",
      "        .   x; the angles are measured in radians (from 0 to 2\\*Pi) or in degrees (0 to 360 degrees).\n",
      "        .   @param angleInDegrees a flag, indicating whether the angles are measured\n",
      "        .   in radians (which is by default), or in degrees.\n",
      "        .   @sa Sobel, Scharr\n",
      "    \n",
      "    checkChessboard(...)\n",
      "        checkChessboard(img, size) -> retval\n",
      "        .\n",
      "    \n",
      "    checkHardwareSupport(...)\n",
      "        checkHardwareSupport(feature) -> retval\n",
      "        .   @brief Returns true if the specified feature is supported by the host hardware.\n",
      "        .   \n",
      "        .   The function returns true if the host hardware supports the specified feature. When user calls\n",
      "        .   setUseOptimized(false), the subsequent calls to checkHardwareSupport() will return false until\n",
      "        .   setUseOptimized(true) is called. This way user can dynamically switch on and off the optimized code\n",
      "        .   in OpenCV.\n",
      "        .   @param feature The feature of interest, one of cv::CpuFeatures\n",
      "    \n",
      "    checkRange(...)\n",
      "        checkRange(a[, quiet[, minVal[, maxVal]]]) -> retval, pos\n",
      "        .   @brief Checks every element of an input array for invalid values.\n",
      "        .   \n",
      "        .   The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal \\>\n",
      "        .   -DBL_MAX and maxVal \\< DBL_MAX, the function also checks that each value is between minVal and\n",
      "        .   maxVal. In case of multi-channel arrays, each channel is processed independently. If some values\n",
      "        .   are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the\n",
      "        .   function either returns false (when quiet=true) or throws an exception.\n",
      "        .   @param a input array.\n",
      "        .   @param quiet a flag, indicating whether the functions quietly return false when the array elements\n",
      "        .   are out of range or they throw an exception.\n",
      "        .   @param pos optional output parameter, when not NULL, must be a pointer to array of src.dims\n",
      "        .   elements.\n",
      "        .   @param minVal inclusive lower boundary of valid values range.\n",
      "        .   @param maxVal exclusive upper boundary of valid values range.\n",
      "    \n",
      "    circle(...)\n",
      "        circle(img, center, radius, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @brief Draws a circle.\n",
      "        .   \n",
      "        .   The function cv::circle draws a simple or filled circle with a given center and radius.\n",
      "        .   @param img Image where the circle is drawn.\n",
      "        .   @param center Center of the circle.\n",
      "        .   @param radius Radius of the circle.\n",
      "        .   @param color Circle color.\n",
      "        .   @param thickness Thickness of the circle outline, if positive. Negative values, like #FILLED,\n",
      "        .   mean that a filled circle is to be drawn.\n",
      "        .   @param lineType Type of the circle boundary. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the coordinates of the center and in the radius value.\n",
      "    \n",
      "    clipLine(...)\n",
      "        clipLine(imgRect, pt1, pt2) -> retval, pt1, pt2\n",
      "        .   @overload\n",
      "        .   @param imgRect Image rectangle.\n",
      "        .   @param pt1 First line point.\n",
      "        .   @param pt2 Second line point.\n",
      "    \n",
      "    colorChange(...)\n",
      "        colorChange(src, mask[, dst[, red_mul[, green_mul[, blue_mul]]]]) -> dst\n",
      "        .   @brief Given an original color image, two differently colored versions of this image can be mixed\n",
      "        .   seamlessly.\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param mask Input 8-bit 1 or 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src .\n",
      "        .   @param red_mul R-channel multiply factor.\n",
      "        .   @param green_mul G-channel multiply factor.\n",
      "        .   @param blue_mul B-channel multiply factor.\n",
      "        .   \n",
      "        .   Multiplication factor is between .5 to 2.5.\n",
      "    \n",
      "    compare(...)\n",
      "        compare(src1, src2, cmpop[, dst]) -> dst\n",
      "        .   @brief Performs the per-element comparison of two arrays or an array and scalar value.\n",
      "        .   \n",
      "        .   The function compares:\n",
      "        .   *   Elements of two arrays when src1 and src2 have the same size:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1} (I)  \\,\\texttt{cmpop}\\, \\texttt{src2} (I)\\f]\n",
      "        .   *   Elements of src1 with a scalar src2 when src2 is constructed from\n",
      "        .       Scalar or has a single element:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1}(I) \\,\\texttt{cmpop}\\,  \\texttt{src2}\\f]\n",
      "        .   *   src1 with elements of src2 when src1 is constructed from Scalar or\n",
      "        .       has a single element:\n",
      "        .       \\f[\\texttt{dst} (I) =  \\texttt{src1}  \\,\\texttt{cmpop}\\, \\texttt{src2} (I)\\f]\n",
      "        .   When the comparison result is true, the corresponding element of output\n",
      "        .   array is set to 255. The comparison operations can be replaced with the\n",
      "        .   equivalent matrix expressions:\n",
      "        .   @code{.cpp}\n",
      "        .       Mat dst1 = src1 >= src2;\n",
      "        .       Mat dst2 = src1 < 8;\n",
      "        .       ...\n",
      "        .   @endcode\n",
      "        .   @param src1 first input array or a scalar; when it is an array, it must have a single channel.\n",
      "        .   @param src2 second input array or a scalar; when it is an array, it must have a single channel.\n",
      "        .   @param dst output array of type ref CV_8U that has the same size and the same number of channels as\n",
      "        .       the input arrays.\n",
      "        .   @param cmpop a flag, that specifies correspondence between the arrays (cv::CmpTypes)\n",
      "        .   @sa checkRange, min, max, threshold\n",
      "    \n",
      "    compareHist(...)\n",
      "        compareHist(H1, H2, method) -> retval\n",
      "        .   @brief Compares two histograms.\n",
      "        .   \n",
      "        .   The function cv::compareHist compares two dense or two sparse histograms using the specified method.\n",
      "        .   \n",
      "        .   The function returns \\f$d(H_1, H_2)\\f$ .\n",
      "        .   \n",
      "        .   While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable\n",
      "        .   for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling\n",
      "        .   problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms\n",
      "        .   or more general sparse configurations of weighted points, consider using the #EMD function.\n",
      "        .   \n",
      "        .   @param H1 First compared histogram.\n",
      "        .   @param H2 Second compared histogram of the same size as H1 .\n",
      "        .   @param method Comparison method, see #HistCompMethods\n",
      "    \n",
      "    completeSymm(...)\n",
      "        completeSymm(m[, lowerToUpper]) -> m\n",
      "        .   @brief Copies the lower or the upper half of a square matrix to its another half.\n",
      "        .   \n",
      "        .   The function cv::completeSymm copies the lower or the upper half of a square matrix to\n",
      "        .   its another half. The matrix diagonal remains unchanged:\n",
      "        .    - \\f$\\texttt{m}_{ij}=\\texttt{m}_{ji}\\f$ for \\f$i > j\\f$ if\n",
      "        .       lowerToUpper=false\n",
      "        .    - \\f$\\texttt{m}_{ij}=\\texttt{m}_{ji}\\f$ for \\f$i < j\\f$ if\n",
      "        .       lowerToUpper=true\n",
      "        .   \n",
      "        .   @param m input-output floating-point square matrix.\n",
      "        .   @param lowerToUpper operation flag; if true, the lower half is copied to\n",
      "        .   the upper half. Otherwise, the upper half is copied to the lower half.\n",
      "        .   @sa flip, transpose\n",
      "    \n",
      "    composeRT(...)\n",
      "        composeRT(rvec1, tvec1, rvec2, tvec2[, rvec3[, tvec3[, dr3dr1[, dr3dt1[, dr3dr2[, dr3dt2[, dt3dr1[, dt3dt1[, dt3dr2[, dt3dt2]]]]]]]]]]) -> rvec3, tvec3, dr3dr1, dr3dt1, dr3dr2, dr3dt2, dt3dr1, dt3dt1, dt3dr2, dt3dt2\n",
      "        .   @brief Combines two rotation-and-shift transformations.\n",
      "        .   \n",
      "        .   @param rvec1 First rotation vector.\n",
      "        .   @param tvec1 First translation vector.\n",
      "        .   @param rvec2 Second rotation vector.\n",
      "        .   @param tvec2 Second translation vector.\n",
      "        .   @param rvec3 Output rotation vector of the superposition.\n",
      "        .   @param tvec3 Output translation vector of the superposition.\n",
      "        .   @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1\n",
      "        .   @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1\n",
      "        .   @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2\n",
      "        .   @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2\n",
      "        .   @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1\n",
      "        .   @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1\n",
      "        .   @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2\n",
      "        .   @param dt3dt2 Optional output derivative of tvec3 with regard to tvec2\n",
      "        .   \n",
      "        .   The functions compute:\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} \\texttt{rvec3} =  \\mathrm{rodrigues} ^{-1} \\left ( \\mathrm{rodrigues} ( \\texttt{rvec2} )  \\cdot \\mathrm{rodrigues} ( \\texttt{rvec1} ) \\right )  \\\\ \\texttt{tvec3} =  \\mathrm{rodrigues} ( \\texttt{rvec2} )  \\cdot \\texttt{tvec1} +  \\texttt{tvec2} \\end{array} ,\\f]\n",
      "        .   \n",
      "        .   where \\f$\\mathrm{rodrigues}\\f$ denotes a rotation vector to a rotation matrix transformation, and\n",
      "        .   \\f$\\mathrm{rodrigues}^{-1}\\f$ denotes the inverse transformation. See Rodrigues for details.\n",
      "        .   \n",
      "        .   Also, the functions can compute the derivatives of the output vectors with regards to the input\n",
      "        .   vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in\n",
      "        .   your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a\n",
      "        .   function that contains a matrix multiplication.\n",
      "    \n",
      "    computeCorrespondEpilines(...)\n",
      "        computeCorrespondEpilines(points, whichImage, F[, lines]) -> lines\n",
      "        .   @brief For points in an image of a stereo pair, computes the corresponding epilines in the other image.\n",
      "        .   \n",
      "        .   @param points Input points. \\f$N \\times 1\\f$ or \\f$1 \\times N\\f$ matrix of type CV_32FC2 or\n",
      "        .   vector\\<Point2f\\> .\n",
      "        .   @param whichImage Index of the image (1 or 2) that contains the points .\n",
      "        .   @param F Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .\n",
      "        .   @param lines Output vector of the epipolar lines corresponding to the points in the other image.\n",
      "        .   Each line \\f$ax + by + c=0\\f$ is encoded by 3 numbers \\f$(a, b, c)\\f$ .\n",
      "        .   \n",
      "        .   For every point in one of the two images of a stereo pair, the function finds the equation of the\n",
      "        .   corresponding epipolar line in the other image.\n",
      "        .   \n",
      "        .   From the fundamental matrix definition (see findFundamentalMat ), line \\f$l^{(2)}_i\\f$ in the second\n",
      "        .   image for the point \\f$p^{(1)}_i\\f$ in the first image (when whichImage=1 ) is computed as:\n",
      "        .   \n",
      "        .   \\f[l^{(2)}_i = F p^{(1)}_i\\f]\n",
      "        .   \n",
      "        .   And vice versa, when whichImage=2, \\f$l^{(1)}_i\\f$ is computed from \\f$p^{(2)}_i\\f$ as:\n",
      "        .   \n",
      "        .   \\f[l^{(1)}_i = F^T p^{(2)}_i\\f]\n",
      "        .   \n",
      "        .   Line coefficients are defined up to a scale. They are normalized so that \\f$a_i^2+b_i^2=1\\f$ .\n",
      "    \n",
      "    computeECC(...)\n",
      "        computeECC(templateImage, inputImage[, inputMask]) -> retval\n",
      "        .   @brief Computes the Enhanced Correlation Coefficient value between two images @cite EP08 .\n",
      "        .   \n",
      "        .   @param templateImage single-channel template image; CV_8U or CV_32F array.\n",
      "        .   @param inputImage single-channel input image to be warped to provide an image similar to\n",
      "        .    templateImage, same type as templateImage.\n",
      "        .   @param inputMask An optional mask to indicate valid values of inputImage.\n",
      "        .   \n",
      "        .   @sa\n",
      "        .   findTransformECC\n",
      "    \n",
      "    connectedComponents(...)\n",
      "        connectedComponents(image[, labels[, connectivity[, ltype]]]) -> retval, labels\n",
      "        .   @overload\n",
      "        .   \n",
      "        .   @param image the 8-bit single-channel image to be labeled\n",
      "        .   @param labels destination labeled image\n",
      "        .   @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n",
      "        .   @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n",
      "    \n",
      "    connectedComponentsWithAlgorithm(...)\n",
      "        connectedComponentsWithAlgorithm(image, connectivity, ltype, ccltype[, labels]) -> retval, labels\n",
      "        .   @brief computes the connected components labeled image of boolean image\n",
      "        .   \n",
      "        .   image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n",
      "        .   represents the background label. ltype specifies the output label image type, an important\n",
      "        .   consideration based on the total number of labels or alternatively the total number of pixels in\n",
      "        .   the source image. ccltype specifies the connected components labeling algorithm to use, currently\n",
      "        .   Grana (BBDT) and Wu's (SAUF) algorithms are supported, see the #ConnectedComponentsAlgorithmsTypes\n",
      "        .   for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.\n",
      "        .   This function uses parallel version of both Grana and Wu's algorithms if at least one allowed\n",
      "        .   parallel framework is enabled and if the rows of the image are at least twice the number returned by #getNumberOfCPUs.\n",
      "        .   \n",
      "        .   @param image the 8-bit single-channel image to be labeled\n",
      "        .   @param labels destination labeled image\n",
      "        .   @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n",
      "        .   @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n",
      "        .   @param ccltype connected components algorithm type (see the #ConnectedComponentsAlgorithmsTypes).\n",
      "    \n",
      "    connectedComponentsWithStats(...)\n",
      "        connectedComponentsWithStats(image[, labels[, stats[, centroids[, connectivity[, ltype]]]]]) -> retval, labels, stats, centroids\n",
      "        .   @overload\n",
      "        .   @param image the 8-bit single-channel image to be labeled\n",
      "        .   @param labels destination labeled image\n",
      "        .   @param stats statistics output for each label, including the background label, see below for\n",
      "        .   available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n",
      "        .   #ConnectedComponentsTypes. The data type is CV_32S.\n",
      "        .   @param centroids centroid output for each label, including the background label. Centroids are\n",
      "        .   accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n",
      "        .   @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n",
      "        .   @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n",
      "    \n",
      "    connectedComponentsWithStatsWithAlgorithm(...)\n",
      "        connectedComponentsWithStatsWithAlgorithm(image, connectivity, ltype, ccltype[, labels[, stats[, centroids]]]) -> retval, labels, stats, centroids\n",
      "        .   @brief computes the connected components labeled image of boolean image and also produces a statistics output for each label\n",
      "        .   \n",
      "        .   image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0\n",
      "        .   represents the background label. ltype specifies the output label image type, an important\n",
      "        .   consideration based on the total number of labels or alternatively the total number of pixels in\n",
      "        .   the source image. ccltype specifies the connected components labeling algorithm to use, currently\n",
      "        .   Grana's (BBDT) and Wu's (SAUF) algorithms are supported, see the #ConnectedComponentsAlgorithmsTypes\n",
      "        .   for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not.\n",
      "        .   This function uses parallel version of both Grana and Wu's algorithms (statistics included) if at least one allowed\n",
      "        .   parallel framework is enabled and if the rows of the image are at least twice the number returned by #getNumberOfCPUs.\n",
      "        .   \n",
      "        .   @param image the 8-bit single-channel image to be labeled\n",
      "        .   @param labels destination labeled image\n",
      "        .   @param stats statistics output for each label, including the background label, see below for\n",
      "        .   available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of\n",
      "        .   #ConnectedComponentsTypes. The data type is CV_32S.\n",
      "        .   @param centroids centroid output for each label, including the background label. Centroids are\n",
      "        .   accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.\n",
      "        .   @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively\n",
      "        .   @param ltype output image label type. Currently CV_32S and CV_16U are supported.\n",
      "        .   @param ccltype connected components algorithm type (see #ConnectedComponentsAlgorithmsTypes).\n",
      "    \n",
      "    contourArea(...)\n",
      "        contourArea(contour[, oriented]) -> retval\n",
      "        .   @brief Calculates a contour area.\n",
      "        .   \n",
      "        .   The function computes a contour area. Similarly to moments , the area is computed using the Green\n",
      "        .   formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using\n",
      "        .   #drawContours or #fillPoly , can be different. Also, the function will most certainly give a wrong\n",
      "        .   results for contours with self-intersections.\n",
      "        .   \n",
      "        .   Example:\n",
      "        .   @code\n",
      "        .       vector<Point> contour;\n",
      "        .       contour.push_back(Point2f(0, 0));\n",
      "        .       contour.push_back(Point2f(10, 0));\n",
      "        .       contour.push_back(Point2f(10, 10));\n",
      "        .       contour.push_back(Point2f(5, 4));\n",
      "        .   \n",
      "        .       double area0 = contourArea(contour);\n",
      "        .       vector<Point> approx;\n",
      "        .       approxPolyDP(contour, approx, 5, true);\n",
      "        .       double area1 = contourArea(approx);\n",
      "        .   \n",
      "        .       cout << \"area0 =\" << area0 << endl <<\n",
      "        .               \"area1 =\" << area1 << endl <<\n",
      "        .               \"approx poly vertices\" << approx.size() << endl;\n",
      "        .   @endcode\n",
      "        .   @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.\n",
      "        .   @param oriented Oriented area flag. If it is true, the function returns a signed area value,\n",
      "        .   depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can\n",
      "        .   determine orientation of a contour by taking the sign of an area. By default, the parameter is\n",
      "        .   false, which means that the absolute value is returned.\n",
      "    \n",
      "    convertFp16(...)\n",
      "        convertFp16(src[, dst]) -> dst\n",
      "        .   @brief Converts an array to half precision floating number.\n",
      "        .   \n",
      "        .   This function converts FP32 (single precision floating point) from/to FP16 (half precision floating point). CV_16S format is used to represent FP16 data.\n",
      "        .   There are two use modes (src -> dst): CV_32F -> CV_16S and CV_16S -> CV_32F. The input array has to have type of CV_32F or\n",
      "        .   CV_16S to represent the bit depth. If the input array is neither of them, the function will raise an error.\n",
      "        .   The format of half precision floating point is defined in IEEE 754-2008.\n",
      "        .   \n",
      "        .   @param src input array.\n",
      "        .   @param dst output array.\n",
      "    \n",
      "    convertMaps(...)\n",
      "        convertMaps(map1, map2, dstmap1type[, dstmap1[, dstmap2[, nninterpolation]]]) -> dstmap1, dstmap2\n",
      "        .   @brief Converts image transformation maps from one representation to another.\n",
      "        .   \n",
      "        .   The function converts a pair of maps for remap from one representation to another. The following\n",
      "        .   options ( (map1.type(), map2.type()) \\f$\\rightarrow\\f$ (dstmap1.type(), dstmap2.type()) ) are\n",
      "        .   supported:\n",
      "        .   \n",
      "        .   - \\f$\\texttt{(CV_32FC1, CV_32FC1)} \\rightarrow \\texttt{(CV_16SC2, CV_16UC1)}\\f$. This is the\n",
      "        .   most frequently used conversion operation, in which the original floating-point maps (see remap )\n",
      "        .   are converted to a more compact and much faster fixed-point representation. The first output array\n",
      "        .   contains the rounded coordinates and the second array (created only when nninterpolation=false )\n",
      "        .   contains indices in the interpolation tables.\n",
      "        .   \n",
      "        .   - \\f$\\texttt{(CV_32FC2)} \\rightarrow \\texttt{(CV_16SC2, CV_16UC1)}\\f$. The same as above but\n",
      "        .   the original maps are stored in one 2-channel matrix.\n",
      "        .   \n",
      "        .   - Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same\n",
      "        .   as the originals.\n",
      "        .   \n",
      "        .   @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .\n",
      "        .   @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix),\n",
      "        .   respectively.\n",
      "        .   @param dstmap1 The first output map that has the type dstmap1type and the same size as src .\n",
      "        .   @param dstmap2 The second output map.\n",
      "        .   @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or\n",
      "        .   CV_32FC2 .\n",
      "        .   @param nninterpolation Flag indicating whether the fixed-point maps are used for the\n",
      "        .   nearest-neighbor or for a more complex interpolation.\n",
      "        .   \n",
      "        .   @sa  remap, undistort, initUndistortRectifyMap\n",
      "    \n",
      "    convertPointsFromHomogeneous(...)\n",
      "        convertPointsFromHomogeneous(src[, dst]) -> dst\n",
      "        .   @brief Converts points from homogeneous to Euclidean space.\n",
      "        .   \n",
      "        .   @param src Input vector of N-dimensional points.\n",
      "        .   @param dst Output vector of N-1-dimensional points.\n",
      "        .   \n",
      "        .   The function converts points homogeneous to Euclidean space using perspective projection. That is,\n",
      "        .   each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the\n",
      "        .   output point coordinates will be (0,0,0,...).\n",
      "    \n",
      "    convertPointsToHomogeneous(...)\n",
      "        convertPointsToHomogeneous(src[, dst]) -> dst\n",
      "        .   @brief Converts points from Euclidean to homogeneous space.\n",
      "        .   \n",
      "        .   @param src Input vector of N-dimensional points.\n",
      "        .   @param dst Output vector of N+1-dimensional points.\n",
      "        .   \n",
      "        .   The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of\n",
      "        .   point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).\n",
      "    \n",
      "    convertScaleAbs(...)\n",
      "        convertScaleAbs(src[, dst[, alpha[, beta]]]) -> dst\n",
      "        .   @brief Scales, calculates absolute values, and converts the result to 8-bit.\n",
      "        .   \n",
      "        .   On each element of the input array, the function convertScaleAbs\n",
      "        .   performs three operations sequentially: scaling, taking an absolute\n",
      "        .   value, conversion to an unsigned 8-bit type:\n",
      "        .   \\f[\\texttt{dst} (I)= \\texttt{saturate\\_cast<uchar>} (| \\texttt{src} (I)* \\texttt{alpha} +  \\texttt{beta} |)\\f]\n",
      "        .   In case of multi-channel arrays, the function processes each channel\n",
      "        .   independently. When the output is not 8-bit, the operation can be\n",
      "        .   emulated by calling the Mat::convertTo method (or by using matrix\n",
      "        .   expressions) and then by calculating an absolute value of the result.\n",
      "        .   For example:\n",
      "        .   @code{.cpp}\n",
      "        .       Mat_<float> A(30,30);\n",
      "        .       randu(A, Scalar(-100), Scalar(100));\n",
      "        .       Mat_<float> B = A*5 + 3;\n",
      "        .       B = abs(B);\n",
      "        .       // Mat_<float> B = abs(A*5+3) will also do the job,\n",
      "        .       // but it will allocate a temporary matrix\n",
      "        .   @endcode\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array.\n",
      "        .   @param alpha optional scale factor.\n",
      "        .   @param beta optional delta added to the scaled values.\n",
      "        .   @sa  Mat::convertTo, cv::abs(const Mat&)\n",
      "    \n",
      "    convexHull(...)\n",
      "        convexHull(points[, hull[, clockwise[, returnPoints]]]) -> hull\n",
      "        .   @brief Finds the convex hull of a point set.\n",
      "        .   \n",
      "        .   The function cv::convexHull finds the convex hull of a 2D point set using the Sklansky's algorithm @cite Sklansky82\n",
      "        .   that has *O(N logN)* complexity in the current implementation.\n",
      "        .   \n",
      "        .   @param points Input 2D point set, stored in std::vector or Mat.\n",
      "        .   @param hull Output convex hull. It is either an integer vector of indices or vector of points. In\n",
      "        .   the first case, the hull elements are 0-based indices of the convex hull points in the original\n",
      "        .   array (since the set of convex hull points is a subset of the original point set). In the second\n",
      "        .   case, hull elements are the convex hull points themselves.\n",
      "        .   @param clockwise Orientation flag. If it is true, the output convex hull is oriented clockwise.\n",
      "        .   Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing\n",
      "        .   to the right, and its Y axis pointing upwards.\n",
      "        .   @param returnPoints Operation flag. In case of a matrix, when the flag is true, the function\n",
      "        .   returns convex hull points. Otherwise, it returns indices of the convex hull points. When the\n",
      "        .   output array is std::vector, the flag is ignored, and the output depends on the type of the\n",
      "        .   vector: std::vector\\<int\\> implies returnPoints=false, std::vector\\<Point\\> implies\n",
      "        .   returnPoints=true.\n",
      "        .   \n",
      "        .   @note `points` and `hull` should be different arrays, inplace processing isn't supported.\n",
      "        .   \n",
      "        .   Check @ref tutorial_hull \"the corresponding tutorial\" for more details.\n",
      "        .   \n",
      "        .   useful links:\n",
      "        .   \n",
      "        .   https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/\n",
      "    \n",
      "    convexityDefects(...)\n",
      "        convexityDefects(contour, convexhull[, convexityDefects]) -> convexityDefects\n",
      "        .   @brief Finds the convexity defects of a contour.\n",
      "        .   \n",
      "        .   The figure below displays convexity defects of a hand contour:\n",
      "        .   \n",
      "        .   ![image](pics/defects.png)\n",
      "        .   \n",
      "        .   @param contour Input contour.\n",
      "        .   @param convexhull Convex hull obtained using convexHull that should contain indices of the contour\n",
      "        .   points that make the hull.\n",
      "        .   @param convexityDefects The output vector of convexity defects. In C++ and the new Python/Java\n",
      "        .   interface each convexity defect is represented as 4-element integer vector (a.k.a. #Vec4i):\n",
      "        .   (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices\n",
      "        .   in the original contour of the convexity defect beginning, end and the farthest point, and\n",
      "        .   fixpt_depth is fixed-point approximation (with 8 fractional bits) of the distance between the\n",
      "        .   farthest contour point and the hull. That is, to get the floating-point value of the depth will be\n",
      "        .   fixpt_depth/256.0.\n",
      "    \n",
      "    copyMakeBorder(...)\n",
      "        copyMakeBorder(src, top, bottom, left, right, borderType[, dst[, value]]) -> dst\n",
      "        .   @brief Forms a border around an image.\n",
      "        .   \n",
      "        .   The function copies the source image into the middle of the destination image. The areas to the\n",
      "        .   left, to the right, above and below the copied source image will be filled with extrapolated\n",
      "        .   pixels. This is not what filtering functions based on it do (they extrapolate pixels on-fly), but\n",
      "        .   what other more complex functions, including your own, may do to simplify image boundary handling.\n",
      "        .   \n",
      "        .   The function supports the mode when src is already in the middle of dst . In this case, the\n",
      "        .   function does not copy src itself but simply constructs the border, for example:\n",
      "        .   \n",
      "        .   @code{.cpp}\n",
      "        .       // let border be the same in all directions\n",
      "        .       int border=2;\n",
      "        .       // constructs a larger image to fit both the image and the border\n",
      "        .       Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());\n",
      "        .       // select the middle part of it w/o copying data\n",
      "        .       Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));\n",
      "        .       // convert image from RGB to grayscale\n",
      "        .       cvtColor(rgb, gray, COLOR_RGB2GRAY);\n",
      "        .       // form a border in-place\n",
      "        .       copyMakeBorder(gray, gray_buf, border, border,\n",
      "        .                      border, border, BORDER_REPLICATE);\n",
      "        .       // now do some custom filtering ...\n",
      "        .       ...\n",
      "        .   @endcode\n",
      "        .   @note When the source image is a part (ROI) of a bigger image, the function will try to use the\n",
      "        .   pixels outside of the ROI to form a border. To disable this feature and always do extrapolation, as\n",
      "        .   if src was not a ROI, use borderType | #BORDER_ISOLATED.\n",
      "        .   \n",
      "        .   @param src Source image.\n",
      "        .   @param dst Destination image of the same type as src and the size Size(src.cols+left+right,\n",
      "        .   src.rows+top+bottom) .\n",
      "        .   @param top the top pixels\n",
      "        .   @param bottom the bottom pixels\n",
      "        .   @param left the left pixels\n",
      "        .   @param right Parameter specifying how many pixels in each direction from the source image rectangle\n",
      "        .   to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs\n",
      "        .   to be built.\n",
      "        .   @param borderType Border type. See borderInterpolate for details.\n",
      "        .   @param value Border value if borderType==BORDER_CONSTANT .\n",
      "        .   \n",
      "        .   @sa  borderInterpolate\n",
      "    \n",
      "    copyTo(...)\n",
      "        copyTo(src, mask[, dst]) -> dst\n",
      "        .   @brief  This is an overloaded member function, provided for convenience (python)\n",
      "        .   Copies the matrix to another one.\n",
      "        .   When the operation mask is specified, if the Mat::create call shown above reallocates the matrix, the newly allocated matrix is initialized with all zeros before copying the data.\n",
      "        .   @param src source matrix.\n",
      "        .   @param dst Destination matrix. If it does not have a proper size or type before the operation, it is\n",
      "        .   reallocated.\n",
      "        .   @param mask Operation mask of the same size as \\*this. Its non-zero elements indicate which matrix\n",
      "        .   elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.\n",
      "    \n",
      "    cornerEigenValsAndVecs(...)\n",
      "        cornerEigenValsAndVecs(src, blockSize, ksize[, dst[, borderType]]) -> dst\n",
      "        .   @brief Calculates eigenvalues and eigenvectors of image blocks for corner detection.\n",
      "        .   \n",
      "        .   For every pixel \\f$p\\f$ , the function cornerEigenValsAndVecs considers a blockSize \\f$\\times\\f$ blockSize\n",
      "        .   neighborhood \\f$S(p)\\f$ . It calculates the covariation matrix of derivatives over the neighborhood as:\n",
      "        .   \n",
      "        .   \\f[M =  \\begin{bmatrix} \\sum _{S(p)}(dI/dx)^2 &  \\sum _{S(p)}dI/dx dI/dy  \\\\ \\sum _{S(p)}dI/dx dI/dy &  \\sum _{S(p)}(dI/dy)^2 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   where the derivatives are computed using the Sobel operator.\n",
      "        .   \n",
      "        .   After that, it finds eigenvectors and eigenvalues of \\f$M\\f$ and stores them in the destination image as\n",
      "        .   \\f$(\\lambda_1, \\lambda_2, x_1, y_1, x_2, y_2)\\f$ where\n",
      "        .   \n",
      "        .   -   \\f$\\lambda_1, \\lambda_2\\f$ are the non-sorted eigenvalues of \\f$M\\f$\n",
      "        .   -   \\f$x_1, y_1\\f$ are the eigenvectors corresponding to \\f$\\lambda_1\\f$\n",
      "        .   -   \\f$x_2, y_2\\f$ are the eigenvectors corresponding to \\f$\\lambda_2\\f$\n",
      "        .   \n",
      "        .   The output of the function can be used for robust edge or corner detection.\n",
      "        .   \n",
      "        .   @param src Input single-channel 8-bit or floating-point image.\n",
      "        .   @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .\n",
      "        .   @param blockSize Neighborhood size (see details below).\n",
      "        .   @param ksize Aperture parameter for the Sobel operator.\n",
      "        .   @param borderType Pixel extrapolation method. See #BorderTypes.\n",
      "        .   \n",
      "        .   @sa  cornerMinEigenVal, cornerHarris, preCornerDetect\n",
      "    \n",
      "    cornerHarris(...)\n",
      "        cornerHarris(src, blockSize, ksize, k[, dst[, borderType]]) -> dst\n",
      "        .   @brief Harris corner detector.\n",
      "        .   \n",
      "        .   The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and\n",
      "        .   cornerEigenValsAndVecs , for each pixel \\f$(x, y)\\f$ it calculates a \\f$2\\times2\\f$ gradient covariance\n",
      "        .   matrix \\f$M^{(x,y)}\\f$ over a \\f$\\texttt{blockSize} \\times \\texttt{blockSize}\\f$ neighborhood. Then, it\n",
      "        .   computes the following characteristic:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\mathrm{det} M^{(x,y)} - k  \\cdot \\left ( \\mathrm{tr} M^{(x,y)} \\right )^2\\f]\n",
      "        .   \n",
      "        .   Corners in the image can be found as the local maxima of this response map.\n",
      "        .   \n",
      "        .   @param src Input single-channel 8-bit or floating-point image.\n",
      "        .   @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same\n",
      "        .   size as src .\n",
      "        .   @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).\n",
      "        .   @param ksize Aperture parameter for the Sobel operator.\n",
      "        .   @param k Harris detector free parameter. See the formula above.\n",
      "        .   @param borderType Pixel extrapolation method. See #BorderTypes.\n",
      "    \n",
      "    cornerMinEigenVal(...)\n",
      "        cornerMinEigenVal(src, blockSize[, dst[, ksize[, borderType]]]) -> dst\n",
      "        .   @brief Calculates the minimal eigenvalue of gradient matrices for corner detection.\n",
      "        .   \n",
      "        .   The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal\n",
      "        .   eigenvalue of the covariance matrix of derivatives, that is, \\f$\\min(\\lambda_1, \\lambda_2)\\f$ in terms\n",
      "        .   of the formulae in the cornerEigenValsAndVecs description.\n",
      "        .   \n",
      "        .   @param src Input single-channel 8-bit or floating-point image.\n",
      "        .   @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as\n",
      "        .   src .\n",
      "        .   @param blockSize Neighborhood size (see the details on #cornerEigenValsAndVecs ).\n",
      "        .   @param ksize Aperture parameter for the Sobel operator.\n",
      "        .   @param borderType Pixel extrapolation method. See #BorderTypes.\n",
      "    \n",
      "    cornerSubPix(...)\n",
      "        cornerSubPix(image, corners, winSize, zeroZone, criteria) -> corners\n",
      "        .   @brief Refines the corner locations.\n",
      "        .   \n",
      "        .   The function iterates to find the sub-pixel accurate location of corners or radial saddle points, as\n",
      "        .   shown on the figure below.\n",
      "        .   \n",
      "        .   ![image](pics/cornersubpix.png)\n",
      "        .   \n",
      "        .   Sub-pixel accurate corner locator is based on the observation that every vector from the center \\f$q\\f$\n",
      "        .   to a point \\f$p\\f$ located within a neighborhood of \\f$q\\f$ is orthogonal to the image gradient at \\f$p\\f$\n",
      "        .   subject to image and measurement noise. Consider the expression:\n",
      "        .   \n",
      "        .   \\f[\\epsilon _i = {DI_{p_i}}^T  \\cdot (q - p_i)\\f]\n",
      "        .   \n",
      "        .   where \\f${DI_{p_i}}\\f$ is an image gradient at one of the points \\f$p_i\\f$ in a neighborhood of \\f$q\\f$ . The\n",
      "        .   value of \\f$q\\f$ is to be found so that \\f$\\epsilon_i\\f$ is minimized. A system of equations may be set up\n",
      "        .   with \\f$\\epsilon_i\\f$ set to zero:\n",
      "        .   \n",
      "        .   \\f[\\sum _i(DI_{p_i}  \\cdot {DI_{p_i}}^T) \\cdot q -  \\sum _i(DI_{p_i}  \\cdot {DI_{p_i}}^T  \\cdot p_i)\\f]\n",
      "        .   \n",
      "        .   where the gradients are summed within a neighborhood (\"search window\") of \\f$q\\f$ . Calling the first\n",
      "        .   gradient term \\f$G\\f$ and the second gradient term \\f$b\\f$ gives:\n",
      "        .   \n",
      "        .   \\f[q = G^{-1}  \\cdot b\\f]\n",
      "        .   \n",
      "        .   The algorithm sets the center of the neighborhood window at this new center \\f$q\\f$ and then iterates\n",
      "        .   until the center stays within a set threshold.\n",
      "        .   \n",
      "        .   @param image Input single-channel, 8-bit or float image.\n",
      "        .   @param corners Initial coordinates of the input corners and refined coordinates provided for\n",
      "        .   output.\n",
      "        .   @param winSize Half of the side length of the search window. For example, if winSize=Size(5,5) ,\n",
      "        .   then a \\f$(5*2+1) \\times (5*2+1) = 11 \\times 11\\f$ search window is used.\n",
      "        .   @param zeroZone Half of the size of the dead region in the middle of the search zone over which\n",
      "        .   the summation in the formula below is not done. It is used sometimes to avoid possible\n",
      "        .   singularities of the autocorrelation matrix. The value of (-1,-1) indicates that there is no such\n",
      "        .   a size.\n",
      "        .   @param criteria Criteria for termination of the iterative process of corner refinement. That is,\n",
      "        .   the process of corner position refinement stops either after criteria.maxCount iterations or when\n",
      "        .   the corner position moves by less than criteria.epsilon on some iteration.\n",
      "    \n",
      "    correctMatches(...)\n",
      "        correctMatches(F, points1, points2[, newPoints1[, newPoints2]]) -> newPoints1, newPoints2\n",
      "        .   @brief Refines coordinates of corresponding points.\n",
      "        .   \n",
      "        .   @param F 3x3 fundamental matrix.\n",
      "        .   @param points1 1xN array containing the first set of points.\n",
      "        .   @param points2 1xN array containing the second set of points.\n",
      "        .   @param newPoints1 The optimized points1.\n",
      "        .   @param newPoints2 The optimized points2.\n",
      "        .   \n",
      "        .   The function implements the Optimal Triangulation Method (see Multiple View Geometry for details).\n",
      "        .   For each given point correspondence points1[i] \\<-\\> points2[i], and a fundamental matrix F, it\n",
      "        .   computes the corrected correspondences newPoints1[i] \\<-\\> newPoints2[i] that minimize the geometric\n",
      "        .   error \\f$d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2\\f$ (where \\f$d(a,b)\\f$ is the\n",
      "        .   geometric distance between points \\f$a\\f$ and \\f$b\\f$ ) subject to the epipolar constraint\n",
      "        .   \\f$newPoints2^T * F * newPoints1 = 0\\f$ .\n",
      "    \n",
      "    countNonZero(...)\n",
      "        countNonZero(src) -> retval\n",
      "        .   @brief Counts non-zero array elements.\n",
      "        .   \n",
      "        .   The function returns the number of non-zero elements in src :\n",
      "        .   \\f[\\sum _{I: \\; \\texttt{src} (I) \\ne0 } 1\\f]\n",
      "        .   @param src single-channel array.\n",
      "        .   @sa  mean, meanStdDev, norm, minMaxLoc, calcCovarMatrix\n",
      "    \n",
      "    createAlignMTB(...)\n",
      "        createAlignMTB([, max_bits[, exclude_range[, cut]]]) -> retval\n",
      "        .   @brief Creates AlignMTB object\n",
      "        .   \n",
      "        .   @param max_bits logarithm to the base 2 of maximal shift in each dimension. Values of 5 and 6 are\n",
      "        .   usually good enough (31 and 63 pixels shift respectively).\n",
      "        .   @param exclude_range range for exclusion bitmap that is constructed to suppress noise around the\n",
      "        .   median value.\n",
      "        .   @param cut if true cuts images, otherwise fills the new regions with zeros.\n",
      "    \n",
      "    createBackgroundSubtractorKNN(...)\n",
      "        createBackgroundSubtractorKNN([, history[, dist2Threshold[, detectShadows]]]) -> retval\n",
      "        .   @brief Creates KNN Background Subtractor\n",
      "        .   \n",
      "        .   @param history Length of the history.\n",
      "        .   @param dist2Threshold Threshold on the squared distance between the pixel and the sample to decide\n",
      "        .   whether a pixel is close to that sample. This parameter does not affect the background update.\n",
      "        .   @param detectShadows If true, the algorithm will detect shadows and mark them. It decreases the\n",
      "        .   speed a bit, so if you do not need this feature, set the parameter to false.\n",
      "    \n",
      "    createBackgroundSubtractorMOG2(...)\n",
      "        createBackgroundSubtractorMOG2([, history[, varThreshold[, detectShadows]]]) -> retval\n",
      "        .   @brief Creates MOG2 Background Subtractor\n",
      "        .   \n",
      "        .   @param history Length of the history.\n",
      "        .   @param varThreshold Threshold on the squared Mahalanobis distance between the pixel and the model\n",
      "        .   to decide whether a pixel is well described by the background model. This parameter does not\n",
      "        .   affect the background update.\n",
      "        .   @param detectShadows If true, the algorithm will detect shadows and mark them. It decreases the\n",
      "        .   speed a bit, so if you do not need this feature, set the parameter to false.\n",
      "    \n",
      "    createButton(...)\n",
      "        createButton(buttonName, onChange [, userData, buttonType, initialButtonState]) -> None\n",
      "    \n",
      "    createCLAHE(...)\n",
      "        createCLAHE([, clipLimit[, tileGridSize]]) -> retval\n",
      "        .   @brief Creates a smart pointer to a cv::CLAHE class and initializes it.\n",
      "        .   \n",
      "        .   @param clipLimit Threshold for contrast limiting.\n",
      "        .   @param tileGridSize Size of grid for histogram equalization. Input image will be divided into\n",
      "        .   equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.\n",
      "    \n",
      "    createCalibrateDebevec(...)\n",
      "        createCalibrateDebevec([, samples[, lambda[, random]]]) -> retval\n",
      "        .   @brief Creates CalibrateDebevec object\n",
      "        .   \n",
      "        .   @param samples number of pixel locations to use\n",
      "        .   @param lambda smoothness term weight. Greater values produce smoother results, but can alter the\n",
      "        .   response.\n",
      "        .   @param random if true sample pixel locations are chosen at random, otherwise they form a\n",
      "        .   rectangular grid.\n",
      "    \n",
      "    createCalibrateRobertson(...)\n",
      "        createCalibrateRobertson([, max_iter[, threshold]]) -> retval\n",
      "        .   @brief Creates CalibrateRobertson object\n",
      "        .   \n",
      "        .   @param max_iter maximal number of Gauss-Seidel solver iterations.\n",
      "        .   @param threshold target difference between results of two successive steps of the minimization.\n",
      "    \n",
      "    createGeneralizedHoughBallard(...)\n",
      "        createGeneralizedHoughBallard() -> retval\n",
      "        .   @brief Creates a smart pointer to a cv::GeneralizedHoughBallard class and initializes it.\n",
      "    \n",
      "    createGeneralizedHoughGuil(...)\n",
      "        createGeneralizedHoughGuil() -> retval\n",
      "        .   @brief Creates a smart pointer to a cv::GeneralizedHoughGuil class and initializes it.\n",
      "    \n",
      "    createHanningWindow(...)\n",
      "        createHanningWindow(winSize, type[, dst]) -> dst\n",
      "        .   @brief This function computes a Hanning window coefficients in two dimensions.\n",
      "        .   \n",
      "        .   See (http://en.wikipedia.org/wiki/Hann_function) and (http://en.wikipedia.org/wiki/Window_function)\n",
      "        .   for more information.\n",
      "        .   \n",
      "        .   An example is shown below:\n",
      "        .   @code\n",
      "        .       // create hanning window of size 100x100 and type CV_32F\n",
      "        .       Mat hann;\n",
      "        .       createHanningWindow(hann, Size(100, 100), CV_32F);\n",
      "        .   @endcode\n",
      "        .   @param dst Destination array to place Hann coefficients in\n",
      "        .   @param winSize The window size specifications (both width and height must be > 1)\n",
      "        .   @param type Created array type\n",
      "    \n",
      "    createLineSegmentDetector(...)\n",
      "        createLineSegmentDetector([, _refine[, _scale[, _sigma_scale[, _quant[, _ang_th[, _log_eps[, _density_th[, _n_bins]]]]]]]]) -> retval\n",
      "        .   @brief Creates a smart pointer to a LineSegmentDetector object and initializes it.\n",
      "        .   \n",
      "        .   The LineSegmentDetector algorithm is defined using the standard values. Only advanced users may want\n",
      "        .   to edit those, as to tailor it for their own application.\n",
      "        .   \n",
      "        .   @param _refine The way found lines will be refined, see #LineSegmentDetectorModes\n",
      "        .   @param _scale The scale of the image that will be used to find the lines. Range (0..1].\n",
      "        .   @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.\n",
      "        .   @param _quant Bound to the quantization error on the gradient norm.\n",
      "        .   @param _ang_th Gradient angle tolerance in degrees.\n",
      "        .   @param _log_eps Detection threshold: -log10(NFA) \\> log_eps. Used only when advance refinement\n",
      "        .   is chosen.\n",
      "        .   @param _density_th Minimal density of aligned region points in the enclosing rectangle.\n",
      "        .   @param _n_bins Number of bins in pseudo-ordering of gradient modulus.\n",
      "        .   \n",
      "        .   @note Implementation has been removed due original code license conflict\n",
      "    \n",
      "    createMergeDebevec(...)\n",
      "        createMergeDebevec() -> retval\n",
      "        .   @brief Creates MergeDebevec object\n",
      "    \n",
      "    createMergeMertens(...)\n",
      "        createMergeMertens([, contrast_weight[, saturation_weight[, exposure_weight]]]) -> retval\n",
      "        .   @brief Creates MergeMertens object\n",
      "        .   \n",
      "        .   @param contrast_weight contrast measure weight. See MergeMertens.\n",
      "        .   @param saturation_weight saturation measure weight\n",
      "        .   @param exposure_weight well-exposedness measure weight\n",
      "    \n",
      "    createMergeRobertson(...)\n",
      "        createMergeRobertson() -> retval\n",
      "        .   @brief Creates MergeRobertson object\n",
      "    \n",
      "    createTonemap(...)\n",
      "        createTonemap([, gamma]) -> retval\n",
      "        .   @brief Creates simple linear mapper with gamma correction\n",
      "        .   \n",
      "        .   @param gamma positive value for gamma correction. Gamma value of 1.0 implies no correction, gamma\n",
      "        .   equal to 2.2f is suitable for most displays.\n",
      "        .   Generally gamma \\> 1 brightens the image and gamma \\< 1 darkens it.\n",
      "    \n",
      "    createTonemapDrago(...)\n",
      "        createTonemapDrago([, gamma[, saturation[, bias]]]) -> retval\n",
      "        .   @brief Creates TonemapDrago object\n",
      "        .   \n",
      "        .   @param gamma gamma value for gamma correction. See createTonemap\n",
      "        .   @param saturation positive saturation enhancement value. 1.0 preserves saturation, values greater\n",
      "        .   than 1 increase saturation and values less than 1 decrease it.\n",
      "        .   @param bias value for bias function in [0, 1] range. Values from 0.7 to 0.9 usually give best\n",
      "        .   results, default value is 0.85.\n",
      "    \n",
      "    createTonemapMantiuk(...)\n",
      "        createTonemapMantiuk([, gamma[, scale[, saturation]]]) -> retval\n",
      "        .   @brief Creates TonemapMantiuk object\n",
      "        .   \n",
      "        .   @param gamma gamma value for gamma correction. See createTonemap\n",
      "        .   @param scale contrast scale factor. HVS response is multiplied by this parameter, thus compressing\n",
      "        .   dynamic range. Values from 0.6 to 0.9 produce best results.\n",
      "        .   @param saturation saturation enhancement value. See createTonemapDrago\n",
      "    \n",
      "    createTonemapReinhard(...)\n",
      "        createTonemapReinhard([, gamma[, intensity[, light_adapt[, color_adapt]]]]) -> retval\n",
      "        .   @brief Creates TonemapReinhard object\n",
      "        .   \n",
      "        .   @param gamma gamma value for gamma correction. See createTonemap\n",
      "        .   @param intensity result intensity in [-8, 8] range. Greater intensity produces brighter results.\n",
      "        .   @param light_adapt light adaptation in [0, 1] range. If 1 adaptation is based only on pixel\n",
      "        .   value, if 0 it's global, otherwise it's a weighted mean of this two cases.\n",
      "        .   @param color_adapt chromatic adaptation in [0, 1] range. If 1 channels are treated independently,\n",
      "        .   if 0 adaptation level is the same for each channel.\n",
      "    \n",
      "    createTrackbar(...)\n",
      "        createTrackbar(trackbarName, windowName, value, count, onChange) -> None\n",
      "    \n",
      "    cubeRoot(...)\n",
      "        cubeRoot(val) -> retval\n",
      "        .   @brief Computes the cube root of an argument.\n",
      "        .   \n",
      "        .    The function cubeRoot computes \\f$\\sqrt[3]{\\texttt{val}}\\f$. Negative arguments are handled correctly.\n",
      "        .    NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for\n",
      "        .    single-precision data.\n",
      "        .    @param val A function argument.\n",
      "    \n",
      "    cvtColor(...)\n",
      "        cvtColor(src, code[, dst[, dstCn]]) -> dst\n",
      "        .   @brief Converts an image from one color space to another.\n",
      "        .   \n",
      "        .   The function converts an input image from one color space to another. In case of a transformation\n",
      "        .   to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note\n",
      "        .   that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the\n",
      "        .   bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue\n",
      "        .   component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and\n",
      "        .   sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.\n",
      "        .   \n",
      "        .   The conventional ranges for R, G, and B channel values are:\n",
      "        .   -   0 to 255 for CV_8U images\n",
      "        .   -   0 to 65535 for CV_16U images\n",
      "        .   -   0 to 1 for CV_32F images\n",
      "        .   \n",
      "        .   In case of linear transformations, the range does not matter. But in case of a non-linear\n",
      "        .   transformation, an input RGB image should be normalized to the proper value range to get the correct\n",
      "        .   results, for example, for RGB \\f$\\rightarrow\\f$ L\\*u\\*v\\* transformation. For example, if you have a\n",
      "        .   32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will\n",
      "        .   have the 0..255 value range instead of 0..1 assumed by the function. So, before calling #cvtColor ,\n",
      "        .   you need first to scale the image down:\n",
      "        .   @code\n",
      "        .       img *= 1./255;\n",
      "        .       cvtColor(img, img, COLOR_BGR2Luv);\n",
      "        .   @endcode\n",
      "        .   If you use #cvtColor with 8-bit images, the conversion will have some information lost. For many\n",
      "        .   applications, this will not be noticeable but it is recommended to use 32-bit images in applications\n",
      "        .   that need the full range of colors or that convert an image before an operation and then convert\n",
      "        .   back.\n",
      "        .   \n",
      "        .   If conversion adds the alpha channel, its value will set to the maximum of corresponding channel\n",
      "        .   range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.\n",
      "        .   \n",
      "        .   @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision\n",
      "        .   floating-point.\n",
      "        .   @param dst output image of the same size and depth as src.\n",
      "        .   @param code color space conversion code (see #ColorConversionCodes).\n",
      "        .   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n",
      "        .   channels is derived automatically from src and code.\n",
      "        .   \n",
      "        .   @see @ref imgproc_color_conversions\n",
      "    \n",
      "    cvtColorTwoPlane(...)\n",
      "        cvtColorTwoPlane(src1, src2, code[, dst]) -> dst\n",
      "        .   @brief Converts an image from one color space to another where the source image is\n",
      "        .   stored in two planes.\n",
      "        .   \n",
      "        .   This function only supports YUV420 to RGB conversion as of now.\n",
      "        .   \n",
      "        .   @param src1: 8-bit image (#CV_8U) of the Y plane.\n",
      "        .   @param src2: image containing interleaved U/V plane.\n",
      "        .   @param dst: output image.\n",
      "        .   @param code: Specifies the type of conversion. It can take any of the following values:\n",
      "        .   - #COLOR_YUV2BGR_NV12\n",
      "        .   - #COLOR_YUV2RGB_NV12\n",
      "        .   - #COLOR_YUV2BGRA_NV12\n",
      "        .   - #COLOR_YUV2RGBA_NV12\n",
      "        .   - #COLOR_YUV2BGR_NV21\n",
      "        .   - #COLOR_YUV2RGB_NV21\n",
      "        .   - #COLOR_YUV2BGRA_NV21\n",
      "        .   - #COLOR_YUV2RGBA_NV21\n",
      "    \n",
      "    dct(...)\n",
      "        dct(src[, dst[, flags]]) -> dst\n",
      "        .   @brief Performs a forward or inverse discrete Cosine transform of 1D or 2D array.\n",
      "        .   \n",
      "        .   The function cv::dct performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D\n",
      "        .   floating-point array:\n",
      "        .   -   Forward Cosine transform of a 1D vector of N elements:\n",
      "        .       \\f[Y = C^{(N)}  \\cdot X\\f]\n",
      "        .       where\n",
      "        .       \\f[C^{(N)}_{jk}= \\sqrt{\\alpha_j/N} \\cos \\left ( \\frac{\\pi(2k+1)j}{2N} \\right )\\f]\n",
      "        .       and\n",
      "        .       \\f$\\alpha_0=1\\f$, \\f$\\alpha_j=2\\f$ for *j \\> 0*.\n",
      "        .   -   Inverse Cosine transform of a 1D vector of N elements:\n",
      "        .       \\f[X =  \\left (C^{(N)} \\right )^{-1}  \\cdot Y =  \\left (C^{(N)} \\right )^T  \\cdot Y\\f]\n",
      "        .       (since \\f$C^{(N)}\\f$ is an orthogonal matrix, \\f$C^{(N)} \\cdot \\left(C^{(N)}\\right)^T = I\\f$ )\n",
      "        .   -   Forward 2D Cosine transform of M x N matrix:\n",
      "        .       \\f[Y = C^{(N)}  \\cdot X  \\cdot \\left (C^{(N)} \\right )^T\\f]\n",
      "        .   -   Inverse 2D Cosine transform of M x N matrix:\n",
      "        .       \\f[X =  \\left (C^{(N)} \\right )^T  \\cdot X  \\cdot C^{(N)}\\f]\n",
      "        .   \n",
      "        .   The function chooses the mode of operation by looking at the flags and size of the input array:\n",
      "        .   -   If (flags & #DCT_INVERSE) == 0 , the function does a forward 1D or 2D transform. Otherwise, it\n",
      "        .       is an inverse 1D or 2D transform.\n",
      "        .   -   If (flags & #DCT_ROWS) != 0 , the function performs a 1D transform of each row.\n",
      "        .   -   If the array is a single column or a single row, the function performs a 1D transform.\n",
      "        .   -   If none of the above is true, the function performs a 2D transform.\n",
      "        .   \n",
      "        .   @note Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you\n",
      "        .   can pad the array when necessary.\n",
      "        .   Also, the function performance depends very much, and not monotonically, on the array size (see\n",
      "        .   getOptimalDFTSize ). In the current implementation DCT of a vector of size N is calculated via DFT\n",
      "        .   of a vector of size N/2 . Thus, the optimal DCT size N1 \\>= N can be calculated as:\n",
      "        .   @code\n",
      "        .       size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }\n",
      "        .       N1 = getOptimalDCTSize(N);\n",
      "        .   @endcode\n",
      "        .   @param src input floating-point array.\n",
      "        .   @param dst output array of the same size and type as src .\n",
      "        .   @param flags transformation flags as a combination of cv::DftFlags (DCT_*)\n",
      "        .   @sa dft , getOptimalDFTSize , idct\n",
      "    \n",
      "    decolor(...)\n",
      "        decolor(src[, grayscale[, color_boost]]) -> grayscale, color_boost\n",
      "        .   @brief Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized\n",
      "        .   black-and-white photograph rendering, and in many single channel image processing applications\n",
      "        .   @cite CL12 .\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param grayscale Output 8-bit 1-channel image.\n",
      "        .   @param color_boost Output 8-bit 3-channel image.\n",
      "        .   \n",
      "        .   This function is to be applied on color images.\n",
      "    \n",
      "    decomposeEssentialMat(...)\n",
      "        decomposeEssentialMat(E[, R1[, R2[, t]]]) -> R1, R2, t\n",
      "        .   @brief Decompose an essential matrix to possible rotations and translation.\n",
      "        .   \n",
      "        .   @param E The input essential matrix.\n",
      "        .   @param R1 One possible rotation matrix.\n",
      "        .   @param R2 Another possible rotation matrix.\n",
      "        .   @param t One possible translation.\n",
      "        .   \n",
      "        .   This function decompose an essential matrix E using svd decomposition @cite HartleyZ00 . Generally 4\n",
      "        .   possible poses exists for a given E. They are \\f$[R_1, t]\\f$, \\f$[R_1, -t]\\f$, \\f$[R_2, t]\\f$, \\f$[R_2, -t]\\f$. By\n",
      "        .   decomposing E, you can only get the direction of the translation, so the function returns unit t.\n",
      "    \n",
      "    decomposeHomographyMat(...)\n",
      "        decomposeHomographyMat(H, K[, rotations[, translations[, normals]]]) -> retval, rotations, translations, normals\n",
      "        .   @brief Decompose a homography matrix to rotation(s), translation(s) and plane normal(s).\n",
      "        .   \n",
      "        .   @param H The input homography matrix between two images.\n",
      "        .   @param K The input intrinsic camera calibration matrix.\n",
      "        .   @param rotations Array of rotation matrices.\n",
      "        .   @param translations Array of translation matrices.\n",
      "        .   @param normals Array of plane normal matrices.\n",
      "        .   \n",
      "        .   This function extracts relative camera motion between two views observing a planar object from the\n",
      "        .   homography H induced by the plane. The intrinsic camera matrix K must also be provided. The function\n",
      "        .   may return up to four mathematical solution sets. At least two of the solutions may further be\n",
      "        .   invalidated if point correspondences are available by applying positive depth constraint (all points\n",
      "        .   must be in front of the camera). The decomposition method is described in detail in @cite Malis .\n",
      "    \n",
      "    decomposeProjectionMatrix(...)\n",
      "        decomposeProjectionMatrix(projMatrix[, cameraMatrix[, rotMatrix[, transVect[, rotMatrixX[, rotMatrixY[, rotMatrixZ[, eulerAngles]]]]]]]) -> cameraMatrix, rotMatrix, transVect, rotMatrixX, rotMatrixY, rotMatrixZ, eulerAngles\n",
      "        .   @brief Decomposes a projection matrix into a rotation matrix and a camera matrix.\n",
      "        .   \n",
      "        .   @param projMatrix 3x4 input projection matrix P.\n",
      "        .   @param cameraMatrix Output 3x3 camera matrix K.\n",
      "        .   @param rotMatrix Output 3x3 external rotation matrix R.\n",
      "        .   @param transVect Output 4x1 translation vector T.\n",
      "        .   @param rotMatrixX Optional 3x3 rotation matrix around x-axis.\n",
      "        .   @param rotMatrixY Optional 3x3 rotation matrix around y-axis.\n",
      "        .   @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.\n",
      "        .   @param eulerAngles Optional three-element vector containing three Euler angles of rotation in\n",
      "        .   degrees.\n",
      "        .   \n",
      "        .   The function computes a decomposition of a projection matrix into a calibration and a rotation\n",
      "        .   matrix and the position of a camera.\n",
      "        .   \n",
      "        .   It optionally returns three rotation matrices, one for each axis, and three Euler angles that could\n",
      "        .   be used in OpenGL. Note, there is always more than one sequence of rotations about the three\n",
      "        .   principal axes that results in the same orientation of an object, e.g. see @cite Slabaugh . Returned\n",
      "        .   tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.\n",
      "        .   \n",
      "        .   The function is based on RQDecomp3x3 .\n",
      "    \n",
      "    demosaicing(...)\n",
      "        demosaicing(src, code[, dst[, dstCn]]) -> dst\n",
      "        .   @brief main function for all demosaicing processes\n",
      "        .   \n",
      "        .   @param src input image: 8-bit unsigned or 16-bit unsigned.\n",
      "        .   @param dst output image of the same size and depth as src.\n",
      "        .   @param code Color space conversion code (see the description below).\n",
      "        .   @param dstCn number of channels in the destination image; if the parameter is 0, the number of the\n",
      "        .   channels is derived automatically from src and code.\n",
      "        .   \n",
      "        .   The function can do the following transformations:\n",
      "        .   \n",
      "        .   -   Demosaicing using bilinear interpolation\n",
      "        .   \n",
      "        .       #COLOR_BayerBG2BGR , #COLOR_BayerGB2BGR , #COLOR_BayerRG2BGR , #COLOR_BayerGR2BGR\n",
      "        .   \n",
      "        .       #COLOR_BayerBG2GRAY , #COLOR_BayerGB2GRAY , #COLOR_BayerRG2GRAY , #COLOR_BayerGR2GRAY\n",
      "        .   \n",
      "        .   -   Demosaicing using Variable Number of Gradients.\n",
      "        .   \n",
      "        .       #COLOR_BayerBG2BGR_VNG , #COLOR_BayerGB2BGR_VNG , #COLOR_BayerRG2BGR_VNG , #COLOR_BayerGR2BGR_VNG\n",
      "        .   \n",
      "        .   -   Edge-Aware Demosaicing.\n",
      "        .   \n",
      "        .       #COLOR_BayerBG2BGR_EA , #COLOR_BayerGB2BGR_EA , #COLOR_BayerRG2BGR_EA , #COLOR_BayerGR2BGR_EA\n",
      "        .   \n",
      "        .   -   Demosaicing with alpha channel\n",
      "        .   \n",
      "        .       #COLOR_BayerBG2BGRA , #COLOR_BayerGB2BGRA , #COLOR_BayerRG2BGRA , #COLOR_BayerGR2BGRA\n",
      "        .   \n",
      "        .   @sa cvtColor\n",
      "    \n",
      "    denoise_TVL1(...)\n",
      "        denoise_TVL1(observations, result[, lambda[, niters]]) -> None\n",
      "        .   @brief Primal-dual algorithm is an algorithm for solving special types of variational problems (that is,\n",
      "        .   finding a function to minimize some functional). As the image denoising, in particular, may be seen\n",
      "        .   as the variational problem, primal-dual algorithm then can be used to perform denoising and this is\n",
      "        .   exactly what is implemented.\n",
      "        .   \n",
      "        .   It should be noted, that this implementation was taken from the July 2013 blog entry\n",
      "        .   @cite MA13 , which also contained (slightly more general) ready-to-use source code on Python.\n",
      "        .   Subsequently, that code was rewritten on C++ with the usage of openCV by Vadim Pisarevsky at the end\n",
      "        .   of July 2013 and finally it was slightly adapted by later authors.\n",
      "        .   \n",
      "        .   Although the thorough discussion and justification of the algorithm involved may be found in\n",
      "        .   @cite ChambolleEtAl, it might make sense to skim over it here, following @cite MA13 . To begin\n",
      "        .   with, we consider the 1-byte gray-level images as the functions from the rectangular domain of\n",
      "        .   pixels (it may be seen as set\n",
      "        .   \\f$\\left\\{(x,y)\\in\\mathbb{N}\\times\\mathbb{N}\\mid 1\\leq x\\leq n,\\;1\\leq y\\leq m\\right\\}\\f$ for some\n",
      "        .   \\f$m,\\;n\\in\\mathbb{N}\\f$) into \\f$\\{0,1,\\dots,255\\}\\f$. We shall denote the noised images as \\f$f_i\\f$ and with\n",
      "        .   this view, given some image \\f$x\\f$ of the same size, we may measure how bad it is by the formula\n",
      "        .   \n",
      "        .   \\f[\\left\\|\\left\\|\\nabla x\\right\\|\\right\\| + \\lambda\\sum_i\\left\\|\\left\\|x-f_i\\right\\|\\right\\|\\f]\n",
      "        .   \n",
      "        .   \\f$\\|\\|\\cdot\\|\\|\\f$ here denotes \\f$L_2\\f$-norm and as you see, the first addend states that we want our\n",
      "        .   image to be smooth (ideally, having zero gradient, thus being constant) and the second states that\n",
      "        .   we want our result to be close to the observations we've got. If we treat \\f$x\\f$ as a function, this is\n",
      "        .   exactly the functional what we seek to minimize and here the Primal-Dual algorithm comes into play.\n",
      "        .   \n",
      "        .   @param observations This array should contain one or more noised versions of the image that is to\n",
      "        .   be restored.\n",
      "        .   @param result Here the denoised image will be stored. There is no need to do pre-allocation of\n",
      "        .   storage space, as it will be automatically allocated, if necessary.\n",
      "        .   @param lambda Corresponds to \\f$\\lambda\\f$ in the formulas above. As it is enlarged, the smooth\n",
      "        .   (blurred) images are treated more favorably than detailed (but maybe more noised) ones. Roughly\n",
      "        .   speaking, as it becomes smaller, the result will be more blur but more sever outliers will be\n",
      "        .   removed.\n",
      "        .   @param niters Number of iterations that the algorithm will run. Of course, as more iterations as\n",
      "        .   better, but it is hard to quantitatively refine this statement, so just use the default and\n",
      "        .   increase it if the results are poor.\n",
      "    \n",
      "    destroyAllWindows(...)\n",
      "        destroyAllWindows() -> None\n",
      "        .   @brief Destroys all of the HighGUI windows.\n",
      "        .   \n",
      "        .   The function destroyAllWindows destroys all of the opened HighGUI windows.\n",
      "    \n",
      "    destroyWindow(...)\n",
      "        destroyWindow(winname) -> None\n",
      "        .   @brief Destroys the specified window.\n",
      "        .   \n",
      "        .   The function destroyWindow destroys the window with the given name.\n",
      "        .   \n",
      "        .   @param winname Name of the window to be destroyed.\n",
      "    \n",
      "    detailEnhance(...)\n",
      "        detailEnhance(src[, dst[, sigma_s[, sigma_r]]]) -> dst\n",
      "        .   @brief This filter enhances the details of a particular image.\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src.\n",
      "        .   @param sigma_s %Range between 0 to 200.\n",
      "        .   @param sigma_r %Range between 0 to 1.\n",
      "    \n",
      "    determinant(...)\n",
      "        determinant(mtx) -> retval\n",
      "        .   @brief Returns the determinant of a square floating-point matrix.\n",
      "        .   \n",
      "        .   The function cv::determinant calculates and returns the determinant of the\n",
      "        .   specified matrix. For small matrices ( mtx.cols=mtx.rows\\<=3 ), the\n",
      "        .   direct method is used. For larger matrices, the function uses LU\n",
      "        .   factorization with partial pivoting.\n",
      "        .   \n",
      "        .   For symmetric positively-determined matrices, it is also possible to use\n",
      "        .   eigen decomposition to calculate the determinant.\n",
      "        .   @param mtx input matrix that must have CV_32FC1 or CV_64FC1 type and\n",
      "        .   square size.\n",
      "        .   @sa trace, invert, solve, eigen, @ref MatrixExpressions\n",
      "    \n",
      "    dft(...)\n",
      "        dft(src[, dst[, flags[, nonzeroRows]]]) -> dst\n",
      "        .   @brief Performs a forward or inverse Discrete Fourier transform of a 1D or 2D floating-point array.\n",
      "        .   \n",
      "        .   The function cv::dft performs one of the following:\n",
      "        .   -   Forward the Fourier transform of a 1D vector of N elements:\n",
      "        .       \\f[Y = F^{(N)}  \\cdot X,\\f]\n",
      "        .       where \\f$F^{(N)}_{jk}=\\exp(-2\\pi i j k/N)\\f$ and \\f$i=\\sqrt{-1}\\f$\n",
      "        .   -   Inverse the Fourier transform of a 1D vector of N elements:\n",
      "        .       \\f[\\begin{array}{l} X'=  \\left (F^{(N)} \\right )^{-1}  \\cdot Y =  \\left (F^{(N)} \\right )^*  \\cdot y  \\\\ X = (1/N)  \\cdot X, \\end{array}\\f]\n",
      "        .       where \\f$F^*=\\left(\\textrm{Re}(F^{(N)})-\\textrm{Im}(F^{(N)})\\right)^T\\f$\n",
      "        .   -   Forward the 2D Fourier transform of a M x N matrix:\n",
      "        .       \\f[Y = F^{(M)}  \\cdot X  \\cdot F^{(N)}\\f]\n",
      "        .   -   Inverse the 2D Fourier transform of a M x N matrix:\n",
      "        .       \\f[\\begin{array}{l} X'=  \\left (F^{(M)} \\right )^*  \\cdot Y  \\cdot \\left (F^{(N)} \\right )^* \\\\ X =  \\frac{1}{M \\cdot N} \\cdot X' \\end{array}\\f]\n",
      "        .   \n",
      "        .   In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input\n",
      "        .   spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*\n",
      "        .   (complex-conjugate-symmetrical). It was borrowed from IPL (Intel\\* Image Processing Library). Here\n",
      "        .   is how 2D *CCS* spectrum looks:\n",
      "        .   \\f[\\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2} & Im Y_{0,2} &  \\cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2}  \\\\ Re Y_{1,0} & Re Y_{1,1} & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} &  \\cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2}  \\\\ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} &  \\cdots & Re Y_{2,N/2-1} & Im Y_{2,N/2-1} & Im Y_{1,N/2}  \\\\ \\hdotsfor{9} \\\\ Re Y_{M/2-1,0} &  Re Y_{M-3,1}  & Im Y_{M-3,1} &  \\hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2}  \\\\ Im Y_{M/2-1,0} &  Re Y_{M-2,1}  & Im Y_{M-2,1} &  \\hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2}  \\\\ Re Y_{M/2,0}  &  Re Y_{M-1,1} &  Im Y_{M-1,1} &  \\hdotsfor{3} & Re Y_{M-1,N/2-1} & Im Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   In case of 1D transform of a real vector, the output looks like the first row of the matrix above.\n",
      "        .   \n",
      "        .   So, the function chooses an operation mode depending on the flags and size of the input array:\n",
      "        .   -   If #DFT_ROWS is set or the input array has a single row or single column, the function\n",
      "        .       performs a 1D forward or inverse transform of each row of a matrix when #DFT_ROWS is set.\n",
      "        .       Otherwise, it performs a 2D transform.\n",
      "        .   -   If the input array is real and #DFT_INVERSE is not set, the function performs a forward 1D or\n",
      "        .       2D transform:\n",
      "        .       -   When #DFT_COMPLEX_OUTPUT is set, the output is a complex matrix of the same size as\n",
      "        .           input.\n",
      "        .       -   When #DFT_COMPLEX_OUTPUT is not set, the output is a real matrix of the same size as\n",
      "        .           input. In case of 2D transform, it uses the packed format as shown above. In case of a\n",
      "        .           single 1D transform, it looks like the first row of the matrix above. In case of\n",
      "        .           multiple 1D transforms (when using the #DFT_ROWS flag), each row of the output matrix\n",
      "        .           looks like the first row of the matrix above.\n",
      "        .   -   If the input array is complex and either #DFT_INVERSE or #DFT_REAL_OUTPUT are not set, the\n",
      "        .       output is a complex array of the same size as input. The function performs a forward or\n",
      "        .       inverse 1D or 2D transform of the whole input array or each row of the input array\n",
      "        .       independently, depending on the flags DFT_INVERSE and DFT_ROWS.\n",
      "        .   -   When #DFT_INVERSE is set and the input array is real, or it is complex but #DFT_REAL_OUTPUT\n",
      "        .       is set, the output is a real array of the same size as input. The function performs a 1D or 2D\n",
      "        .       inverse transformation of the whole input array or each individual row, depending on the flags\n",
      "        .       #DFT_INVERSE and #DFT_ROWS.\n",
      "        .   \n",
      "        .   If #DFT_SCALE is set, the scaling is done after the transformation.\n",
      "        .   \n",
      "        .   Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed\n",
      "        .   efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the\n",
      "        .   current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize\n",
      "        .   method.\n",
      "        .   \n",
      "        .   The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:\n",
      "        .   @code\n",
      "        .       void convolveDFT(InputArray A, InputArray B, OutputArray C)\n",
      "        .       {\n",
      "        .           // reallocate the output array if needed\n",
      "        .           C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());\n",
      "        .           Size dftSize;\n",
      "        .           // calculate the size of DFT transform\n",
      "        .           dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);\n",
      "        .           dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);\n",
      "        .   \n",
      "        .           // allocate temporary buffers and initialize them with 0's\n",
      "        .           Mat tempA(dftSize, A.type(), Scalar::all(0));\n",
      "        .           Mat tempB(dftSize, B.type(), Scalar::all(0));\n",
      "        .   \n",
      "        .           // copy A and B to the top-left corners of tempA and tempB, respectively\n",
      "        .           Mat roiA(tempA, Rect(0,0,A.cols,A.rows));\n",
      "        .           A.copyTo(roiA);\n",
      "        .           Mat roiB(tempB, Rect(0,0,B.cols,B.rows));\n",
      "        .           B.copyTo(roiB);\n",
      "        .   \n",
      "        .           // now transform the padded A & B in-place;\n",
      "        .           // use \"nonzeroRows\" hint for faster processing\n",
      "        .           dft(tempA, tempA, 0, A.rows);\n",
      "        .           dft(tempB, tempB, 0, B.rows);\n",
      "        .   \n",
      "        .           // multiply the spectrums;\n",
      "        .           // the function handles packed spectrum representations well\n",
      "        .           mulSpectrums(tempA, tempB, tempA);\n",
      "        .   \n",
      "        .           // transform the product back from the frequency domain.\n",
      "        .           // Even though all the result rows will be non-zero,\n",
      "        .           // you need only the first C.rows of them, and thus you\n",
      "        .           // pass nonzeroRows == C.rows\n",
      "        .           dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);\n",
      "        .   \n",
      "        .           // now copy the result back to C.\n",
      "        .           tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);\n",
      "        .   \n",
      "        .           // all the temporary buffers will be deallocated automatically\n",
      "        .       }\n",
      "        .   @endcode\n",
      "        .   To optimize this sample, consider the following approaches:\n",
      "        .   -   Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to\n",
      "        .       the top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole\n",
      "        .       tempA and tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols)\n",
      "        .       rightmost columns of the matrices.\n",
      "        .   -   This DFT-based convolution does not have to be applied to the whole big arrays, especially if B\n",
      "        .       is significantly smaller than A or vice versa. Instead, you can calculate convolution by parts.\n",
      "        .       To do this, you need to split the output array C into multiple tiles. For each tile, estimate\n",
      "        .       which parts of A and B are required to calculate convolution in this tile. If the tiles in C are\n",
      "        .       too small, the speed will decrease a lot because of repeated work. In the ultimate case, when\n",
      "        .       each tile in C is a single pixel, the algorithm becomes equivalent to the naive convolution\n",
      "        .       algorithm. If the tiles are too big, the temporary arrays tempA and tempB become too big and\n",
      "        .       there is also a slowdown because of bad cache locality. So, there is an optimal tile size\n",
      "        .       somewhere in the middle.\n",
      "        .   -   If different tiles in C can be calculated in parallel and, thus, the convolution is done by\n",
      "        .       parts, the loop can be threaded.\n",
      "        .   \n",
      "        .   All of the above improvements have been implemented in #matchTemplate and #filter2D . Therefore, by\n",
      "        .   using them, you can get the performance even better than with the above theoretically optimal\n",
      "        .   implementation. Though, those two functions actually calculate cross-correlation, not convolution,\n",
      "        .   so you need to \"flip\" the second convolution operand B vertically and horizontally using flip .\n",
      "        .   @note\n",
      "        .   -   An example using the discrete fourier transform can be found at\n",
      "        .       opencv_source_code/samples/cpp/dft.cpp\n",
      "        .   -   (Python) An example using the dft functionality to perform Wiener deconvolution can be found\n",
      "        .       at opencv_source/samples/python/deconvolution.py\n",
      "        .   -   (Python) An example rearranging the quadrants of a Fourier image can be found at\n",
      "        .       opencv_source/samples/python/dft.py\n",
      "        .   @param src input array that could be real or complex.\n",
      "        .   @param dst output array whose size and type depends on the flags .\n",
      "        .   @param flags transformation flags, representing a combination of the #DftFlags\n",
      "        .   @param nonzeroRows when the parameter is not zero, the function assumes that only the first\n",
      "        .   nonzeroRows rows of the input array (#DFT_INVERSE is not set) or only the first nonzeroRows of the\n",
      "        .   output array (#DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the\n",
      "        .   rows more efficiently and save some time; this technique is very useful for calculating array\n",
      "        .   cross-correlation or convolution using DFT.\n",
      "        .   @sa dct , getOptimalDFTSize , mulSpectrums, filter2D , matchTemplate , flip , cartToPolar ,\n",
      "        .   magnitude , phase\n",
      "    \n",
      "    dilate(...)\n",
      "        dilate(src, kernel[, dst[, anchor[, iterations[, borderType[, borderValue]]]]]) -> dst\n",
      "        .   @brief Dilates an image by using a specific structuring element.\n",
      "        .   \n",
      "        .   The function dilates the source image using the specified structuring element that determines the\n",
      "        .   shape of a pixel neighborhood over which the maximum is taken:\n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\max _{(x',y'):  \\, \\texttt{element} (x',y') \\ne0 } \\texttt{src} (x+x',y+y')\\f]\n",
      "        .   \n",
      "        .   The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In\n",
      "        .   case of multi-channel images, each channel is processed independently.\n",
      "        .   \n",
      "        .   @param src input image; the number of channels can be arbitrary, but the depth should be one of\n",
      "        .   CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "        .   @param dst output image of the same size and type as src.\n",
      "        .   @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular\n",
      "        .   structuring element is used. Kernel can be created using #getStructuringElement\n",
      "        .   @param anchor position of the anchor within the element; default value (-1, -1) means that the\n",
      "        .   anchor is at the element center.\n",
      "        .   @param iterations number of times dilation is applied.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   @param borderValue border value in case of a constant border\n",
      "        .   @sa  erode, morphologyEx, getStructuringElement\n",
      "    \n",
      "    displayOverlay(...)\n",
      "        displayOverlay(winname, text[, delayms]) -> None\n",
      "        .   @brief Displays a text on a window image as an overlay for a specified duration.\n",
      "        .   \n",
      "        .   The function displayOverlay displays useful information/tips on top of the window for a certain\n",
      "        .   amount of time *delayms*. The function does not modify the image, displayed in the window, that is,\n",
      "        .   after the specified delay the original content of the window is restored.\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param text Overlay text to write on a window image.\n",
      "        .   @param delayms The period (in milliseconds), during which the overlay text is displayed. If this\n",
      "        .   function is called before the previous overlay text timed out, the timer is restarted and the text\n",
      "        .   is updated. If this value is zero, the text never disappears.\n",
      "    \n",
      "    displayStatusBar(...)\n",
      "        displayStatusBar(winname, text[, delayms]) -> None\n",
      "        .   @brief Displays a text on the window statusbar during the specified period of time.\n",
      "        .   \n",
      "        .   The function displayStatusBar displays useful information/tips on top of the window for a certain\n",
      "        .   amount of time *delayms* . This information is displayed on the window statusbar (the window must be\n",
      "        .   created with the CV_GUI_EXPANDED flags).\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param text Text to write on the window statusbar.\n",
      "        .   @param delayms Duration (in milliseconds) to display the text. If this function is called before\n",
      "        .   the previous text timed out, the timer is restarted and the text is updated. If this value is\n",
      "        .   zero, the text never disappears.\n",
      "    \n",
      "    distanceTransform(...)\n",
      "        distanceTransform(src, distanceType, maskSize[, dst[, dstType]]) -> dst\n",
      "        .   @overload\n",
      "        .   @param src 8-bit, single-channel (binary) source image.\n",
      "        .   @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n",
      "        .   single-channel image of the same size as src .\n",
      "        .   @param distanceType Type of distance, see #DistanceTypes\n",
      "        .   @param maskSize Size of the distance transform mask, see #DistanceTransformMasks. In case of the\n",
      "        .   #DIST_L1 or #DIST_C distance type, the parameter is forced to 3 because a \\f$3\\times 3\\f$ mask gives\n",
      "        .   the same result as \\f$5\\times 5\\f$ or any larger aperture.\n",
      "        .   @param dstType Type of output image. It can be CV_8U or CV_32F. Type CV_8U can be used only for\n",
      "        .   the first variant of the function and distanceType == #DIST_L1.\n",
      "    \n",
      "    distanceTransformWithLabels(...)\n",
      "        distanceTransformWithLabels(src, distanceType, maskSize[, dst[, labels[, labelType]]]) -> dst, labels\n",
      "        .   @brief Calculates the distance to the closest zero pixel for each pixel of the source image.\n",
      "        .   \n",
      "        .   The function cv::distanceTransform calculates the approximate or precise distance from every binary\n",
      "        .   image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be zero.\n",
      "        .   \n",
      "        .   When maskSize == #DIST_MASK_PRECISE and distanceType == #DIST_L2 , the function runs the\n",
      "        .   algorithm described in @cite Felzenszwalb04 . This algorithm is parallelized with the TBB library.\n",
      "        .   \n",
      "        .   In other cases, the algorithm @cite Borgefors86 is used. This means that for a pixel the function\n",
      "        .   finds the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,\n",
      "        .   diagonal, or knight's move (the latest is available for a \\f$5\\times 5\\f$ mask). The overall\n",
      "        .   distance is calculated as a sum of these basic distances. Since the distance function should be\n",
      "        .   symmetric, all of the horizontal and vertical shifts must have the same cost (denoted as a ), all\n",
      "        .   the diagonal shifts must have the same cost (denoted as `b`), and all knight's moves must have the\n",
      "        .   same cost (denoted as `c`). For the #DIST_C and #DIST_L1 types, the distance is calculated\n",
      "        .   precisely, whereas for #DIST_L2 (Euclidean distance) the distance can be calculated only with a\n",
      "        .   relative error (a \\f$5\\times 5\\f$ mask gives more accurate results). For `a`,`b`, and `c`, OpenCV\n",
      "        .   uses the values suggested in the original paper:\n",
      "        .   - DIST_L1: `a = 1, b = 2`\n",
      "        .   - DIST_L2:\n",
      "        .       - `3 x 3`: `a=0.955, b=1.3693`\n",
      "        .       - `5 x 5`: `a=1, b=1.4, c=2.1969`\n",
      "        .   - DIST_C: `a = 1, b = 1`\n",
      "        .   \n",
      "        .   Typically, for a fast, coarse distance estimation #DIST_L2, a \\f$3\\times 3\\f$ mask is used. For a\n",
      "        .   more accurate distance estimation #DIST_L2, a \\f$5\\times 5\\f$ mask or the precise algorithm is used.\n",
      "        .   Note that both the precise and the approximate algorithms are linear on the number of pixels.\n",
      "        .   \n",
      "        .   This variant of the function does not only compute the minimum distance for each pixel \\f$(x, y)\\f$\n",
      "        .   but also identifies the nearest connected component consisting of zero pixels\n",
      "        .   (labelType==#DIST_LABEL_CCOMP) or the nearest zero pixel (labelType==#DIST_LABEL_PIXEL). Index of the\n",
      "        .   component/pixel is stored in `labels(x, y)`. When labelType==#DIST_LABEL_CCOMP, the function\n",
      "        .   automatically finds connected components of zero pixels in the input image and marks them with\n",
      "        .   distinct labels. When labelType==#DIST_LABEL_CCOMP, the function scans through the input image and\n",
      "        .   marks all the zero pixels with distinct labels.\n",
      "        .   \n",
      "        .   In this mode, the complexity is still linear. That is, the function provides a very fast way to\n",
      "        .   compute the Voronoi diagram for a binary image. Currently, the second variant can use only the\n",
      "        .   approximate distance transform algorithm, i.e. maskSize=#DIST_MASK_PRECISE is not supported\n",
      "        .   yet.\n",
      "        .   \n",
      "        .   @param src 8-bit, single-channel (binary) source image.\n",
      "        .   @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,\n",
      "        .   single-channel image of the same size as src.\n",
      "        .   @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type\n",
      "        .   CV_32SC1 and the same size as src.\n",
      "        .   @param distanceType Type of distance, see #DistanceTypes\n",
      "        .   @param maskSize Size of the distance transform mask, see #DistanceTransformMasks.\n",
      "        .   #DIST_MASK_PRECISE is not supported by this variant. In case of the #DIST_L1 or #DIST_C distance type,\n",
      "        .   the parameter is forced to 3 because a \\f$3\\times 3\\f$ mask gives the same result as \\f$5\\times\n",
      "        .   5\\f$ or any larger aperture.\n",
      "        .   @param labelType Type of the label array to build, see #DistanceTransformLabelTypes.\n",
      "    \n",
      "    divide(...)\n",
      "        divide(src1, src2[, dst[, scale[, dtype]]]) -> dst\n",
      "        .   @brief Performs per-element division of two arrays or a scalar by an array.\n",
      "        .   \n",
      "        .   The function cv::divide divides one array by another:\n",
      "        .   \\f[\\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\\f]\n",
      "        .   or a scalar by an array when there is no src1 :\n",
      "        .   \\f[\\texttt{dst(I) = saturate(scale/src2(I))}\\f]\n",
      "        .   \n",
      "        .   Different channels of multi-channel arrays are processed independently.\n",
      "        .   \n",
      "        .   For integer types when src2(I) is zero, dst(I) will also be zero.\n",
      "        .   \n",
      "        .   @note In case of floating point data there is no special defined behavior for zero src2(I) values.\n",
      "        .   Regular floating-point division is used.\n",
      "        .   Expect correct IEEE-754 behaviour for floating-point data (with NaN, Inf result values).\n",
      "        .   \n",
      "        .   @note Saturation is not applied when the output array has the depth CV_32S. You may even get\n",
      "        .   result of an incorrect sign in the case of overflow.\n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size and type as src1.\n",
      "        .   @param scale scalar factor.\n",
      "        .   @param dst output array of the same size and type as src2.\n",
      "        .   @param dtype optional depth of the output array; if -1, dst will have depth src2.depth(), but in\n",
      "        .   case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().\n",
      "        .   @sa  multiply, add, subtract\n",
      "        \n",
      "        \n",
      "        \n",
      "        divide(scale, src2[, dst[, dtype]]) -> dst\n",
      "        .   @overload\n",
      "    \n",
      "    dnn_registerLayer(...)\n",
      "        registerLayer(type, class) -> None\n",
      "    \n",
      "    dnn_unregisterLayer(...)\n",
      "        unregisterLayer(type) -> None\n",
      "    \n",
      "    drawChessboardCorners(...)\n",
      "        drawChessboardCorners(image, patternSize, corners, patternWasFound) -> image\n",
      "        .   @brief Renders the detected chessboard corners.\n",
      "        .   \n",
      "        .   @param image Destination image. It must be an 8-bit color image.\n",
      "        .   @param patternSize Number of inner corners per a chessboard row and column\n",
      "        .   (patternSize = cv::Size(points_per_row,points_per_column)).\n",
      "        .   @param corners Array of detected corners, the output of findChessboardCorners.\n",
      "        .   @param patternWasFound Parameter indicating whether the complete board was found or not. The\n",
      "        .   return value of findChessboardCorners should be passed here.\n",
      "        .   \n",
      "        .   The function draws individual chessboard corners detected either as red circles if the board was not\n",
      "        .   found, or as colored corners connected with lines if the board was found.\n",
      "    \n",
      "    drawContours(...)\n",
      "        drawContours(image, contours, contourIdx, color[, thickness[, lineType[, hierarchy[, maxLevel[, offset]]]]]) -> image\n",
      "        .   @brief Draws contours outlines or filled contours.\n",
      "        .   \n",
      "        .   The function draws contour outlines in the image if \\f$\\texttt{thickness} \\ge 0\\f$ or fills the area\n",
      "        .   bounded by the contours if \\f$\\texttt{thickness}<0\\f$ . The example below shows how to retrieve\n",
      "        .   connected components from the binary image and label them: :\n",
      "        .   @include snippets/imgproc_drawContours.cpp\n",
      "        .   \n",
      "        .   @param image Destination image.\n",
      "        .   @param contours All the input contours. Each contour is stored as a point vector.\n",
      "        .   @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are drawn.\n",
      "        .   @param color Color of the contours.\n",
      "        .   @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,\n",
      "        .   thickness=#FILLED ), the contour interiors are drawn.\n",
      "        .   @param lineType Line connectivity. See #LineTypes\n",
      "        .   @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only\n",
      "        .   some of the contours (see maxLevel ).\n",
      "        .   @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.\n",
      "        .   If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function\n",
      "        .   draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This\n",
      "        .   parameter is only taken into account when there is hierarchy available.\n",
      "        .   @param offset Optional contour shift parameter. Shift all the drawn contours by the specified\n",
      "        .   \\f$\\texttt{offset}=(dx,dy)\\f$ .\n",
      "        .   @note When thickness=#FILLED, the function is designed to handle connected components with holes correctly\n",
      "        .   even when no hierarchy date is provided. This is done by analyzing all the outlines together\n",
      "        .   using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved\n",
      "        .   contours. In order to solve this problem, you need to call #drawContours separately for each sub-group\n",
      "        .   of contours, or iterate over the collection using contourIdx parameter.\n",
      "    \n",
      "    drawFrameAxes(...)\n",
      "        drawFrameAxes(image, cameraMatrix, distCoeffs, rvec, tvec, length[, thickness]) -> image\n",
      "        .   @brief Draw axes of the world/object coordinate system from pose estimation. @sa solvePnP\n",
      "        .   \n",
      "        .   @param image Input/output image. It must have 1 or 3 channels. The number of channels is not altered.\n",
      "        .   @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters.\n",
      "        .   \\f$A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.\n",
      "        .   @param rvec Rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system.\n",
      "        .   @param tvec Translation vector.\n",
      "        .   @param length Length of the painted axes in the same unit than tvec (usually in meters).\n",
      "        .   @param thickness Line thickness of the painted axes.\n",
      "        .   \n",
      "        .   This function draws the axes of the world/object coordinate system w.r.t. to the camera frame.\n",
      "        .   OX is drawn in red, OY in green and OZ in blue.\n",
      "    \n",
      "    drawKeypoints(...)\n",
      "        drawKeypoints(image, keypoints, outImage[, color[, flags]]) -> outImage\n",
      "        .   @brief Draws keypoints.\n",
      "        .   \n",
      "        .   @param image Source image.\n",
      "        .   @param keypoints Keypoints from the source image.\n",
      "        .   @param outImage Output image. Its content depends on the flags value defining what is drawn in the\n",
      "        .   output image. See possible flags bit values below.\n",
      "        .   @param color Color of keypoints.\n",
      "        .   @param flags Flags setting drawing features. Possible flags bit values are defined by\n",
      "        .   DrawMatchesFlags. See details above in drawMatches .\n",
      "        .   \n",
      "        .   @note\n",
      "        .   For Python API, flags are modified as cv.DRAW_MATCHES_FLAGS_DEFAULT,\n",
      "        .   cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,\n",
      "        .   cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS\n",
      "    \n",
      "    drawMarker(...)\n",
      "        drawMarker(img, position, color[, markerType[, markerSize[, thickness[, line_type]]]]) -> img\n",
      "        .   @brief Draws a marker on a predefined position in an image.\n",
      "        .   \n",
      "        .   The function cv::drawMarker draws a marker on a given position in the image. For the moment several\n",
      "        .   marker types are supported, see #MarkerTypes for more information.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param position The point where the crosshair is positioned.\n",
      "        .   @param color Line color.\n",
      "        .   @param markerType The specific type of marker you want to use, see #MarkerTypes\n",
      "        .   @param thickness Line thickness.\n",
      "        .   @param line_type Type of the line, See #LineTypes\n",
      "        .   @param markerSize The length of the marker axis [default = 20 pixels]\n",
      "    \n",
      "    drawMatches(...)\n",
      "        drawMatches(img1, keypoints1, img2, keypoints2, matches1to2, outImg[, matchColor[, singlePointColor[, matchesMask[, flags]]]]) -> outImg\n",
      "        .   @brief Draws the found matches of keypoints from two images.\n",
      "        .   \n",
      "        .   @param img1 First source image.\n",
      "        .   @param keypoints1 Keypoints from the first source image.\n",
      "        .   @param img2 Second source image.\n",
      "        .   @param keypoints2 Keypoints from the second source image.\n",
      "        .   @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]\n",
      "        .   has a corresponding point in keypoints2[matches[i]] .\n",
      "        .   @param outImg Output image. Its content depends on the flags value defining what is drawn in the\n",
      "        .   output image. See possible flags bit values below.\n",
      "        .   @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1)\n",
      "        .   , the color is generated randomly.\n",
      "        .   @param singlePointColor Color of single keypoints (circles), which means that keypoints do not\n",
      "        .   have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.\n",
      "        .   @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are\n",
      "        .   drawn.\n",
      "        .   @param flags Flags setting drawing features. Possible flags bit values are defined by\n",
      "        .   DrawMatchesFlags.\n",
      "        .   \n",
      "        .   This function draws matches of keypoints from two images in the output image. Match is a line\n",
      "        .   connecting two keypoints (circles). See cv::DrawMatchesFlags.\n",
      "    \n",
      "    drawMatchesKnn(...)\n",
      "        drawMatchesKnn(img1, keypoints1, img2, keypoints2, matches1to2, outImg[, matchColor[, singlePointColor[, matchesMask[, flags]]]]) -> outImg\n",
      "        .   @overload\n",
      "    \n",
      "    edgePreservingFilter(...)\n",
      "        edgePreservingFilter(src[, dst[, flags[, sigma_s[, sigma_r]]]]) -> dst\n",
      "        .   @brief Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing\n",
      "        .   filters are used in many different applications @cite EM11 .\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst Output 8-bit 3-channel image.\n",
      "        .   @param flags Edge preserving filters: cv::RECURS_FILTER or cv::NORMCONV_FILTER\n",
      "        .   @param sigma_s %Range between 0 to 200.\n",
      "        .   @param sigma_r %Range between 0 to 1.\n",
      "    \n",
      "    eigen(...)\n",
      "        eigen(src[, eigenvalues[, eigenvectors]]) -> retval, eigenvalues, eigenvectors\n",
      "        .   @brief Calculates eigenvalues and eigenvectors of a symmetric matrix.\n",
      "        .   \n",
      "        .   The function cv::eigen calculates just eigenvalues, or eigenvalues and eigenvectors of the symmetric\n",
      "        .   matrix src:\n",
      "        .   @code\n",
      "        .       src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @note Use cv::eigenNonSymmetric for calculation of real eigenvalues and eigenvectors of non-symmetric matrix.\n",
      "        .   \n",
      "        .   @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical\n",
      "        .   (src ^T^ == src).\n",
      "        .   @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored\n",
      "        .   in the descending order.\n",
      "        .   @param eigenvectors output matrix of eigenvectors; it has the same size and type as src; the\n",
      "        .   eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding\n",
      "        .   eigenvalues.\n",
      "        .   @sa eigenNonSymmetric, completeSymm , PCA\n",
      "    \n",
      "    eigenNonSymmetric(...)\n",
      "        eigenNonSymmetric(src[, eigenvalues[, eigenvectors]]) -> eigenvalues, eigenvectors\n",
      "        .   @brief Calculates eigenvalues and eigenvectors of a non-symmetric matrix (real eigenvalues only).\n",
      "        .   \n",
      "        .   @note Assumes real eigenvalues.\n",
      "        .   \n",
      "        .   The function calculates eigenvalues and eigenvectors (optional) of the square matrix src:\n",
      "        .   @code\n",
      "        .       src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @param src input matrix (CV_32FC1 or CV_64FC1 type).\n",
      "        .   @param eigenvalues output vector of eigenvalues (type is the same type as src).\n",
      "        .   @param eigenvectors output matrix of eigenvectors (type is the same type as src). The eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.\n",
      "        .   @sa eigen\n",
      "    \n",
      "    ellipse(...)\n",
      "        ellipse(img, center, axes, angle, startAngle, endAngle, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @brief Draws a simple or thick elliptic arc or fills an ellipse sector.\n",
      "        .   \n",
      "        .   The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic\n",
      "        .   arc, or a filled ellipse sector. The drawing code uses general parametric form.\n",
      "        .   A piecewise-linear curve is used to approximate the elliptic arc\n",
      "        .   boundary. If you need more control of the ellipse rendering, you can retrieve the curve using\n",
      "        .   #ellipse2Poly and then render it with #polylines or fill it with #fillPoly. If you use the first\n",
      "        .   variant of the function and want to draw the whole ellipse, not an arc, pass `startAngle=0` and\n",
      "        .   `endAngle=360`. If `startAngle` is greater than `endAngle`, they are swapped. The figure below explains\n",
      "        .   the meaning of the parameters to draw the blue arc.\n",
      "        .   \n",
      "        .   ![Parameters of Elliptic Arc](pics/ellipse.svg)\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param center Center of the ellipse.\n",
      "        .   @param axes Half of the size of the ellipse main axes.\n",
      "        .   @param angle Ellipse rotation angle in degrees.\n",
      "        .   @param startAngle Starting angle of the elliptic arc in degrees.\n",
      "        .   @param endAngle Ending angle of the elliptic arc in degrees.\n",
      "        .   @param color Ellipse color.\n",
      "        .   @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that\n",
      "        .   a filled ellipse sector is to be drawn.\n",
      "        .   @param lineType Type of the ellipse boundary. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the coordinates of the center and values of axes.\n",
      "        \n",
      "        \n",
      "        \n",
      "        ellipse(img, box, color[, thickness[, lineType]]) -> img\n",
      "        .   @overload\n",
      "        .   @param img Image.\n",
      "        .   @param box Alternative ellipse representation via RotatedRect. This means that the function draws\n",
      "        .   an ellipse inscribed in the rotated rectangle.\n",
      "        .   @param color Ellipse color.\n",
      "        .   @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that\n",
      "        .   a filled ellipse sector is to be drawn.\n",
      "        .   @param lineType Type of the ellipse boundary. See #LineTypes\n",
      "    \n",
      "    ellipse2Poly(...)\n",
      "        ellipse2Poly(center, axes, angle, arcStart, arcEnd, delta) -> pts\n",
      "        .   @brief Approximates an elliptic arc with a polyline.\n",
      "        .   \n",
      "        .   The function ellipse2Poly computes the vertices of a polyline that approximates the specified\n",
      "        .   elliptic arc. It is used by #ellipse. If `arcStart` is greater than `arcEnd`, they are swapped.\n",
      "        .   \n",
      "        .   @param center Center of the arc.\n",
      "        .   @param axes Half of the size of the ellipse main axes. See #ellipse for details.\n",
      "        .   @param angle Rotation angle of the ellipse in degrees. See #ellipse for details.\n",
      "        .   @param arcStart Starting angle of the elliptic arc in degrees.\n",
      "        .   @param arcEnd Ending angle of the elliptic arc in degrees.\n",
      "        .   @param delta Angle between the subsequent polyline vertices. It defines the approximation\n",
      "        .   accuracy.\n",
      "        .   @param pts Output vector of polyline vertices.\n",
      "    \n",
      "    equalizeHist(...)\n",
      "        equalizeHist(src[, dst]) -> dst\n",
      "        .   @brief Equalizes the histogram of a grayscale image.\n",
      "        .   \n",
      "        .   The function equalizes the histogram of the input image using the following algorithm:\n",
      "        .   \n",
      "        .   - Calculate the histogram \\f$H\\f$ for src .\n",
      "        .   - Normalize the histogram so that the sum of histogram bins is 255.\n",
      "        .   - Compute the integral of the histogram:\n",
      "        .   \\f[H'_i =  \\sum _{0  \\le j < i} H(j)\\f]\n",
      "        .   - Transform the image using \\f$H'\\f$ as a look-up table: \\f$\\texttt{dst}(x,y) = H'(\\texttt{src}(x,y))\\f$\n",
      "        .   \n",
      "        .   The algorithm normalizes the brightness and increases the contrast of the image.\n",
      "        .   \n",
      "        .   @param src Source 8-bit single channel image.\n",
      "        .   @param dst Destination image of the same size and type as src .\n",
      "    \n",
      "    erode(...)\n",
      "        erode(src, kernel[, dst[, anchor[, iterations[, borderType[, borderValue]]]]]) -> dst\n",
      "        .   @brief Erodes an image by using a specific structuring element.\n",
      "        .   \n",
      "        .   The function erodes the source image using the specified structuring element that determines the\n",
      "        .   shape of a pixel neighborhood over which the minimum is taken:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\min _{(x',y'):  \\, \\texttt{element} (x',y') \\ne0 } \\texttt{src} (x+x',y+y')\\f]\n",
      "        .   \n",
      "        .   The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In\n",
      "        .   case of multi-channel images, each channel is processed independently.\n",
      "        .   \n",
      "        .   @param src input image; the number of channels can be arbitrary, but the depth should be one of\n",
      "        .   CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "        .   @param dst output image of the same size and type as src.\n",
      "        .   @param kernel structuring element used for erosion; if `element=Mat()`, a `3 x 3` rectangular\n",
      "        .   structuring element is used. Kernel can be created using #getStructuringElement.\n",
      "        .   @param anchor position of the anchor within the element; default value (-1, -1) means that the\n",
      "        .   anchor is at the element center.\n",
      "        .   @param iterations number of times erosion is applied.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   @param borderValue border value in case of a constant border\n",
      "        .   @sa  dilate, morphologyEx, getStructuringElement\n",
      "    \n",
      "    estimateAffine2D(...)\n",
      "        estimateAffine2D(from, to[, inliers[, method[, ransacReprojThreshold[, maxIters[, confidence[, refineIters]]]]]]) -> retval, inliers\n",
      "        .   @brief Computes an optimal affine transformation between two 2D point sets.\n",
      "        .   \n",
      "        .   It computes\n",
      "        .   \\f[\n",
      "        .   \\begin{bmatrix}\n",
      "        .   x\\\\\n",
      "        .   y\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   =\n",
      "        .   \\begin{bmatrix}\n",
      "        .   a_{11} & a_{12}\\\\\n",
      "        .   a_{21} & a_{22}\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\begin{bmatrix}\n",
      "        .   X\\\\\n",
      "        .   Y\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   +\n",
      "        .   \\begin{bmatrix}\n",
      "        .   b_1\\\\\n",
      "        .   b_2\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   @param from First input 2D point set containing \\f$(X,Y)\\f$.\n",
      "        .   @param to Second input 2D point set containing \\f$(x,y)\\f$.\n",
      "        .   @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n",
      "        .   @param method Robust method used to compute transformation. The following methods are possible:\n",
      "        .   -   cv::RANSAC - RANSAC-based robust method\n",
      "        .   -   cv::LMEDS - Least-Median robust method\n",
      "        .   RANSAC is the default method.\n",
      "        .   @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider\n",
      "        .   a point as an inlier. Applies only to RANSAC.\n",
      "        .   @param maxIters The maximum number of robust method iterations.\n",
      "        .   @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n",
      "        .   between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n",
      "        .   significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n",
      "        .   @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt).\n",
      "        .   Passing 0 will disable refining, so the output matrix will be output of robust method.\n",
      "        .   \n",
      "        .   @return Output 2D affine transformation matrix \\f$2 \\times 3\\f$ or empty matrix if transformation\n",
      "        .   could not be estimated. The returned matrix has the following form:\n",
      "        .   \\f[\n",
      "        .   \\begin{bmatrix}\n",
      "        .   a_{11} & a_{12} & b_1\\\\\n",
      "        .   a_{21} & a_{22} & b_2\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   The function estimates an optimal 2D affine transformation between two 2D point sets using the\n",
      "        .   selected robust algorithm.\n",
      "        .   \n",
      "        .   The computed transformation is then refined further (using only inliers) with the\n",
      "        .   Levenberg-Marquardt method to reduce the re-projection error even more.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   The RANSAC method can handle practically any ratio of outliers but needs a threshold to\n",
      "        .   distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n",
      "        .   correctly only when there are more than 50% of inliers.\n",
      "        .   \n",
      "        .   @sa estimateAffinePartial2D, getAffineTransform\n",
      "    \n",
      "    estimateAffine3D(...)\n",
      "        estimateAffine3D(src, dst[, out[, inliers[, ransacThreshold[, confidence]]]]) -> retval, out, inliers\n",
      "        .   @brief Computes an optimal affine transformation between two 3D point sets.\n",
      "        .   \n",
      "        .   It computes\n",
      "        .   \\f[\n",
      "        .   \\begin{bmatrix}\n",
      "        .   x\\\\\n",
      "        .   y\\\\\n",
      "        .   z\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   =\n",
      "        .   \\begin{bmatrix}\n",
      "        .   a_{11} & a_{12} & a_{13}\\\\\n",
      "        .   a_{21} & a_{22} & a_{23}\\\\\n",
      "        .   a_{31} & a_{32} & a_{33}\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\begin{bmatrix}\n",
      "        .   X\\\\\n",
      "        .   Y\\\\\n",
      "        .   Z\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   +\n",
      "        .   \\begin{bmatrix}\n",
      "        .   b_1\\\\\n",
      "        .   b_2\\\\\n",
      "        .   b_3\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   @param src First input 3D point set containing \\f$(X,Y,Z)\\f$.\n",
      "        .   @param dst Second input 3D point set containing \\f$(x,y,z)\\f$.\n",
      "        .   @param out Output 3D affine transformation matrix \\f$3 \\times 4\\f$ of the form\n",
      "        .   \\f[\n",
      "        .   \\begin{bmatrix}\n",
      "        .   a_{11} & a_{12} & a_{13} & b_1\\\\\n",
      "        .   a_{21} & a_{22} & a_{23} & b_2\\\\\n",
      "        .   a_{31} & a_{32} & a_{33} & b_3\\\\\n",
      "        .   \\end{bmatrix}\n",
      "        .   \\f]\n",
      "        .   @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).\n",
      "        .   @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as\n",
      "        .   an inlier.\n",
      "        .   @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n",
      "        .   between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n",
      "        .   significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n",
      "        .   \n",
      "        .   The function estimates an optimal 3D affine transformation between two 3D point sets using the\n",
      "        .   RANSAC algorithm.\n",
      "    \n",
      "    estimateAffinePartial2D(...)\n",
      "        estimateAffinePartial2D(from, to[, inliers[, method[, ransacReprojThreshold[, maxIters[, confidence[, refineIters]]]]]]) -> retval, inliers\n",
      "        .   @brief Computes an optimal limited affine transformation with 4 degrees of freedom between\n",
      "        .   two 2D point sets.\n",
      "        .   \n",
      "        .   @param from First input 2D point set.\n",
      "        .   @param to Second input 2D point set.\n",
      "        .   @param inliers Output vector indicating which points are inliers.\n",
      "        .   @param method Robust method used to compute transformation. The following methods are possible:\n",
      "        .   -   cv::RANSAC - RANSAC-based robust method\n",
      "        .   -   cv::LMEDS - Least-Median robust method\n",
      "        .   RANSAC is the default method.\n",
      "        .   @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider\n",
      "        .   a point as an inlier. Applies only to RANSAC.\n",
      "        .   @param maxIters The maximum number of robust method iterations.\n",
      "        .   @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything\n",
      "        .   between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation\n",
      "        .   significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.\n",
      "        .   @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt).\n",
      "        .   Passing 0 will disable refining, so the output matrix will be output of robust method.\n",
      "        .   \n",
      "        .   @return Output 2D affine transformation (4 degrees of freedom) matrix \\f$2 \\times 3\\f$ or\n",
      "        .   empty matrix if transformation could not be estimated.\n",
      "        .   \n",
      "        .   The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to\n",
      "        .   combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust\n",
      "        .   estimation.\n",
      "        .   \n",
      "        .   The computed transformation is then refined further (using only inliers) with the\n",
      "        .   Levenberg-Marquardt method to reduce the re-projection error even more.\n",
      "        .   \n",
      "        .   Estimated transformation matrix is:\n",
      "        .   \\f[ \\begin{bmatrix} \\cos(\\theta) \\cdot s & -\\sin(\\theta) \\cdot s & t_x \\\\\n",
      "        .                   \\sin(\\theta) \\cdot s & \\cos(\\theta) \\cdot s & t_y\n",
      "        .   \\end{bmatrix} \\f]\n",
      "        .   Where \\f$ \\theta \\f$ is the rotation angle, \\f$ s \\f$ the scaling factor and \\f$ t_x, t_y \\f$ are\n",
      "        .   translations in \\f$ x, y \\f$ axes respectively.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   The RANSAC method can handle practically any ratio of outliers but need a threshold to\n",
      "        .   distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n",
      "        .   correctly only when there are more than 50% of inliers.\n",
      "        .   \n",
      "        .   @sa estimateAffine2D, getAffineTransform\n",
      "    \n",
      "    exp(...)\n",
      "        exp(src[, dst]) -> dst\n",
      "        .   @brief Calculates the exponent of every array element.\n",
      "        .   \n",
      "        .   The function cv::exp calculates the exponent of every element of the input\n",
      "        .   array:\n",
      "        .   \\f[\\texttt{dst} [I] = e^{ src(I) }\\f]\n",
      "        .   \n",
      "        .   The maximum relative error is about 7e-6 for single-precision input and\n",
      "        .   less than 1e-10 for double-precision input. Currently, the function\n",
      "        .   converts denormalized values to zeros on output. Special values (NaN,\n",
      "        .   Inf) are not handled.\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @sa log , cartToPolar , polarToCart , phase , pow , sqrt , magnitude\n",
      "    \n",
      "    extractChannel(...)\n",
      "        extractChannel(src, coi[, dst]) -> dst\n",
      "        .   @brief Extracts a single channel from src (coi is 0-based index)\n",
      "        .   @param src input array\n",
      "        .   @param dst output array\n",
      "        .   @param coi index of channel to extract\n",
      "        .   @sa mixChannels, split\n",
      "    \n",
      "    fastAtan2(...)\n",
      "        fastAtan2(y, x) -> retval\n",
      "        .   @brief Calculates the angle of a 2D vector in degrees.\n",
      "        .   \n",
      "        .    The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured\n",
      "        .    in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.\n",
      "        .    @param x x-coordinate of the vector.\n",
      "        .    @param y y-coordinate of the vector.\n",
      "    \n",
      "    fastNlMeansDenoising(...)\n",
      "        fastNlMeansDenoising(src[, dst[, h[, templateWindowSize[, searchWindowSize]]]]) -> dst\n",
      "        .   @brief Perform image denoising using Non-local Means Denoising algorithm\n",
      "        .   <http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/> with several computational\n",
      "        .   optimizations. Noise expected to be a gaussian white noise\n",
      "        .   \n",
      "        .   @param src Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.\n",
      "        .   @param dst Output image with the same size and type as src .\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Parameter regulating filter strength. Big h value perfectly removes noise but also\n",
      "        .   removes image details, smaller h value preserves details but also preserves some noise\n",
      "        .   \n",
      "        .   This function expected to be applied to grayscale images. For colored images look at\n",
      "        .   fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored\n",
      "        .   image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting\n",
      "        .   image to CIELAB colorspace and then separately denoise L and AB components with different h\n",
      "        .   parameter.\n",
      "        \n",
      "        \n",
      "        \n",
      "        fastNlMeansDenoising(src, h[, dst[, templateWindowSize[, searchWindowSize[, normType]]]]) -> dst\n",
      "        .   @brief Perform image denoising using Non-local Means Denoising algorithm\n",
      "        .   <http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/> with several computational\n",
      "        .   optimizations. Noise expected to be a gaussian white noise\n",
      "        .   \n",
      "        .   @param src Input 8-bit or 16-bit (only with NORM_L1) 1-channel,\n",
      "        .   2-channel, 3-channel or 4-channel image.\n",
      "        .   @param dst Output image with the same size and type as src .\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Array of parameters regulating filter strength, either one\n",
      "        .   parameter applied to all channels or one per channel in dst. Big h value\n",
      "        .   perfectly removes noise but also removes image details, smaller h\n",
      "        .   value preserves details but also preserves some noise\n",
      "        .   @param normType Type of norm used for weight calculation. Can be either NORM_L2 or NORM_L1\n",
      "        .   \n",
      "        .   This function expected to be applied to grayscale images. For colored images look at\n",
      "        .   fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored\n",
      "        .   image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting\n",
      "        .   image to CIELAB colorspace and then separately denoise L and AB components with different h\n",
      "        .   parameter.\n",
      "    \n",
      "    fastNlMeansDenoisingColored(...)\n",
      "        fastNlMeansDenoisingColored(src[, dst[, h[, hColor[, templateWindowSize[, searchWindowSize]]]]]) -> dst\n",
      "        .   @brief Modification of fastNlMeansDenoising function for colored images\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src .\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly\n",
      "        .   removes noise but also removes image details, smaller h value preserves details but also preserves\n",
      "        .   some noise\n",
      "        .   @param hColor The same as h but for color components. For most images value equals 10\n",
      "        .   will be enough to remove colored noise and do not distort colors\n",
      "        .   \n",
      "        .   The function converts image to CIELAB colorspace and then separately denoise L and AB components\n",
      "        .   with given h parameters using fastNlMeansDenoising function.\n",
      "    \n",
      "    fastNlMeansDenoisingColoredMulti(...)\n",
      "        fastNlMeansDenoisingColoredMulti(srcImgs, imgToDenoiseIndex, temporalWindowSize[, dst[, h[, hColor[, templateWindowSize[, searchWindowSize]]]]]) -> dst\n",
      "        .   @brief Modification of fastNlMeansDenoisingMulti function for colored images sequences\n",
      "        .   \n",
      "        .   @param srcImgs Input 8-bit 3-channel images sequence. All images should have the same type and\n",
      "        .   size.\n",
      "        .   @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence\n",
      "        .   @param temporalWindowSize Number of surrounding images to use for target image denoising. Should\n",
      "        .   be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to\n",
      "        .   imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise\n",
      "        .   srcImgs[imgToDenoiseIndex] image.\n",
      "        .   @param dst Output image with the same size and type as srcImgs images.\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Parameter regulating filter strength for luminance component. Bigger h value perfectly\n",
      "        .   removes noise but also removes image details, smaller h value preserves details but also preserves\n",
      "        .   some noise.\n",
      "        .   @param hColor The same as h but for color components.\n",
      "        .   \n",
      "        .   The function converts images to CIELAB colorspace and then separately denoise L and AB components\n",
      "        .   with given h parameters using fastNlMeansDenoisingMulti function.\n",
      "    \n",
      "    fastNlMeansDenoisingMulti(...)\n",
      "        fastNlMeansDenoisingMulti(srcImgs, imgToDenoiseIndex, temporalWindowSize[, dst[, h[, templateWindowSize[, searchWindowSize]]]]) -> dst\n",
      "        .   @brief Modification of fastNlMeansDenoising function for images sequence where consecutive images have been\n",
      "        .   captured in small period of time. For example video. This version of the function is for grayscale\n",
      "        .   images or for manual manipulation with colorspaces. For more details see\n",
      "        .   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394>\n",
      "        .   \n",
      "        .   @param srcImgs Input 8-bit 1-channel, 2-channel, 3-channel or\n",
      "        .   4-channel images sequence. All images should have the same type and\n",
      "        .   size.\n",
      "        .   @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence\n",
      "        .   @param temporalWindowSize Number of surrounding images to use for target image denoising. Should\n",
      "        .   be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to\n",
      "        .   imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise\n",
      "        .   srcImgs[imgToDenoiseIndex] image.\n",
      "        .   @param dst Output image with the same size and type as srcImgs images.\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Parameter regulating filter strength. Bigger h value\n",
      "        .   perfectly removes noise but also removes image details, smaller h\n",
      "        .   value preserves details but also preserves some noise\n",
      "        \n",
      "        \n",
      "        \n",
      "        fastNlMeansDenoisingMulti(srcImgs, imgToDenoiseIndex, temporalWindowSize, h[, dst[, templateWindowSize[, searchWindowSize[, normType]]]]) -> dst\n",
      "        .   @brief Modification of fastNlMeansDenoising function for images sequence where consecutive images have been\n",
      "        .   captured in small period of time. For example video. This version of the function is for grayscale\n",
      "        .   images or for manual manipulation with colorspaces. For more details see\n",
      "        .   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394>\n",
      "        .   \n",
      "        .   @param srcImgs Input 8-bit or 16-bit (only with NORM_L1) 1-channel,\n",
      "        .   2-channel, 3-channel or 4-channel images sequence. All images should\n",
      "        .   have the same type and size.\n",
      "        .   @param imgToDenoiseIndex Target image to denoise index in srcImgs sequence\n",
      "        .   @param temporalWindowSize Number of surrounding images to use for target image denoising. Should\n",
      "        .   be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to\n",
      "        .   imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise\n",
      "        .   srcImgs[imgToDenoiseIndex] image.\n",
      "        .   @param dst Output image with the same size and type as srcImgs images.\n",
      "        .   @param templateWindowSize Size in pixels of the template patch that is used to compute weights.\n",
      "        .   Should be odd. Recommended value 7 pixels\n",
      "        .   @param searchWindowSize Size in pixels of the window that is used to compute weighted average for\n",
      "        .   given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater\n",
      "        .   denoising time. Recommended value 21 pixels\n",
      "        .   @param h Array of parameters regulating filter strength, either one\n",
      "        .   parameter applied to all channels or one per channel in dst. Big h value\n",
      "        .   perfectly removes noise but also removes image details, smaller h\n",
      "        .   value preserves details but also preserves some noise\n",
      "        .   @param normType Type of norm used for weight calculation. Can be either NORM_L2 or NORM_L1\n",
      "    \n",
      "    fillConvexPoly(...)\n",
      "        fillConvexPoly(img, points, color[, lineType[, shift]]) -> img\n",
      "        .   @brief Fills a convex polygon.\n",
      "        .   \n",
      "        .   The function cv::fillConvexPoly draws a filled convex polygon. This function is much faster than the\n",
      "        .   function #fillPoly . It can fill not only convex polygons but any monotonic polygon without\n",
      "        .   self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)\n",
      "        .   twice at the most (though, its top-most and/or the bottom edge could be horizontal).\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param points Polygon vertices.\n",
      "        .   @param color Polygon color.\n",
      "        .   @param lineType Type of the polygon boundaries. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the vertex coordinates.\n",
      "    \n",
      "    fillPoly(...)\n",
      "        fillPoly(img, pts, color[, lineType[, shift[, offset]]]) -> img\n",
      "        .   @brief Fills the area bounded by one or more polygons.\n",
      "        .   \n",
      "        .   The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill\n",
      "        .   complex areas, for example, areas with holes, contours with self-intersections (some of their\n",
      "        .   parts), and so forth.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param pts Array of polygons where each polygon is represented as an array of points.\n",
      "        .   @param color Polygon color.\n",
      "        .   @param lineType Type of the polygon boundaries. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the vertex coordinates.\n",
      "        .   @param offset Optional offset of all points of the contours.\n",
      "    \n",
      "    filter2D(...)\n",
      "        filter2D(src, ddepth, kernel[, dst[, anchor[, delta[, borderType]]]]) -> dst\n",
      "        .   @brief Convolves an image with the kernel.\n",
      "        .   \n",
      "        .   The function applies an arbitrary linear filter to an image. In-place operation is supported. When\n",
      "        .   the aperture is partially outside the image, the function interpolates outlier pixel values\n",
      "        .   according to the specified border mode.\n",
      "        .   \n",
      "        .   The function does actually compute correlation, not the convolution:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\sum _{ \\stackrel{0\\leq x' < \\texttt{kernel.cols},}{0\\leq y' < \\texttt{kernel.rows}} }  \\texttt{kernel} (x',y')* \\texttt{src} (x+x'- \\texttt{anchor.x} ,y+y'- \\texttt{anchor.y} )\\f]\n",
      "        .   \n",
      "        .   That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip\n",
      "        .   the kernel using #flip and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -\n",
      "        .   anchor.y - 1)`.\n",
      "        .   \n",
      "        .   The function uses the DFT-based algorithm in case of sufficiently large kernels (~`11 x 11` or\n",
      "        .   larger) and the direct algorithm for small kernels.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image of the same size and the same number of channels as src.\n",
      "        .   @param ddepth desired depth of the destination image, see @ref filter_depths \"combinations\"\n",
      "        .   @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point\n",
      "        .   matrix; if you want to apply different kernels to different channels, split the image into\n",
      "        .   separate color planes using split and process them individually.\n",
      "        .   @param anchor anchor of the kernel that indicates the relative position of a filtered point within\n",
      "        .   the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor\n",
      "        .   is at the kernel center.\n",
      "        .   @param delta optional value added to the filtered pixels before storing them in dst.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   @sa  sepFilter2D, dft, matchTemplate\n",
      "    \n",
      "    filterHomographyDecompByVisibleRefpoints(...)\n",
      "        filterHomographyDecompByVisibleRefpoints(rotations, normals, beforePoints, afterPoints[, possibleSolutions[, pointsMask]]) -> possibleSolutions\n",
      "        .   @brief Filters homography decompositions based on additional information.\n",
      "        .   \n",
      "        .   @param rotations Vector of rotation matrices.\n",
      "        .   @param normals Vector of plane normal matrices.\n",
      "        .   @param beforePoints Vector of (rectified) visible reference points before the homography is applied\n",
      "        .   @param afterPoints Vector of (rectified) visible reference points after the homography is applied\n",
      "        .   @param possibleSolutions Vector of int indices representing the viable solution set after filtering\n",
      "        .   @param pointsMask optional Mat/Vector of 8u type representing the mask for the inliers as given by the findHomography function\n",
      "        .   \n",
      "        .   This function is intended to filter the output of the decomposeHomographyMat based on additional\n",
      "        .   information as described in @cite Malis . The summary of the method: the decomposeHomographyMat function\n",
      "        .   returns 2 unique solutions and their \"opposites\" for a total of 4 solutions. If we have access to the\n",
      "        .   sets of points visible in the camera frame before and after the homography transformation is applied,\n",
      "        .   we can determine which are the true potential solutions and which are the opposites by verifying which\n",
      "        .   homographies are consistent with all visible reference points being in front of the camera. The inputs\n",
      "        .   are left unchanged; the filtered solution set is returned as indices into the existing one.\n",
      "    \n",
      "    filterSpeckles(...)\n",
      "        filterSpeckles(img, newVal, maxSpeckleSize, maxDiff[, buf]) -> img, buf\n",
      "        .   @brief Filters off small noise blobs (speckles) in the disparity map\n",
      "        .   \n",
      "        .   @param img The input 16-bit signed disparity image\n",
      "        .   @param newVal The disparity value used to paint-off the speckles\n",
      "        .   @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not\n",
      "        .   affected by the algorithm\n",
      "        .   @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same\n",
      "        .   blob. Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point\n",
      "        .   disparity map, where disparity values are multiplied by 16, this scale factor should be taken into\n",
      "        .   account when specifying this parameter value.\n",
      "        .   @param buf The optional temporary buffer to avoid memory allocation within the function.\n",
      "    \n",
      "    find4QuadCornerSubpix(...)\n",
      "        find4QuadCornerSubpix(img, corners, region_size) -> retval, corners\n",
      "        .\n",
      "    \n",
      "    findChessboardCorners(...)\n",
      "        findChessboardCorners(image, patternSize[, corners[, flags]]) -> retval, corners\n",
      "        .   @brief Finds the positions of internal corners of the chessboard.\n",
      "        .   \n",
      "        .   @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n",
      "        .   @param patternSize Number of inner corners per a chessboard row and column\n",
      "        .   ( patternSize = cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n",
      "        .   @param corners Output array of detected corners.\n",
      "        .   @param flags Various operation flags that can be zero or a combination of the following values:\n",
      "        .   -   **CALIB_CB_ADAPTIVE_THRESH** Use adaptive thresholding to convert the image to black\n",
      "        .   and white, rather than a fixed threshold level (computed from the average image brightness).\n",
      "        .   -   **CALIB_CB_NORMALIZE_IMAGE** Normalize the image gamma with equalizeHist before\n",
      "        .   applying fixed or adaptive thresholding.\n",
      "        .   -   **CALIB_CB_FILTER_QUADS** Use additional criteria (like contour area, perimeter,\n",
      "        .   square-like shape) to filter out false quads extracted at the contour retrieval stage.\n",
      "        .   -   **CALIB_CB_FAST_CHECK** Run a fast check on the image that looks for chessboard corners,\n",
      "        .   and shortcut the call if none is found. This can drastically speed up the call in the\n",
      "        .   degenerate condition when no chessboard is observed.\n",
      "        .   \n",
      "        .   The function attempts to determine whether the input image is a view of the chessboard pattern and\n",
      "        .   locate the internal chessboard corners. The function returns a non-zero value if all of the corners\n",
      "        .   are found and they are placed in a certain order (row by row, left to right in every row).\n",
      "        .   Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,\n",
      "        .   a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black\n",
      "        .   squares touch each other. The detected coordinates are approximate, and to determine their positions\n",
      "        .   more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with\n",
      "        .   different parameters if returned coordinates are not accurate enough.\n",
      "        .   \n",
      "        .   Sample usage of detecting and drawing chessboard corners: :\n",
      "        .   @code\n",
      "        .       Size patternsize(8,6); //interior number of corners\n",
      "        .       Mat gray = ....; //source image\n",
      "        .       vector<Point2f> corners; //this will be filled by the detected corners\n",
      "        .   \n",
      "        .       //CALIB_CB_FAST_CHECK saves a lot of time on images\n",
      "        .       //that do not contain any chessboard corners\n",
      "        .       bool patternfound = findChessboardCorners(gray, patternsize, corners,\n",
      "        .               CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE\n",
      "        .               + CALIB_CB_FAST_CHECK);\n",
      "        .   \n",
      "        .       if(patternfound)\n",
      "        .         cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),\n",
      "        .           TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));\n",
      "        .   \n",
      "        .       drawChessboardCorners(img, patternsize, Mat(corners), patternfound);\n",
      "        .   @endcode\n",
      "        .   @note The function requires white space (like a square-thick border, the wider the better) around\n",
      "        .   the board to make the detection more robust in various environments. Otherwise, if there is no\n",
      "        .   border and the background is dark, the outer black squares cannot be segmented properly and so the\n",
      "        .   square grouping and ordering algorithm fails.\n",
      "    \n",
      "    findChessboardCornersSB(...)\n",
      "        findChessboardCornersSB(image, patternSize[, corners[, flags]]) -> retval, corners\n",
      "        .   @brief Finds the positions of internal corners of the chessboard using a sector based approach.\n",
      "        .   \n",
      "        .   @param image Source chessboard view. It must be an 8-bit grayscale or color image.\n",
      "        .   @param patternSize Number of inner corners per a chessboard row and column\n",
      "        .   ( patternSize = cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).\n",
      "        .   @param corners Output array of detected corners.\n",
      "        .   @param flags Various operation flags that can be zero or a combination of the following values:\n",
      "        .   -   **CALIB_CB_NORMALIZE_IMAGE** Normalize the image gamma with equalizeHist before detection.\n",
      "        .   -   **CALIB_CB_EXHAUSTIVE** Run an exhaustive search to improve detection rate.\n",
      "        .   -   **CALIB_CB_ACCURACY** Up sample input image to improve sub-pixel accuracy due to aliasing effects.\n",
      "        .   This should be used if an accurate camera calibration is required.\n",
      "        .   \n",
      "        .   The function is analog to findchessboardCorners but uses a localized radon\n",
      "        .   transformation approximated by box filters being more robust to all sort of\n",
      "        .   noise, faster on larger images and is able to directly return the sub-pixel\n",
      "        .   position of the internal chessboard corners. The Method is based on the paper\n",
      "        .   @cite duda2018 \"Accurate Detection and Localization of Checkerboard Corners for\n",
      "        .   Calibration\" demonstrating that the returned sub-pixel positions are more\n",
      "        .   accurate than the one returned by cornerSubPix allowing a precise camera\n",
      "        .   calibration for demanding applications.\n",
      "        .   \n",
      "        .   @note The function requires a white boarder with roughly the same width as one\n",
      "        .   of the checkerboard fields around the whole board to improve the detection in\n",
      "        .   various environments. In addition, because of the localized radon\n",
      "        .   transformation it is beneficial to use round corners for the field corners\n",
      "        .   which are located on the outside of the board. The following figure illustrates\n",
      "        .   a sample checkerboard optimized for the detection. However, any other checkerboard\n",
      "        .   can be used as well.\n",
      "        .   ![Checkerboard](pics/checkerboard_radon.png)\n",
      "    \n",
      "    findCirclesGrid(...)\n",
      "        findCirclesGrid(image, patternSize, flags, blobDetector, parameters[, centers]) -> retval, centers\n",
      "        .   @brief Finds centers in the grid of circles.\n",
      "        .   \n",
      "        .   @param image grid view of input circles; it must be an 8-bit grayscale or color image.\n",
      "        .   @param patternSize number of circles per row and column\n",
      "        .   ( patternSize = Size(points_per_row, points_per_colum) ).\n",
      "        .   @param centers output array of detected centers.\n",
      "        .   @param flags various operation flags that can be one of the following values:\n",
      "        .   -   **CALIB_CB_SYMMETRIC_GRID** uses symmetric pattern of circles.\n",
      "        .   -   **CALIB_CB_ASYMMETRIC_GRID** uses asymmetric pattern of circles.\n",
      "        .   -   **CALIB_CB_CLUSTERING** uses a special algorithm for grid detection. It is more robust to\n",
      "        .   perspective distortions but much more sensitive to background clutter.\n",
      "        .   @param blobDetector feature detector that finds blobs like dark circles on light background.\n",
      "        .   @param parameters struct for finding circles in a grid pattern.\n",
      "        .   \n",
      "        .   The function attempts to determine whether the input image contains a grid of circles. If it is, the\n",
      "        .   function locates centers of the circles. The function returns a non-zero value if all of the centers\n",
      "        .   have been found and they have been placed in a certain order (row by row, left to right in every\n",
      "        .   row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0.\n",
      "        .   \n",
      "        .   Sample usage of detecting and drawing the centers of circles: :\n",
      "        .   @code\n",
      "        .       Size patternsize(7,7); //number of centers\n",
      "        .       Mat gray = ....; //source image\n",
      "        .       vector<Point2f> centers; //this will be filled by the detected centers\n",
      "        .   \n",
      "        .       bool patternfound = findCirclesGrid(gray, patternsize, centers);\n",
      "        .   \n",
      "        .       drawChessboardCorners(img, patternsize, Mat(centers), patternfound);\n",
      "        .   @endcode\n",
      "        .   @note The function requires white space (like a square-thick border, the wider the better) around\n",
      "        .   the board to make the detection more robust in various environments.\n",
      "        \n",
      "        \n",
      "        \n",
      "        findCirclesGrid(image, patternSize[, centers[, flags[, blobDetector]]]) -> retval, centers\n",
      "        .   @overload\n",
      "    \n",
      "    findContours(...)\n",
      "        findContours(image, mode, method[, contours[, hierarchy[, offset]]]) -> contours, hierarchy\n",
      "        .   @brief Finds contours in a binary image.\n",
      "        .   \n",
      "        .   The function retrieves contours from the binary image using the algorithm @cite Suzuki85 . The contours\n",
      "        .   are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the\n",
      "        .   OpenCV sample directory.\n",
      "        .   @note Since opencv 3.2 source image is not modified by this function.\n",
      "        .   \n",
      "        .   @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero\n",
      "        .   pixels remain 0's, so the image is treated as binary . You can use #compare, #inRange, #threshold ,\n",
      "        .   #adaptiveThreshold, #Canny, and others to create a binary image out of a grayscale or color one.\n",
      "        .   If mode equals to #RETR_CCOMP or #RETR_FLOODFILL, the input can also be a 32-bit integer image of labels (CV_32SC1).\n",
      "        .   @param contours Detected contours. Each contour is stored as a vector of points (e.g.\n",
      "        .   std::vector<std::vector<cv::Point> >).\n",
      "        .   @param hierarchy Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about the image topology. It has\n",
      "        .   as many elements as the number of contours. For each i-th contour contours[i], the elements\n",
      "        .   hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3] are set to 0-based indices\n",
      "        .   in contours of the next and previous contours at the same hierarchical level, the first child\n",
      "        .   contour and the parent contour, respectively. If for the contour i there are no next, previous,\n",
      "        .   parent, or nested contours, the corresponding elements of hierarchy[i] will be negative.\n",
      "        .   @param mode Contour retrieval mode, see #RetrievalModes\n",
      "        .   @param method Contour approximation method, see #ContourApproximationModes\n",
      "        .   @param offset Optional offset by which every contour point is shifted. This is useful if the\n",
      "        .   contours are extracted from the image ROI and then they should be analyzed in the whole image\n",
      "        .   context.\n",
      "    \n",
      "    findEssentialMat(...)\n",
      "        findEssentialMat(points1, points2, cameraMatrix[, method[, prob[, threshold[, mask]]]]) -> retval, mask\n",
      "        .   @brief Calculates an essential matrix from the corresponding points in two images.\n",
      "        .   \n",
      "        .   @param points1 Array of N (N \\>= 5) 2D points from the first image. The point coordinates should\n",
      "        .   be floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1 .\n",
      "        .   @param cameraMatrix Camera matrix \\f$K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   Note that this function assumes that points1 and points2 are feature points from cameras with the\n",
      "        .   same camera matrix.\n",
      "        .   @param method Method for computing an essential matrix.\n",
      "        .   -   **RANSAC** for the RANSAC algorithm.\n",
      "        .   -   **LMEDS** for the LMedS algorithm.\n",
      "        .   @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n",
      "        .   confidence (probability) that the estimated matrix is correct.\n",
      "        .   @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n",
      "        .   line in pixels, beyond which the point is considered an outlier and is not used for computing the\n",
      "        .   final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n",
      "        .   point localization, image resolution, and the image noise.\n",
      "        .   @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1\n",
      "        .   for the other points. The array is computed only in the RANSAC and LMedS methods.\n",
      "        .   \n",
      "        .   This function estimates essential matrix based on the five-point algorithm solver in @cite Nister03 .\n",
      "        .   @cite SteweniusCFS is also a related. The epipolar geometry is described by the following equation:\n",
      "        .   \n",
      "        .   \\f[[p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\\f]\n",
      "        .   \n",
      "        .   where \\f$E\\f$ is an essential matrix, \\f$p_1\\f$ and \\f$p_2\\f$ are corresponding points in the first and the\n",
      "        .   second images, respectively. The result of this function may be passed further to\n",
      "        .   decomposeEssentialMat or recoverPose to recover the relative pose between cameras.\n",
      "        \n",
      "        \n",
      "        \n",
      "        findEssentialMat(points1, points2[, focal[, pp[, method[, prob[, threshold[, mask]]]]]]) -> retval, mask\n",
      "        .   @overload\n",
      "        .   @param points1 Array of N (N \\>= 5) 2D points from the first image. The point coordinates should\n",
      "        .   be floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1 .\n",
      "        .   @param focal focal length of the camera. Note that this function assumes that points1 and points2\n",
      "        .   are feature points from cameras with same focal length and principal point.\n",
      "        .   @param pp principal point of the camera.\n",
      "        .   @param method Method for computing a fundamental matrix.\n",
      "        .   -   **RANSAC** for the RANSAC algorithm.\n",
      "        .   -   **LMEDS** for the LMedS algorithm.\n",
      "        .   @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar\n",
      "        .   line in pixels, beyond which the point is considered an outlier and is not used for computing the\n",
      "        .   final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n",
      "        .   point localization, image resolution, and the image noise.\n",
      "        .   @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of\n",
      "        .   confidence (probability) that the estimated matrix is correct.\n",
      "        .   @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1\n",
      "        .   for the other points. The array is computed only in the RANSAC and LMedS methods.\n",
      "        .   \n",
      "        .   This function differs from the one above that it computes camera matrix from focal length and\n",
      "        .   principal point:\n",
      "        .   \n",
      "        .   \\f[K =\n",
      "        .   \\begin{bmatrix}\n",
      "        .   f & 0 & x_{pp}  \\\\\n",
      "        .   0 & f & y_{pp}  \\\\\n",
      "        .   0 & 0 & 1\n",
      "        .   \\end{bmatrix}\\f]\n",
      "    \n",
      "    findFundamentalMat(...)\n",
      "        findFundamentalMat(points1, points2[, method[, ransacReprojThreshold[, confidence[, mask]]]]) -> retval, mask\n",
      "        .   @brief Calculates a fundamental matrix from the corresponding points in two images.\n",
      "        .   \n",
      "        .   @param points1 Array of N points from the first image. The point coordinates should be\n",
      "        .   floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1 .\n",
      "        .   @param method Method for computing a fundamental matrix.\n",
      "        .   -   **CV_FM_7POINT** for a 7-point algorithm. \\f$N = 7\\f$\n",
      "        .   -   **CV_FM_8POINT** for an 8-point algorithm. \\f$N \\ge 8\\f$\n",
      "        .   -   **CV_FM_RANSAC** for the RANSAC algorithm. \\f$N \\ge 8\\f$\n",
      "        .   -   **CV_FM_LMEDS** for the LMedS algorithm. \\f$N \\ge 8\\f$\n",
      "        .   @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point to an epipolar\n",
      "        .   line in pixels, beyond which the point is considered an outlier and is not used for computing the\n",
      "        .   final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the\n",
      "        .   point localization, image resolution, and the image noise.\n",
      "        .   @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable level\n",
      "        .   of confidence (probability) that the estimated matrix is correct.\n",
      "        .   @param mask\n",
      "        .   \n",
      "        .   The epipolar geometry is described by the following equation:\n",
      "        .   \n",
      "        .   \\f[[p_2; 1]^T F [p_1; 1] = 0\\f]\n",
      "        .   \n",
      "        .   where \\f$F\\f$ is a fundamental matrix, \\f$p_1\\f$ and \\f$p_2\\f$ are corresponding points in the first and the\n",
      "        .   second images, respectively.\n",
      "        .   \n",
      "        .   The function calculates the fundamental matrix using one of four methods listed above and returns\n",
      "        .   the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point\n",
      "        .   algorithm, the function may return up to 3 solutions ( \\f$9 \\times 3\\f$ matrix that stores all 3\n",
      "        .   matrices sequentially).\n",
      "        .   \n",
      "        .   The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the\n",
      "        .   epipolar lines corresponding to the specified points. It can also be passed to\n",
      "        .   stereoRectifyUncalibrated to compute the rectification transformation. :\n",
      "        .   @code\n",
      "        .       // Example. Estimation of fundamental matrix using the RANSAC algorithm\n",
      "        .       int point_count = 100;\n",
      "        .       vector<Point2f> points1(point_count);\n",
      "        .       vector<Point2f> points2(point_count);\n",
      "        .   \n",
      "        .       // initialize the points here ...\n",
      "        .       for( int i = 0; i < point_count; i++ )\n",
      "        .       {\n",
      "        .           points1[i] = ...;\n",
      "        .           points2[i] = ...;\n",
      "        .       }\n",
      "        .   \n",
      "        .       Mat fundamental_matrix =\n",
      "        .        findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);\n",
      "        .   @endcode\n",
      "    \n",
      "    findHomography(...)\n",
      "        findHomography(srcPoints, dstPoints[, method[, ransacReprojThreshold[, mask[, maxIters[, confidence]]]]]) -> retval, mask\n",
      "        .   @brief Finds a perspective transformation between two planes.\n",
      "        .   \n",
      "        .   @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2\n",
      "        .   or vector\\<Point2f\\> .\n",
      "        .   @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or\n",
      "        .   a vector\\<Point2f\\> .\n",
      "        .   @param method Method used to compute a homography matrix. The following methods are possible:\n",
      "        .   -   **0** - a regular method using all the points, i.e., the least squares method\n",
      "        .   -   **RANSAC** - RANSAC-based robust method\n",
      "        .   -   **LMEDS** - Least-Median robust method\n",
      "        .   -   **RHO** - PROSAC-based robust method\n",
      "        .   @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier\n",
      "        .   (used in the RANSAC and RHO methods only). That is, if\n",
      "        .   \\f[\\| \\texttt{dstPoints} _i -  \\texttt{convertPointsHomogeneous} ( \\texttt{H} * \\texttt{srcPoints} _i) \\|_2  >  \\texttt{ransacReprojThreshold}\\f]\n",
      "        .   then the point \\f$i\\f$ is considered as an outlier. If srcPoints and dstPoints are measured in pixels,\n",
      "        .   it usually makes sense to set this parameter somewhere in the range of 1 to 10.\n",
      "        .   @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input\n",
      "        .   mask values are ignored.\n",
      "        .   @param maxIters The maximum number of RANSAC iterations.\n",
      "        .   @param confidence Confidence level, between 0 and 1.\n",
      "        .   \n",
      "        .   The function finds and returns the perspective transformation \\f$H\\f$ between the source and the\n",
      "        .   destination planes:\n",
      "        .   \n",
      "        .   \\f[s_i  \\vecthree{x'_i}{y'_i}{1} \\sim H  \\vecthree{x_i}{y_i}{1}\\f]\n",
      "        .   \n",
      "        .   so that the back-projection error\n",
      "        .   \n",
      "        .   \\f[\\sum _i \\left ( x'_i- \\frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \\right )^2+ \\left ( y'_i- \\frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \\right )^2\\f]\n",
      "        .   \n",
      "        .   is minimized. If the parameter method is set to the default value 0, the function uses all the point\n",
      "        .   pairs to compute an initial homography estimate with a simple least-squares scheme.\n",
      "        .   \n",
      "        .   However, if not all of the point pairs ( \\f$srcPoints_i\\f$, \\f$dstPoints_i\\f$ ) fit the rigid perspective\n",
      "        .   transformation (that is, there are some outliers), this initial estimate will be poor. In this case,\n",
      "        .   you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different\n",
      "        .   random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix\n",
      "        .   using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the\n",
      "        .   computed homography (which is the number of inliers for RANSAC or the least median re-projection error for\n",
      "        .   LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and\n",
      "        .   the mask of inliers/outliers.\n",
      "        .   \n",
      "        .   Regardless of the method, robust or not, the computed homography matrix is refined further (using\n",
      "        .   inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the\n",
      "        .   re-projection error even more.\n",
      "        .   \n",
      "        .   The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to\n",
      "        .   distinguish inliers from outliers. The method LMeDS does not need any threshold but it works\n",
      "        .   correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the\n",
      "        .   noise is rather small, use the default method (method=0).\n",
      "        .   \n",
      "        .   The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is\n",
      "        .   determined up to a scale. Thus, it is normalized so that \\f$h_{33}=1\\f$. Note that whenever an \\f$H\\f$ matrix\n",
      "        .   cannot be estimated, an empty one will be returned.\n",
      "        .   \n",
      "        .   @sa\n",
      "        .   getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective,\n",
      "        .   perspectiveTransform\n",
      "    \n",
      "    findNonZero(...)\n",
      "        findNonZero(src[, idx]) -> idx\n",
      "        .   @brief Returns the list of locations of non-zero pixels\n",
      "        .   \n",
      "        .   Given a binary matrix (likely returned from an operation such\n",
      "        .   as threshold(), compare(), >, ==, etc, return all of\n",
      "        .   the non-zero indices as a cv::Mat or std::vector<cv::Point> (x,y)\n",
      "        .   For example:\n",
      "        .   @code{.cpp}\n",
      "        .       cv::Mat binaryImage; // input, binary image\n",
      "        .       cv::Mat locations;   // output, locations of non-zero pixels\n",
      "        .       cv::findNonZero(binaryImage, locations);\n",
      "        .   \n",
      "        .       // access pixel coordinates\n",
      "        .       Point pnt = locations.at<Point>(i);\n",
      "        .   @endcode\n",
      "        .   or\n",
      "        .   @code{.cpp}\n",
      "        .       cv::Mat binaryImage; // input, binary image\n",
      "        .       vector<Point> locations;   // output, locations of non-zero pixels\n",
      "        .       cv::findNonZero(binaryImage, locations);\n",
      "        .   \n",
      "        .       // access pixel coordinates\n",
      "        .       Point pnt = locations[i];\n",
      "        .   @endcode\n",
      "        .   @param src single-channel array\n",
      "        .   @param idx the output array, type of cv::Mat or std::vector<Point>, corresponding to non-zero indices in the input\n",
      "    \n",
      "    findTransformECC(...)\n",
      "        findTransformECC(templateImage, inputImage, warpMatrix, motionType, criteria, inputMask, gaussFiltSize) -> retval, warpMatrix\n",
      "        .   @brief Finds the geometric transform (warp) between two images in terms of the ECC criterion @cite EP08 .\n",
      "        .   \n",
      "        .   @param templateImage single-channel template image; CV_8U or CV_32F array.\n",
      "        .   @param inputImage single-channel input image which should be warped with the final warpMatrix in\n",
      "        .   order to provide an image similar to templateImage, same type as templateImage.\n",
      "        .   @param warpMatrix floating-point \\f$2\\times 3\\f$ or \\f$3\\times 3\\f$ mapping matrix (warp).\n",
      "        .   @param motionType parameter, specifying the type of motion:\n",
      "        .    -   **MOTION_TRANSLATION** sets a translational motion model; warpMatrix is \\f$2\\times 3\\f$ with\n",
      "        .        the first \\f$2\\times 2\\f$ part being the unity matrix and the rest two parameters being\n",
      "        .        estimated.\n",
      "        .    -   **MOTION_EUCLIDEAN** sets a Euclidean (rigid) transformation as motion model; three\n",
      "        .        parameters are estimated; warpMatrix is \\f$2\\times 3\\f$.\n",
      "        .    -   **MOTION_AFFINE** sets an affine motion model (DEFAULT); six parameters are estimated;\n",
      "        .        warpMatrix is \\f$2\\times 3\\f$.\n",
      "        .    -   **MOTION_HOMOGRAPHY** sets a homography as a motion model; eight parameters are\n",
      "        .        estimated;\\`warpMatrix\\` is \\f$3\\times 3\\f$.\n",
      "        .   @param criteria parameter, specifying the termination criteria of the ECC algorithm;\n",
      "        .   criteria.epsilon defines the threshold of the increment in the correlation coefficient between two\n",
      "        .   iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion).\n",
      "        .   Default values are shown in the declaration above.\n",
      "        .   @param inputMask An optional mask to indicate valid values of inputImage.\n",
      "        .   @param gaussFiltSize An optional value indicating size of gaussian blur filter; (DEFAULT: 5)\n",
      "        .   \n",
      "        .   The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion\n",
      "        .   (@cite EP08), that is\n",
      "        .   \n",
      "        .   \\f[\\texttt{warpMatrix} = \\texttt{warpMatrix} = \\arg\\max_{W} \\texttt{ECC}(\\texttt{templateImage}(x,y),\\texttt{inputImage}(x',y'))\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \n",
      "        .   \\f[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = W \\cdot \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   (the equation holds with homogeneous coordinates for homography). It returns the final enhanced\n",
      "        .   correlation coefficient, that is the correlation coefficient between the template image and the\n",
      "        .   final warped input image. When a \\f$3\\times 3\\f$ matrix is given with motionType =0, 1 or 2, the third\n",
      "        .   row is ignored.\n",
      "        .   \n",
      "        .   Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an\n",
      "        .   area-based alignment that builds on intensity similarities. In essence, the function updates the\n",
      "        .   initial transformation that roughly aligns the images. If this information is missing, the identity\n",
      "        .   warp (unity matrix) is used as an initialization. Note that if images undergo strong\n",
      "        .   displacements/rotations, an initial transformation that roughly aligns the images is necessary\n",
      "        .   (e.g., a simple euclidean/similarity transform that allows for the images showing the same image\n",
      "        .   content approximately). Use inverse warping in the second image to take an image close to the first\n",
      "        .   one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV\n",
      "        .   sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws\n",
      "        .   an exception if algorithm does not converges.\n",
      "        .   \n",
      "        .   @sa\n",
      "        .   computeECC, estimateAffine2D, estimateAffinePartial2D, findHomography\n",
      "    \n",
      "    fitEllipse(...)\n",
      "        fitEllipse(points) -> retval\n",
      "        .   @brief Fits an ellipse around a set of 2D points.\n",
      "        .   \n",
      "        .   The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of\n",
      "        .   all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm described by @cite Fitzgibbon95\n",
      "        .   is used. Developer should keep in mind that it is possible that the returned\n",
      "        .   ellipse/rotatedRect data contains negative indices, due to the data points being close to the\n",
      "        .   border of the containing Mat element.\n",
      "        .   \n",
      "        .   @param points Input 2D point set, stored in std::vector\\<\\> or Mat\n",
      "    \n",
      "    fitEllipseAMS(...)\n",
      "        fitEllipseAMS(points) -> retval\n",
      "        .   @brief Fits an ellipse around a set of 2D points.\n",
      "        .   \n",
      "        .    The function calculates the ellipse that fits a set of 2D points.\n",
      "        .    It returns the rotated rectangle in which the ellipse is inscribed.\n",
      "        .    The Approximate Mean Square (AMS) proposed by @cite Taubin1991 is used.\n",
      "        .   \n",
      "        .    For an ellipse, this basis set is \\f$ \\chi= \\left(x^2, x y, y^2, x, y, 1\\right) \\f$,\n",
      "        .    which is a set of six free coefficients \\f$ A^T=\\left\\{A_{\\text{xx}},A_{\\text{xy}},A_{\\text{yy}},A_x,A_y,A_0\\right\\} \\f$.\n",
      "        .    However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \\f$ (a,b) \\f$,\n",
      "        .    the position \\f$ (x_0,y_0) \\f$, and the orientation \\f$ \\theta \\f$. This is because the basis set includes lines,\n",
      "        .    quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits.\n",
      "        .    If the fit is found to be a parabolic or hyperbolic function then the standard #fitEllipse method is used.\n",
      "        .    The AMS method restricts the fit to parabolic, hyperbolic and elliptical curves\n",
      "        .    by imposing the condition that \\f$ A^T ( D_x^T D_x  +   D_y^T D_y) A = 1 \\f$ where\n",
      "        .    the matrices \\f$ Dx \\f$ and \\f$ Dy \\f$ are the partial derivatives of the design matrix \\f$ D \\f$ with\n",
      "        .    respect to x and y. The matrices are formed row by row applying the following to\n",
      "        .    each of the points in the set:\n",
      "        .    \\f{align*}{\n",
      "        .    D(i,:)&=\\left\\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\\right\\} &\n",
      "        .    D_x(i,:)&=\\left\\{2 x_i,y_i,0,1,0,0\\right\\} &\n",
      "        .    D_y(i,:)&=\\left\\{0,x_i,2 y_i,0,1,0\\right\\}\n",
      "        .    \\f}\n",
      "        .    The AMS method minimizes the cost function\n",
      "        .    \\f{equation*}{\n",
      "        .    \\epsilon ^2=\\frac{ A^T D^T D A }{ A^T (D_x^T D_x +  D_y^T D_y) A^T }\n",
      "        .    \\f}\n",
      "        .   \n",
      "        .    The minimum cost is found by solving the generalized eigenvalue problem.\n",
      "        .   \n",
      "        .    \\f{equation*}{\n",
      "        .    D^T D A = \\lambda  \\left( D_x^T D_x +  D_y^T D_y\\right) A\n",
      "        .    \\f}\n",
      "        .   \n",
      "        .    @param points Input 2D point set, stored in std::vector\\<\\> or Mat\n",
      "    \n",
      "    fitEllipseDirect(...)\n",
      "        fitEllipseDirect(points) -> retval\n",
      "        .   @brief Fits an ellipse around a set of 2D points.\n",
      "        .   \n",
      "        .    The function calculates the ellipse that fits a set of 2D points.\n",
      "        .    It returns the rotated rectangle in which the ellipse is inscribed.\n",
      "        .    The Direct least square (Direct) method by @cite Fitzgibbon1999 is used.\n",
      "        .   \n",
      "        .    For an ellipse, this basis set is \\f$ \\chi= \\left(x^2, x y, y^2, x, y, 1\\right) \\f$,\n",
      "        .    which is a set of six free coefficients \\f$ A^T=\\left\\{A_{\\text{xx}},A_{\\text{xy}},A_{\\text{yy}},A_x,A_y,A_0\\right\\} \\f$.\n",
      "        .    However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \\f$ (a,b) \\f$,\n",
      "        .    the position \\f$ (x_0,y_0) \\f$, and the orientation \\f$ \\theta \\f$. This is because the basis set includes lines,\n",
      "        .    quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits.\n",
      "        .    The Direct method confines the fit to ellipses by ensuring that \\f$ 4 A_{xx} A_{yy}- A_{xy}^2 > 0 \\f$.\n",
      "        .    The condition imposed is that \\f$ 4 A_{xx} A_{yy}- A_{xy}^2=1 \\f$ which satisfies the inequality\n",
      "        .    and as the coefficients can be arbitrarily scaled is not overly restrictive.\n",
      "        .   \n",
      "        .    \\f{equation*}{\n",
      "        .    \\epsilon ^2= A^T D^T D A \\quad \\text{with} \\quad A^T C A =1 \\quad \\text{and} \\quad C=\\left(\\begin{matrix}\n",
      "        .    0 & 0  & 2  & 0  & 0  &  0  \\\\\n",
      "        .    0 & -1  & 0  & 0  & 0  &  0 \\\\\n",
      "        .    2 & 0  & 0  & 0  & 0  &  0 \\\\\n",
      "        .    0 & 0  & 0  & 0  & 0  &  0 \\\\\n",
      "        .    0 & 0  & 0  & 0  & 0  &  0 \\\\\n",
      "        .    0 & 0  & 0  & 0  & 0  &  0\n",
      "        .    \\end{matrix} \\right)\n",
      "        .    \\f}\n",
      "        .   \n",
      "        .    The minimum cost is found by solving the generalized eigenvalue problem.\n",
      "        .   \n",
      "        .    \\f{equation*}{\n",
      "        .    D^T D A = \\lambda  \\left( C\\right) A\n",
      "        .    \\f}\n",
      "        .   \n",
      "        .    The system produces only one positive eigenvalue \\f$ \\lambda\\f$ which is chosen as the solution\n",
      "        .    with its eigenvector \\f$\\mathbf{u}\\f$. These are used to find the coefficients\n",
      "        .   \n",
      "        .    \\f{equation*}{\n",
      "        .    A = \\sqrt{\\frac{1}{\\mathbf{u}^T C \\mathbf{u}}}  \\mathbf{u}\n",
      "        .    \\f}\n",
      "        .    The scaling factor guarantees that  \\f$A^T C A =1\\f$.\n",
      "        .   \n",
      "        .    @param points Input 2D point set, stored in std::vector\\<\\> or Mat\n",
      "    \n",
      "    fitLine(...)\n",
      "        fitLine(points, distType, param, reps, aeps[, line]) -> line\n",
      "        .   @brief Fits a line to a 2D or 3D point set.\n",
      "        .   \n",
      "        .   The function fitLine fits a line to a 2D or 3D point set by minimizing \\f$\\sum_i \\rho(r_i)\\f$ where\n",
      "        .   \\f$r_i\\f$ is a distance between the \\f$i^{th}\\f$ point, the line and \\f$\\rho(r)\\f$ is a distance function, one\n",
      "        .   of the following:\n",
      "        .   -  DIST_L2\n",
      "        .   \\f[\\rho (r) = r^2/2  \\quad \\text{(the simplest and the fastest least-squares method)}\\f]\n",
      "        .   - DIST_L1\n",
      "        .   \\f[\\rho (r) = r\\f]\n",
      "        .   - DIST_L12\n",
      "        .   \\f[\\rho (r) = 2  \\cdot ( \\sqrt{1 + \\frac{r^2}{2}} - 1)\\f]\n",
      "        .   - DIST_FAIR\n",
      "        .   \\f[\\rho \\left (r \\right ) = C^2  \\cdot \\left (  \\frac{r}{C} -  \\log{\\left(1 + \\frac{r}{C}\\right)} \\right )  \\quad \\text{where} \\quad C=1.3998\\f]\n",
      "        .   - DIST_WELSCH\n",
      "        .   \\f[\\rho \\left (r \\right ) =  \\frac{C^2}{2} \\cdot \\left ( 1 -  \\exp{\\left(-\\left(\\frac{r}{C}\\right)^2\\right)} \\right )  \\quad \\text{where} \\quad C=2.9846\\f]\n",
      "        .   - DIST_HUBER\n",
      "        .   \\f[\\rho (r) =  \\fork{r^2/2}{if \\(r < C\\)}{C \\cdot (r-C/2)}{otherwise} \\quad \\text{where} \\quad C=1.345\\f]\n",
      "        .   \n",
      "        .   The algorithm is based on the M-estimator ( <http://en.wikipedia.org/wiki/M-estimator> ) technique\n",
      "        .   that iteratively fits the line using the weighted least-squares algorithm. After each iteration the\n",
      "        .   weights \\f$w_i\\f$ are adjusted to be inversely proportional to \\f$\\rho(r_i)\\f$ .\n",
      "        .   \n",
      "        .   @param points Input vector of 2D or 3D points, stored in std::vector\\<\\> or Mat.\n",
      "        .   @param line Output line parameters. In case of 2D fitting, it should be a vector of 4 elements\n",
      "        .   (like Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and\n",
      "        .   (x0, y0) is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like\n",
      "        .   Vec6f) - (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line\n",
      "        .   and (x0, y0, z0) is a point on the line.\n",
      "        .   @param distType Distance used by the M-estimator, see #DistanceTypes\n",
      "        .   @param param Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value\n",
      "        .   is chosen.\n",
      "        .   @param reps Sufficient accuracy for the radius (distance between the coordinate origin and the line).\n",
      "        .   @param aeps Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.\n",
      "    \n",
      "    flip(...)\n",
      "        flip(src, flipCode[, dst]) -> dst\n",
      "        .   @brief Flips a 2D array around vertical, horizontal, or both axes.\n",
      "        .   \n",
      "        .   The function cv::flip flips the array in one of three different ways (row\n",
      "        .   and column indices are 0-based):\n",
      "        .   \\f[\\texttt{dst} _{ij} =\n",
      "        .   \\left\\{\n",
      "        .   \\begin{array}{l l}\n",
      "        .   \\texttt{src} _{\\texttt{src.rows}-i-1,j} & if\\;  \\texttt{flipCode} = 0 \\\\\n",
      "        .   \\texttt{src} _{i, \\texttt{src.cols} -j-1} & if\\;  \\texttt{flipCode} > 0 \\\\\n",
      "        .   \\texttt{src} _{ \\texttt{src.rows} -i-1, \\texttt{src.cols} -j-1} & if\\; \\texttt{flipCode} < 0 \\\\\n",
      "        .   \\end{array}\n",
      "        .   \\right.\\f]\n",
      "        .   The example scenarios of using the function are the following:\n",
      "        .   *   Vertical flipping of the image (flipCode == 0) to switch between\n",
      "        .       top-left and bottom-left image origin. This is a typical operation\n",
      "        .       in video processing on Microsoft Windows\\* OS.\n",
      "        .   *   Horizontal flipping of the image with the subsequent horizontal\n",
      "        .       shift and absolute difference calculation to check for a\n",
      "        .       vertical-axis symmetry (flipCode \\> 0).\n",
      "        .   *   Simultaneous horizontal and vertical flipping of the image with\n",
      "        .       the subsequent shift and absolute difference calculation to check\n",
      "        .       for a central symmetry (flipCode \\< 0).\n",
      "        .   *   Reversing the order of point arrays (flipCode \\> 0 or\n",
      "        .       flipCode == 0).\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @param flipCode a flag to specify how to flip the array; 0 means\n",
      "        .   flipping around the x-axis and positive value (for example, 1) means\n",
      "        .   flipping around y-axis. Negative value (for example, -1) means flipping\n",
      "        .   around both axes.\n",
      "        .   @sa transpose , repeat , completeSymm\n",
      "    \n",
      "    floodFill(...)\n",
      "        floodFill(image, mask, seedPoint, newVal[, loDiff[, upDiff[, flags]]]) -> retval, image, mask, rect\n",
      "        .   @brief Fills a connected component with the given color.\n",
      "        .   \n",
      "        .   The function cv::floodFill fills a connected component starting from the seed point with the specified\n",
      "        .   color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The\n",
      "        .   pixel at \\f$(x,y)\\f$ is considered to belong to the repainted domain if:\n",
      "        .   \n",
      "        .   - in case of a grayscale image and floating range\n",
      "        .   \\f[\\texttt{src} (x',y')- \\texttt{loDiff} \\leq \\texttt{src} (x,y)  \\leq \\texttt{src} (x',y')+ \\texttt{upDiff}\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   - in case of a grayscale image and fixed range\n",
      "        .   \\f[\\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)- \\texttt{loDiff} \\leq \\texttt{src} (x,y)  \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)+ \\texttt{upDiff}\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   - in case of a color image and floating range\n",
      "        .   \\f[\\texttt{src} (x',y')_r- \\texttt{loDiff} _r \\leq \\texttt{src} (x,y)_r \\leq \\texttt{src} (x',y')_r+ \\texttt{upDiff} _r,\\f]\n",
      "        .   \\f[\\texttt{src} (x',y')_g- \\texttt{loDiff} _g \\leq \\texttt{src} (x,y)_g \\leq \\texttt{src} (x',y')_g+ \\texttt{upDiff} _g\\f]\n",
      "        .   and\n",
      "        .   \\f[\\texttt{src} (x',y')_b- \\texttt{loDiff} _b \\leq \\texttt{src} (x,y)_b \\leq \\texttt{src} (x',y')_b+ \\texttt{upDiff} _b\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   - in case of a color image and fixed range\n",
      "        .   \\f[\\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_r- \\texttt{loDiff} _r \\leq \\texttt{src} (x,y)_r \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_r+ \\texttt{upDiff} _r,\\f]\n",
      "        .   \\f[\\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_g- \\texttt{loDiff} _g \\leq \\texttt{src} (x,y)_g \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_g+ \\texttt{upDiff} _g\\f]\n",
      "        .   and\n",
      "        .   \\f[\\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_b- \\texttt{loDiff} _b \\leq \\texttt{src} (x,y)_b \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_b+ \\texttt{upDiff} _b\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   where \\f$src(x',y')\\f$ is the value of one of pixel neighbors that is already known to belong to the\n",
      "        .   component. That is, to be added to the connected component, a color/brightness of the pixel should\n",
      "        .   be close enough to:\n",
      "        .   - Color/brightness of one of its neighbors that already belong to the connected component in case\n",
      "        .   of a floating range.\n",
      "        .   - Color/brightness of the seed point in case of a fixed range.\n",
      "        .   \n",
      "        .   Use these functions to either mark a connected component with the specified color in-place, or build\n",
      "        .   a mask and then extract the contour, or copy the region to another image, and so on.\n",
      "        .   \n",
      "        .   @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the\n",
      "        .   function unless the #FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See\n",
      "        .   the details below.\n",
      "        .   @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels\n",
      "        .   taller than image. Since this is both an input and output parameter, you must take responsibility\n",
      "        .   of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example,\n",
      "        .   an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the\n",
      "        .   mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags\n",
      "        .   as described below. Additionally, the function fills the border of the mask with ones to simplify\n",
      "        .   internal processing. It is therefore possible to use the same mask in multiple calls to the function\n",
      "        .   to make sure the filled areas do not overlap.\n",
      "        .   @param seedPoint Starting point.\n",
      "        .   @param newVal New value of the repainted domain pixels.\n",
      "        .   @param loDiff Maximal lower brightness/color difference between the currently observed pixel and\n",
      "        .   one of its neighbors belonging to the component, or a seed pixel being added to the component.\n",
      "        .   @param upDiff Maximal upper brightness/color difference between the currently observed pixel and\n",
      "        .   one of its neighbors belonging to the component, or a seed pixel being added to the component.\n",
      "        .   @param rect Optional output parameter set by the function to the minimum bounding rectangle of the\n",
      "        .   repainted domain.\n",
      "        .   @param flags Operation flags. The first 8 bits contain a connectivity value. The default value of\n",
      "        .   4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A\n",
      "        .   connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)\n",
      "        .   will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill\n",
      "        .   the mask (the default value is 1). For example, 4 | ( 255 \\<\\< 8 ) will consider 4 nearest\n",
      "        .   neighbours and fill the mask with a value of 255. The following additional options occupy higher\n",
      "        .   bits and therefore may be further combined with the connectivity and mask fill values using\n",
      "        .   bit-wise or (|), see #FloodFillFlags.\n",
      "        .   \n",
      "        .   @note Since the mask is larger than the filled image, a pixel \\f$(x, y)\\f$ in image corresponds to the\n",
      "        .   pixel \\f$(x+1, y+1)\\f$ in the mask .\n",
      "        .   \n",
      "        .   @sa findContours\n",
      "    \n",
      "    gemm(...)\n",
      "        gemm(src1, src2, alpha, src3, beta[, dst[, flags]]) -> dst\n",
      "        .   @brief Performs generalized matrix multiplication.\n",
      "        .   \n",
      "        .   The function cv::gemm performs generalized matrix multiplication similar to the\n",
      "        .   gemm functions in BLAS level 3. For example,\n",
      "        .   `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)`\n",
      "        .   corresponds to\n",
      "        .   \\f[\\texttt{dst} =  \\texttt{alpha} \\cdot \\texttt{src1} ^T  \\cdot \\texttt{src2} +  \\texttt{beta} \\cdot \\texttt{src3} ^T\\f]\n",
      "        .   \n",
      "        .   In case of complex (two-channel) data, performed a complex matrix\n",
      "        .   multiplication.\n",
      "        .   \n",
      "        .   The function can be replaced with a matrix expression. For example, the\n",
      "        .   above call can be replaced with:\n",
      "        .   @code{.cpp}\n",
      "        .       dst = alpha*src1.t()*src2 + beta*src3.t();\n",
      "        .   @endcode\n",
      "        .   @param src1 first multiplied input matrix that could be real(CV_32FC1,\n",
      "        .   CV_64FC1) or complex(CV_32FC2, CV_64FC2).\n",
      "        .   @param src2 second multiplied input matrix of the same type as src1.\n",
      "        .   @param alpha weight of the matrix product.\n",
      "        .   @param src3 third optional delta matrix added to the matrix product; it\n",
      "        .   should have the same type as src1 and src2.\n",
      "        .   @param beta weight of src3.\n",
      "        .   @param dst output matrix; it has the proper size and the same type as\n",
      "        .   input matrices.\n",
      "        .   @param flags operation flags (cv::GemmFlags)\n",
      "        .   @sa mulTransposed , transform\n",
      "    \n",
      "    getAffineTransform(...)\n",
      "        getAffineTransform(src, dst) -> retval\n",
      "        .   @overload\n",
      "    \n",
      "    getBuildInformation(...)\n",
      "        getBuildInformation() -> retval\n",
      "        .   @brief Returns full configuration time cmake output.\n",
      "        .   \n",
      "        .   Returned value is raw cmake output including version control system revision, compiler version,\n",
      "        .   compiler flags, enabled modules and third party libraries, etc. Output format depends on target\n",
      "        .   architecture.\n",
      "    \n",
      "    getCPUTickCount(...)\n",
      "        getCPUTickCount() -> retval\n",
      "        .   @brief Returns the number of CPU ticks.\n",
      "        .   \n",
      "        .   The function returns the current number of CPU ticks on some architectures (such as x86, x64,\n",
      "        .   PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for\n",
      "        .   very accurate time measurements, as well as for RNG initialization. Note that in case of multi-CPU\n",
      "        .   systems a thread, from which getCPUTickCount is called, can be suspended and resumed at another CPU\n",
      "        .   with its own counter. So, theoretically (and practically) the subsequent calls to the function do\n",
      "        .   not necessary return the monotonously increasing values. Also, since a modern CPU varies the CPU\n",
      "        .   frequency depending on the load, the number of CPU clocks spent in some code cannot be directly\n",
      "        .   converted to time units. Therefore, getTickCount is generally a preferable solution for measuring\n",
      "        .   execution time.\n",
      "    \n",
      "    getDefaultNewCameraMatrix(...)\n",
      "        getDefaultNewCameraMatrix(cameraMatrix[, imgsize[, centerPrincipalPoint]]) -> retval\n",
      "        .   @brief Returns the default new camera matrix.\n",
      "        .   \n",
      "        .   The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when\n",
      "        .   centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).\n",
      "        .   \n",
      "        .   In the latter case, the new camera matrix will be:\n",
      "        .   \n",
      "        .   \\f[\\begin{bmatrix} f_x && 0 && ( \\texttt{imgSize.width} -1)*0.5  \\\\ 0 && f_y && ( \\texttt{imgSize.height} -1)*0.5  \\\\ 0 && 0 && 1 \\end{bmatrix} ,\\f]\n",
      "        .   \n",
      "        .   where \\f$f_x\\f$ and \\f$f_y\\f$ are \\f$(0,0)\\f$ and \\f$(1,1)\\f$ elements of cameraMatrix, respectively.\n",
      "        .   \n",
      "        .   By default, the undistortion functions in OpenCV (see #initUndistortRectifyMap, #undistort) do not\n",
      "        .   move the principal point. However, when you work with stereo, it is important to move the principal\n",
      "        .   points in both views to the same y-coordinate (which is required by most of stereo correspondence\n",
      "        .   algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for\n",
      "        .   each view where the principal points are located at the center.\n",
      "        .   \n",
      "        .   @param cameraMatrix Input camera matrix.\n",
      "        .   @param imgsize Camera view image size in pixels.\n",
      "        .   @param centerPrincipalPoint Location of the principal point in the new camera matrix. The\n",
      "        .   parameter indicates whether this location should be at the image center or not.\n",
      "    \n",
      "    getDerivKernels(...)\n",
      "        getDerivKernels(dx, dy, ksize[, kx[, ky[, normalize[, ktype]]]]) -> kx, ky\n",
      "        .   @brief Returns filter coefficients for computing spatial image derivatives.\n",
      "        .   \n",
      "        .   The function computes and returns the filter coefficients for spatial image derivatives. When\n",
      "        .   `ksize=FILTER_SCHARR`, the Scharr \\f$3 \\times 3\\f$ kernels are generated (see #Scharr). Otherwise, Sobel\n",
      "        .   kernels are generated (see #Sobel). The filters are normally passed to #sepFilter2D or to\n",
      "        .   \n",
      "        .   @param kx Output matrix of row filter coefficients. It has the type ktype .\n",
      "        .   @param ky Output matrix of column filter coefficients. It has the type ktype .\n",
      "        .   @param dx Derivative order in respect of x.\n",
      "        .   @param dy Derivative order in respect of y.\n",
      "        .   @param ksize Aperture size. It can be FILTER_SCHARR, 1, 3, 5, or 7.\n",
      "        .   @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.\n",
      "        .   Theoretically, the coefficients should have the denominator \\f$=2^{ksize*2-dx-dy-2}\\f$. If you are\n",
      "        .   going to filter floating-point images, you are likely to use the normalized kernels. But if you\n",
      "        .   compute derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve\n",
      "        .   all the fractional bits, you may want to set normalize=false .\n",
      "        .   @param ktype Type of filter coefficients. It can be CV_32f or CV_64F .\n",
      "    \n",
      "    getFontScaleFromHeight(...)\n",
      "        getFontScaleFromHeight(fontFace, pixelHeight[, thickness]) -> retval\n",
      "        .   @brief Calculates the font-specific size to use to achieve a given height in pixels.\n",
      "        .   \n",
      "        .   @param fontFace Font to use, see cv::HersheyFonts.\n",
      "        .   @param pixelHeight Pixel height to compute the fontScale for\n",
      "        .   @param thickness Thickness of lines used to render the text.See putText for details.\n",
      "        .   @return The fontSize to use for cv::putText\n",
      "        .   \n",
      "        .   @see cv::putText\n",
      "    \n",
      "    getGaborKernel(...)\n",
      "        getGaborKernel(ksize, sigma, theta, lambd, gamma[, psi[, ktype]]) -> retval\n",
      "        .   @brief Returns Gabor filter coefficients.\n",
      "        .   \n",
      "        .   For more details about gabor filter equations and parameters, see: [Gabor\n",
      "        .   Filter](http://en.wikipedia.org/wiki/Gabor_filter).\n",
      "        .   \n",
      "        .   @param ksize Size of the filter returned.\n",
      "        .   @param sigma Standard deviation of the gaussian envelope.\n",
      "        .   @param theta Orientation of the normal to the parallel stripes of a Gabor function.\n",
      "        .   @param lambd Wavelength of the sinusoidal factor.\n",
      "        .   @param gamma Spatial aspect ratio.\n",
      "        .   @param psi Phase offset.\n",
      "        .   @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n",
      "    \n",
      "    getGaussianKernel(...)\n",
      "        getGaussianKernel(ksize, sigma[, ktype]) -> retval\n",
      "        .   @brief Returns Gaussian filter coefficients.\n",
      "        .   \n",
      "        .   The function computes and returns the \\f$\\texttt{ksize} \\times 1\\f$ matrix of Gaussian filter\n",
      "        .   coefficients:\n",
      "        .   \n",
      "        .   \\f[G_i= \\alpha *e^{-(i-( \\texttt{ksize} -1)/2)^2/(2* \\texttt{sigma}^2)},\\f]\n",
      "        .   \n",
      "        .   where \\f$i=0..\\texttt{ksize}-1\\f$ and \\f$\\alpha\\f$ is the scale factor chosen so that \\f$\\sum_i G_i=1\\f$.\n",
      "        .   \n",
      "        .   Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize\n",
      "        .   smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.\n",
      "        .   You may also use the higher-level GaussianBlur.\n",
      "        .   @param ksize Aperture size. It should be odd ( \\f$\\texttt{ksize} \\mod 2 = 1\\f$ ) and positive.\n",
      "        .   @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as\n",
      "        .   `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`.\n",
      "        .   @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .\n",
      "        .   @sa  sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur\n",
      "    \n",
      "    getHardwareFeatureName(...)\n",
      "        getHardwareFeatureName(feature) -> retval\n",
      "        .   @brief Returns feature name by ID\n",
      "        .   \n",
      "        .   Returns empty string if feature is not defined\n",
      "    \n",
      "    getNumThreads(...)\n",
      "        getNumThreads() -> retval\n",
      "        .   @brief Returns the number of threads used by OpenCV for parallel regions.\n",
      "        .   \n",
      "        .   Always returns 1 if OpenCV is built without threading support.\n",
      "        .   \n",
      "        .   The exact meaning of return value depends on the threading framework used by OpenCV library:\n",
      "        .   - `TBB` - The number of threads, that OpenCV will try to use for parallel regions. If there is\n",
      "        .     any tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns\n",
      "        .     default number of threads used by TBB library.\n",
      "        .   - `OpenMP` - An upper bound on the number of threads that could be used to form a new team.\n",
      "        .   - `Concurrency` - The number of threads, that OpenCV will try to use for parallel regions.\n",
      "        .   - `GCD` - Unsupported; returns the GCD thread pool limit (512) for compatibility.\n",
      "        .   - `C=` - The number of threads, that OpenCV will try to use for parallel regions, if before\n",
      "        .     called setNumThreads with threads \\> 0, otherwise returns the number of logical CPUs,\n",
      "        .     available for the process.\n",
      "        .   @sa setNumThreads, getThreadNum\n",
      "    \n",
      "    getNumberOfCPUs(...)\n",
      "        getNumberOfCPUs() -> retval\n",
      "        .   @brief Returns the number of logical CPUs available for the process.\n",
      "    \n",
      "    getOptimalDFTSize(...)\n",
      "        getOptimalDFTSize(vecsize) -> retval\n",
      "        .   @brief Returns the optimal DFT size for a given vector size.\n",
      "        .   \n",
      "        .   DFT performance is not a monotonic function of a vector size. Therefore, when you calculate\n",
      "        .   convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to\n",
      "        .   pad the input data with zeros to get a bit larger array that can be transformed much faster than the\n",
      "        .   original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process.\n",
      "        .   Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5\\*5\\*3\\*2\\*2)\n",
      "        .   are also processed quite efficiently.\n",
      "        .   \n",
      "        .   The function cv::getOptimalDFTSize returns the minimum number N that is greater than or equal to vecsize\n",
      "        .   so that the DFT of a vector of size N can be processed efficiently. In the current implementation N\n",
      "        .   = 2 ^p^ \\* 3 ^q^ \\* 5 ^r^ for some integer p, q, r.\n",
      "        .   \n",
      "        .   The function returns a negative number if vecsize is too large (very close to INT_MAX ).\n",
      "        .   \n",
      "        .   While the function cannot be used directly to estimate the optimal vector size for DCT transform\n",
      "        .   (since the current DCT implementation supports only even-size vectors), it can be easily processed\n",
      "        .   as getOptimalDFTSize((vecsize+1)/2)\\*2.\n",
      "        .   @param vecsize vector size.\n",
      "        .   @sa dft , dct , idft , idct , mulSpectrums\n",
      "    \n",
      "    getOptimalNewCameraMatrix(...)\n",
      "        getOptimalNewCameraMatrix(cameraMatrix, distCoeffs, imageSize, alpha[, newImgSize[, centerPrincipalPoint]]) -> retval, validPixROI\n",
      "        .   @brief Returns the new camera matrix based on the free scaling parameter.\n",
      "        .   \n",
      "        .   @param cameraMatrix Input camera matrix.\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param imageSize Original image size.\n",
      "        .   @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are\n",
      "        .   valid) and 1 (when all the source image pixels are retained in the undistorted image). See\n",
      "        .   stereoRectify for details.\n",
      "        .   @param newImgSize Image size after rectification. By default, it is set to imageSize .\n",
      "        .   @param validPixROI Optional output rectangle that outlines all-good-pixels region in the\n",
      "        .   undistorted image. See roi1, roi2 description in stereoRectify .\n",
      "        .   @param centerPrincipalPoint Optional flag that indicates whether in the new camera matrix the\n",
      "        .   principal point should be at the image center or not. By default, the principal point is chosen to\n",
      "        .   best fit a subset of the source image (determined by alpha) to the corrected image.\n",
      "        .   @return new_camera_matrix Output new camera matrix.\n",
      "        .   \n",
      "        .   The function computes and returns the optimal new camera matrix based on the free scaling parameter.\n",
      "        .   By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original\n",
      "        .   image pixels if there is valuable information in the corners alpha=1 , or get something in between.\n",
      "        .   When alpha\\>0 , the undistorted result is likely to have some black pixels corresponding to\n",
      "        .   \"virtual\" pixels outside of the captured distorted image. The original camera matrix, distortion\n",
      "        .   coefficients, the computed new camera matrix, and newImageSize should be passed to\n",
      "        .   initUndistortRectifyMap to produce the maps for remap .\n",
      "    \n",
      "    getPerspectiveTransform(...)\n",
      "        getPerspectiveTransform(src, dst[, solveMethod]) -> retval\n",
      "        .   @brief Calculates a perspective transform from four pairs of the corresponding points.\n",
      "        .   \n",
      "        .   The function calculates the \\f$3 \\times 3\\f$ matrix of a perspective transform so that:\n",
      "        .   \n",
      "        .   \\f[\\begin{bmatrix} t_i x'_i \\\\ t_i y'_i \\\\ t_i \\end{bmatrix} = \\texttt{map_matrix} \\cdot \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \n",
      "        .   \\f[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\\f]\n",
      "        .   \n",
      "        .   @param src Coordinates of quadrangle vertices in the source image.\n",
      "        .   @param dst Coordinates of the corresponding quadrangle vertices in the destination image.\n",
      "        .   @param solveMethod method passed to cv::solve (#DecompTypes)\n",
      "        .   \n",
      "        .   @sa  findHomography, warpPerspective, perspectiveTransform\n",
      "    \n",
      "    getRectSubPix(...)\n",
      "        getRectSubPix(image, patchSize, center[, patch[, patchType]]) -> patch\n",
      "        .   @brief Retrieves a pixel rectangle from an image with sub-pixel accuracy.\n",
      "        .   \n",
      "        .   The function getRectSubPix extracts pixels from src:\n",
      "        .   \n",
      "        .   \\f[patch(x, y) = src(x +  \\texttt{center.x} - ( \\texttt{dst.cols} -1)*0.5, y +  \\texttt{center.y} - ( \\texttt{dst.rows} -1)*0.5)\\f]\n",
      "        .   \n",
      "        .   where the values of the pixels at non-integer coordinates are retrieved using bilinear\n",
      "        .   interpolation. Every channel of multi-channel images is processed independently. Also\n",
      "        .   the image should be a single channel or three channel image. While the center of the\n",
      "        .   rectangle must be inside the image, parts of the rectangle may be outside.\n",
      "        .   \n",
      "        .   @param image Source image.\n",
      "        .   @param patchSize Size of the extracted patch.\n",
      "        .   @param center Floating point coordinates of the center of the extracted rectangle within the\n",
      "        .   source image. The center must be inside the image.\n",
      "        .   @param patch Extracted patch that has the size patchSize and the same number of channels as src .\n",
      "        .   @param patchType Depth of the extracted pixels. By default, they have the same depth as src .\n",
      "        .   \n",
      "        .   @sa  warpAffine, warpPerspective\n",
      "    \n",
      "    getRotationMatrix2D(...)\n",
      "        getRotationMatrix2D(center, angle, scale) -> retval\n",
      "        .   @brief Calculates an affine matrix of 2D rotation.\n",
      "        .   \n",
      "        .   The function calculates the following matrix:\n",
      "        .   \n",
      "        .   \\f[\\begin{bmatrix} \\alpha &  \\beta & (1- \\alpha )  \\cdot \\texttt{center.x} -  \\beta \\cdot \\texttt{center.y} \\\\ - \\beta &  \\alpha &  \\beta \\cdot \\texttt{center.x} + (1- \\alpha )  \\cdot \\texttt{center.y} \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} \\alpha =  \\texttt{scale} \\cdot \\cos \\texttt{angle} , \\\\ \\beta =  \\texttt{scale} \\cdot \\sin \\texttt{angle} \\end{array}\\f]\n",
      "        .   \n",
      "        .   The transformation maps the rotation center to itself. If this is not the target, adjust the shift.\n",
      "        .   \n",
      "        .   @param center Center of the rotation in the source image.\n",
      "        .   @param angle Rotation angle in degrees. Positive values mean counter-clockwise rotation (the\n",
      "        .   coordinate origin is assumed to be the top-left corner).\n",
      "        .   @param scale Isotropic scale factor.\n",
      "        .   \n",
      "        .   @sa  getAffineTransform, warpAffine, transform\n",
      "    \n",
      "    getStructuringElement(...)\n",
      "        getStructuringElement(shape, ksize[, anchor]) -> retval\n",
      "        .   @brief Returns a structuring element of the specified size and shape for morphological operations.\n",
      "        .   \n",
      "        .   The function constructs and returns the structuring element that can be further passed to #erode,\n",
      "        .   #dilate or #morphologyEx. But you can also construct an arbitrary binary mask yourself and use it as\n",
      "        .   the structuring element.\n",
      "        .   \n",
      "        .   @param shape Element shape that could be one of #MorphShapes\n",
      "        .   @param ksize Size of the structuring element.\n",
      "        .   @param anchor Anchor position within the element. The default value \\f$(-1, -1)\\f$ means that the\n",
      "        .   anchor is at the center. Note that only the shape of a cross-shaped element depends on the anchor\n",
      "        .   position. In other cases the anchor just regulates how much the result of the morphological\n",
      "        .   operation is shifted.\n",
      "    \n",
      "    getTextSize(...)\n",
      "        getTextSize(text, fontFace, fontScale, thickness) -> retval, baseLine\n",
      "        .   @brief Calculates the width and height of a text string.\n",
      "        .   \n",
      "        .   The function cv::getTextSize calculates and returns the size of a box that contains the specified text.\n",
      "        .   That is, the following code renders some text, the tight box surrounding it, and the baseline: :\n",
      "        .   @code\n",
      "        .       String text = \"Funny text inside the box\";\n",
      "        .       int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;\n",
      "        .       double fontScale = 2;\n",
      "        .       int thickness = 3;\n",
      "        .   \n",
      "        .       Mat img(600, 800, CV_8UC3, Scalar::all(0));\n",
      "        .   \n",
      "        .       int baseline=0;\n",
      "        .       Size textSize = getTextSize(text, fontFace,\n",
      "        .                                   fontScale, thickness, &baseline);\n",
      "        .       baseline += thickness;\n",
      "        .   \n",
      "        .       // center the text\n",
      "        .       Point textOrg((img.cols - textSize.width)/2,\n",
      "        .                     (img.rows + textSize.height)/2);\n",
      "        .   \n",
      "        .       // draw the box\n",
      "        .       rectangle(img, textOrg + Point(0, baseline),\n",
      "        .                 textOrg + Point(textSize.width, -textSize.height),\n",
      "        .                 Scalar(0,0,255));\n",
      "        .       // ... and the baseline first\n",
      "        .       line(img, textOrg + Point(0, thickness),\n",
      "        .            textOrg + Point(textSize.width, thickness),\n",
      "        .            Scalar(0, 0, 255));\n",
      "        .   \n",
      "        .       // then put the text itself\n",
      "        .       putText(img, text, textOrg, fontFace, fontScale,\n",
      "        .               Scalar::all(255), thickness, 8);\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @param text Input text string.\n",
      "        .   @param fontFace Font to use, see #HersheyFonts.\n",
      "        .   @param fontScale Font scale factor that is multiplied by the font-specific base size.\n",
      "        .   @param thickness Thickness of lines used to render the text. See #putText for details.\n",
      "        .   @param[out] baseLine y-coordinate of the baseline relative to the bottom-most text\n",
      "        .   point.\n",
      "        .   @return The size of a box that contains the specified text.\n",
      "        .   \n",
      "        .   @see putText\n",
      "    \n",
      "    getThreadNum(...)\n",
      "        getThreadNum() -> retval\n",
      "        .   @brief Returns the index of the currently executed thread within the current parallel region. Always\n",
      "        .   returns 0 if called outside of parallel region.\n",
      "        .   \n",
      "        .   @deprecated Current implementation doesn't corresponding to this documentation.\n",
      "        .   \n",
      "        .   The exact meaning of the return value depends on the threading framework used by OpenCV library:\n",
      "        .   - `TBB` - Unsupported with current 4.1 TBB release. Maybe will be supported in future.\n",
      "        .   - `OpenMP` - The thread number, within the current team, of the calling thread.\n",
      "        .   - `Concurrency` - An ID for the virtual processor that the current context is executing on (0\n",
      "        .     for master thread and unique number for others, but not necessary 1,2,3,...).\n",
      "        .   - `GCD` - System calling thread's ID. Never returns 0 inside parallel region.\n",
      "        .   - `C=` - The index of the current parallel task.\n",
      "        .   @sa setNumThreads, getNumThreads\n",
      "    \n",
      "    getTickCount(...)\n",
      "        getTickCount() -> retval\n",
      "        .   @brief Returns the number of ticks.\n",
      "        .   \n",
      "        .   The function returns the number of ticks after the certain event (for example, when the machine was\n",
      "        .   turned on). It can be used to initialize RNG or to measure a function execution time by reading the\n",
      "        .   tick count before and after the function call.\n",
      "        .   @sa getTickFrequency, TickMeter\n",
      "    \n",
      "    getTickFrequency(...)\n",
      "        getTickFrequency() -> retval\n",
      "        .   @brief Returns the number of ticks per second.\n",
      "        .   \n",
      "        .   The function returns the number of ticks per second. That is, the following code computes the\n",
      "        .   execution time in seconds:\n",
      "        .   @code\n",
      "        .       double t = (double)getTickCount();\n",
      "        .       // do something ...\n",
      "        .       t = ((double)getTickCount() - t)/getTickFrequency();\n",
      "        .   @endcode\n",
      "        .   @sa getTickCount, TickMeter\n",
      "    \n",
      "    getTrackbarPos(...)\n",
      "        getTrackbarPos(trackbarname, winname) -> retval\n",
      "        .   @brief Returns the trackbar position.\n",
      "        .   \n",
      "        .   The function returns the current position of the specified trackbar.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   [__Qt Backend Only__] winname can be empty if the trackbar is attached to the control\n",
      "        .   panel.\n",
      "        .   \n",
      "        .   @param trackbarname Name of the trackbar.\n",
      "        .   @param winname Name of the window that is the parent of the trackbar.\n",
      "    \n",
      "    getValidDisparityROI(...)\n",
      "        getValidDisparityROI(roi1, roi2, minDisparity, numberOfDisparities, SADWindowSize) -> retval\n",
      "        .\n",
      "    \n",
      "    getVersionMajor(...)\n",
      "        getVersionMajor() -> retval\n",
      "        .   @brief Returns major library version\n",
      "    \n",
      "    getVersionMinor(...)\n",
      "        getVersionMinor() -> retval\n",
      "        .   @brief Returns minor library version\n",
      "    \n",
      "    getVersionRevision(...)\n",
      "        getVersionRevision() -> retval\n",
      "        .   @brief Returns revision field of the library version\n",
      "    \n",
      "    getVersionString(...)\n",
      "        getVersionString() -> retval\n",
      "        .   @brief Returns library version string\n",
      "        .   \n",
      "        .   For example \"3.4.1-dev\".\n",
      "        .   \n",
      "        .   @sa getMajorVersion, getMinorVersion, getRevisionVersion\n",
      "    \n",
      "    getWindowImageRect(...)\n",
      "        getWindowImageRect(winname) -> retval\n",
      "        .   @brief Provides rectangle of image in the window.\n",
      "        .   \n",
      "        .   The function getWindowImageRect returns the client screen coordinates, width and height of the image rendering area.\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   \n",
      "        .   @sa resizeWindow moveWindow\n",
      "    \n",
      "    getWindowProperty(...)\n",
      "        getWindowProperty(winname, prop_id) -> retval\n",
      "        .   @brief Provides parameters of a window.\n",
      "        .   \n",
      "        .   The function getWindowProperty returns properties of a window.\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param prop_id Window property to retrieve. The following operation flags are available: (cv::WindowPropertyFlags)\n",
      "        .   \n",
      "        .   @sa setWindowProperty\n",
      "    \n",
      "    goodFeaturesToTrack(...)\n",
      "        goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance[, corners[, mask[, blockSize[, useHarrisDetector[, k]]]]]) -> corners\n",
      "        .   @brief Determines strong corners on an image.\n",
      "        .   \n",
      "        .   The function finds the most prominent corners in the image or in the specified image region, as\n",
      "        .   described in @cite Shi94\n",
      "        .   \n",
      "        .   -   Function calculates the corner quality measure at every source image pixel using the\n",
      "        .       #cornerMinEigenVal or #cornerHarris .\n",
      "        .   -   Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are\n",
      "        .       retained).\n",
      "        .   -   The corners with the minimal eigenvalue less than\n",
      "        .       \\f$\\texttt{qualityLevel} \\cdot \\max_{x,y} qualityMeasureMap(x,y)\\f$ are rejected.\n",
      "        .   -   The remaining corners are sorted by the quality measure in the descending order.\n",
      "        .   -   Function throws away each corner for which there is a stronger corner at a distance less than\n",
      "        .       maxDistance.\n",
      "        .   \n",
      "        .   The function can be used to initialize a point-based tracker of an object.\n",
      "        .   \n",
      "        .   @note If the function is called with different values A and B of the parameter qualityLevel , and\n",
      "        .   A \\> B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector\n",
      "        .   with qualityLevel=B .\n",
      "        .   \n",
      "        .   @param image Input 8-bit or floating-point 32-bit, single-channel image.\n",
      "        .   @param corners Output vector of detected corners.\n",
      "        .   @param maxCorners Maximum number of corners to return. If there are more corners than are found,\n",
      "        .   the strongest of them is returned. `maxCorners <= 0` implies that no limit on the maximum is set\n",
      "        .   and all detected corners are returned.\n",
      "        .   @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The\n",
      "        .   parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue\n",
      "        .   (see #cornerMinEigenVal ) or the Harris function response (see #cornerHarris ). The corners with the\n",
      "        .   quality measure less than the product are rejected. For example, if the best corner has the\n",
      "        .   quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure\n",
      "        .   less than 15 are rejected.\n",
      "        .   @param minDistance Minimum possible Euclidean distance between the returned corners.\n",
      "        .   @param mask Optional region of interest. If the image is not empty (it needs to have the type\n",
      "        .   CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.\n",
      "        .   @param blockSize Size of an average block for computing a derivative covariation matrix over each\n",
      "        .   pixel neighborhood. See cornerEigenValsAndVecs .\n",
      "        .   @param useHarrisDetector Parameter indicating whether to use a Harris detector (see #cornerHarris)\n",
      "        .   or #cornerMinEigenVal.\n",
      "        .   @param k Free parameter of the Harris detector.\n",
      "        .   \n",
      "        .   @sa  cornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,\n",
      "        \n",
      "        \n",
      "        \n",
      "        goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance, mask, blockSize, gradientSize[, corners[, useHarrisDetector[, k]]]) -> corners\n",
      "        .\n",
      "    \n",
      "    grabCut(...)\n",
      "        grabCut(img, mask, rect, bgdModel, fgdModel, iterCount[, mode]) -> mask, bgdModel, fgdModel\n",
      "        .   @brief Runs the GrabCut algorithm.\n",
      "        .   \n",
      "        .   The function implements the [GrabCut image segmentation algorithm](http://en.wikipedia.org/wiki/GrabCut).\n",
      "        .   \n",
      "        .   @param img Input 8-bit 3-channel image.\n",
      "        .   @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when\n",
      "        .   mode is set to #GC_INIT_WITH_RECT. Its elements may have one of the #GrabCutClasses.\n",
      "        .   @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as\n",
      "        .   \"obvious background\". The parameter is only used when mode==#GC_INIT_WITH_RECT .\n",
      "        .   @param bgdModel Temporary array for the background model. Do not modify it while you are\n",
      "        .   processing the same image.\n",
      "        .   @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are\n",
      "        .   processing the same image.\n",
      "        .   @param iterCount Number of iterations the algorithm should make before returning the result. Note\n",
      "        .   that the result can be refined with further calls with mode==#GC_INIT_WITH_MASK or\n",
      "        .   mode==GC_EVAL .\n",
      "        .   @param mode Operation mode that could be one of the #GrabCutModes\n",
      "    \n",
      "    groupRectangles(...)\n",
      "        groupRectangles(rectList, groupThreshold[, eps]) -> rectList, weights\n",
      "        .   @overload\n",
      "    \n",
      "    haveImageReader(...)\n",
      "        haveImageReader(filename) -> retval\n",
      "        .   @brief Returns true if the specified image can be decoded by OpenCV\n",
      "        .   \n",
      "        .   @param filename File name of the image\n",
      "    \n",
      "    haveImageWriter(...)\n",
      "        haveImageWriter(filename) -> retval\n",
      "        .   @brief Returns true if an image with the specified filename can be encoded by OpenCV\n",
      "        .   \n",
      "        .    @param filename File name of the image\n",
      "    \n",
      "    haveOpenVX(...)\n",
      "        haveOpenVX() -> retval\n",
      "        .\n",
      "    \n",
      "    hconcat(...)\n",
      "        hconcat(src[, dst]) -> dst\n",
      "        .   @overload\n",
      "        .    @code{.cpp}\n",
      "        .       std::vector<cv::Mat> matrices = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),\n",
      "        .                                         cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),\n",
      "        .                                         cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};\n",
      "        .   \n",
      "        .       cv::Mat out;\n",
      "        .       cv::hconcat( matrices, out );\n",
      "        .       //out:\n",
      "        .       //[1, 2, 3;\n",
      "        .       // 1, 2, 3;\n",
      "        .       // 1, 2, 3;\n",
      "        .       // 1, 2, 3]\n",
      "        .    @endcode\n",
      "        .    @param src input array or vector of matrices. all of the matrices must have the same number of rows and the same depth.\n",
      "        .    @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of the src.\n",
      "        .   same depth.\n",
      "    \n",
      "    idct(...)\n",
      "        idct(src[, dst[, flags]]) -> dst\n",
      "        .   @brief Calculates the inverse Discrete Cosine Transform of a 1D or 2D array.\n",
      "        .   \n",
      "        .   idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).\n",
      "        .   @param src input floating-point single-channel array.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @param flags operation flags.\n",
      "        .   @sa  dct, dft, idft, getOptimalDFTSize\n",
      "    \n",
      "    idft(...)\n",
      "        idft(src[, dst[, flags[, nonzeroRows]]]) -> dst\n",
      "        .   @brief Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.\n",
      "        .   \n",
      "        .   idft(src, dst, flags) is equivalent to dft(src, dst, flags | #DFT_INVERSE) .\n",
      "        .   @note None of dft and idft scales the result by default. So, you should pass #DFT_SCALE to one of\n",
      "        .   dft or idft explicitly to make these transforms mutually inverse.\n",
      "        .   @sa dft, dct, idct, mulSpectrums, getOptimalDFTSize\n",
      "        .   @param src input floating-point real or complex array.\n",
      "        .   @param dst output array whose size and type depend on the flags.\n",
      "        .   @param flags operation flags (see dft and #DftFlags).\n",
      "        .   @param nonzeroRows number of dst rows to process; the rest of the rows have undefined content (see\n",
      "        .   the convolution sample in dft description.\n",
      "    \n",
      "    illuminationChange(...)\n",
      "        illuminationChange(src, mask[, dst[, alpha[, beta]]]) -> dst\n",
      "        .   @brief Applying an appropriate non-linear transformation to the gradient field inside the selection and\n",
      "        .   then integrating back with a Poisson solver, modifies locally the apparent illumination of an image.\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param mask Input 8-bit 1 or 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src.\n",
      "        .   @param alpha Value ranges between 0-2.\n",
      "        .   @param beta Value ranges between 0-2.\n",
      "        .   \n",
      "        .   This is useful to highlight under-exposed foreground objects or to reduce specular reflections.\n",
      "    \n",
      "    imdecode(...)\n",
      "        imdecode(buf, flags) -> retval\n",
      "        .   @brief Reads an image from a buffer in memory.\n",
      "        .   \n",
      "        .   The function imdecode reads an image from the specified buffer in the memory. If the buffer is too short or\n",
      "        .   contains invalid data, the function returns an empty matrix ( Mat::data==NULL ).\n",
      "        .   \n",
      "        .   See cv::imread for the list of supported formats and flags description.\n",
      "        .   \n",
      "        .   @note In the case of color images, the decoded images will have the channels stored in **B G R** order.\n",
      "        .   @param buf Input array or vector of bytes.\n",
      "        .   @param flags The same flags as in cv::imread, see cv::ImreadModes.\n",
      "    \n",
      "    imencode(...)\n",
      "        imencode(ext, img[, params]) -> retval, buf\n",
      "        .   @brief Encodes an image into a memory buffer.\n",
      "        .   \n",
      "        .   The function imencode compresses the image and stores it in the memory buffer that is resized to fit the\n",
      "        .   result. See cv::imwrite for the list of supported formats and flags description.\n",
      "        .   \n",
      "        .   @param ext File extension that defines the output format.\n",
      "        .   @param img Image to be written.\n",
      "        .   @param buf Output buffer resized to fit the compressed image.\n",
      "        .   @param params Format-specific parameters. See cv::imwrite and cv::ImwriteFlags.\n",
      "    \n",
      "    imread(...)\n",
      "        imread(filename[, flags]) -> retval\n",
      "        .   @brief Loads an image from a file.\n",
      "        .   \n",
      "        .   @anchor imread\n",
      "        .   \n",
      "        .   The function imread loads an image from the specified file and returns it. If the image cannot be\n",
      "        .   read (because of missing file, improper permissions, unsupported or invalid format), the function\n",
      "        .   returns an empty matrix ( Mat::data==NULL ).\n",
      "        .   \n",
      "        .   Currently, the following file formats are supported:\n",
      "        .   \n",
      "        .   -   Windows bitmaps - \\*.bmp, \\*.dib (always supported)\n",
      "        .   -   JPEG files - \\*.jpeg, \\*.jpg, \\*.jpe (see the *Note* section)\n",
      "        .   -   JPEG 2000 files - \\*.jp2 (see the *Note* section)\n",
      "        .   -   Portable Network Graphics - \\*.png (see the *Note* section)\n",
      "        .   -   WebP - \\*.webp (see the *Note* section)\n",
      "        .   -   Portable image format - \\*.pbm, \\*.pgm, \\*.ppm \\*.pxm, \\*.pnm (always supported)\n",
      "        .   -   PFM files - \\*.pfm (see the *Note* section)\n",
      "        .   -   Sun rasters - \\*.sr, \\*.ras (always supported)\n",
      "        .   -   TIFF files - \\*.tiff, \\*.tif (see the *Note* section)\n",
      "        .   -   OpenEXR Image files - \\*.exr (see the *Note* section)\n",
      "        .   -   Radiance HDR - \\*.hdr, \\*.pic (always supported)\n",
      "        .   -   Raster and Vector geospatial data supported by GDAL (see the *Note* section)\n",
      "        .   \n",
      "        .   @note\n",
      "        .   -   The function determines the type of an image by the content, not by the file extension.\n",
      "        .   -   In the case of color images, the decoded images will have the channels stored in **B G R** order.\n",
      "        .   -   When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available.\n",
      "        .       Results may differ to the output of cvtColor()\n",
      "        .   -   On Microsoft Windows\\* OS and MacOSX\\*, the codecs shipped with an OpenCV image (libjpeg,\n",
      "        .       libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs,\n",
      "        .       and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware\n",
      "        .       that currently these native image loaders give images with different pixel values because of\n",
      "        .       the color management embedded into MacOSX.\n",
      "        .   -   On Linux\\*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for\n",
      "        .       codecs supplied with an OS image. Install the relevant packages (do not forget the development\n",
      "        .       files, for example, \"libjpeg-dev\", in Debian\\* and Ubuntu\\*) to get the codec support or turn\n",
      "        .       on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n",
      "        .   -   In the case you set *WITH_GDAL* flag to true in CMake and @ref IMREAD_LOAD_GDAL to load the image,\n",
      "        .       then the [GDAL](http://www.gdal.org) driver will be used in order to decode the image, supporting\n",
      "        .       the following formats: [Raster](http://www.gdal.org/formats_list.html),\n",
      "        .       [Vector](http://www.gdal.org/ogr_formats.html).\n",
      "        .   -   If EXIF information are embedded in the image file, the EXIF orientation will be taken into account\n",
      "        .       and thus the image will be rotated accordingly except if the flag @ref IMREAD_IGNORE_ORIENTATION is passed.\n",
      "        .   -   Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.\n",
      "        .   -   By default number of pixels must be less than 2^30. Limit can be set using system\n",
      "        .       variable OPENCV_IO_MAX_IMAGE_PIXELS\n",
      "        .   \n",
      "        .   @param filename Name of file to be loaded.\n",
      "        .   @param flags Flag that can take values of cv::ImreadModes\n",
      "    \n",
      "    imreadmulti(...)\n",
      "        imreadmulti(filename[, mats[, flags]]) -> retval, mats\n",
      "        .   @brief Loads a multi-page image from a file.\n",
      "        .   \n",
      "        .   The function imreadmulti loads a multi-page image from the specified file into a vector of Mat objects.\n",
      "        .   @param filename Name of file to be loaded.\n",
      "        .   @param flags Flag that can take values of cv::ImreadModes, default with cv::IMREAD_ANYCOLOR.\n",
      "        .   @param mats A vector of Mat objects holding each page, if more than one.\n",
      "        .   @sa cv::imread\n",
      "    \n",
      "    imshow(...)\n",
      "        imshow(winname, mat) -> None\n",
      "        .   @brief Displays an image in the specified window.\n",
      "        .   \n",
      "        .   The function imshow displays an image in the specified window. If the window was created with the\n",
      "        .   cv::WINDOW_AUTOSIZE flag, the image is shown with its original size, however it is still limited by the screen resolution.\n",
      "        .   Otherwise, the image is scaled to fit the window. The function may scale the image, depending on its depth:\n",
      "        .   \n",
      "        .   -   If the image is 8-bit unsigned, it is displayed as is.\n",
      "        .   -   If the image is 16-bit unsigned or 32-bit integer, the pixels are divided by 256. That is, the\n",
      "        .       value range [0,255\\*256] is mapped to [0,255].\n",
      "        .   -   If the image is 32-bit or 64-bit floating-point, the pixel values are multiplied by 255. That is, the\n",
      "        .       value range [0,1] is mapped to [0,255].\n",
      "        .   \n",
      "        .   If window was created with OpenGL support, cv::imshow also support ogl::Buffer , ogl::Texture2D and\n",
      "        .   cuda::GpuMat as input.\n",
      "        .   \n",
      "        .   If the window was not created before this function, it is assumed creating a window with cv::WINDOW_AUTOSIZE.\n",
      "        .   \n",
      "        .   If you need to show an image that is bigger than the screen resolution, you will need to call namedWindow(\"\", WINDOW_NORMAL) before the imshow.\n",
      "        .   \n",
      "        .   @note This function should be followed by cv::waitKey function which displays the image for specified\n",
      "        .   milliseconds. Otherwise, it won't display the image. For example, **waitKey(0)** will display the window\n",
      "        .   infinitely until any keypress (it is suitable for image display). **waitKey(25)** will display a frame\n",
      "        .   for 25 ms, after which display will be automatically closed. (If you put it in a loop to read\n",
      "        .   videos, it will display the video frame-by-frame)\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   [__Windows Backend Only__] Pressing Ctrl+C will copy the image to the clipboard.\n",
      "        .   \n",
      "        .   [__Windows Backend Only__] Pressing Ctrl+S will show a dialog to save the image.\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param mat Image to be shown.\n",
      "    \n",
      "    imwrite(...)\n",
      "        imwrite(filename, img[, params]) -> retval\n",
      "        .   @brief Saves an image to a specified file.\n",
      "        .   \n",
      "        .   The function imwrite saves the image to the specified file. The image format is chosen based on the\n",
      "        .   filename extension (see cv::imread for the list of extensions). In general, only 8-bit\n",
      "        .   single-channel or 3-channel (with 'BGR' channel order) images\n",
      "        .   can be saved using this function, with these exceptions:\n",
      "        .   \n",
      "        .   - 16-bit unsigned (CV_16U) images can be saved in the case of PNG, JPEG 2000, and TIFF formats\n",
      "        .   - 32-bit float (CV_32F) images can be saved in PFM, TIFF, OpenEXR, and Radiance HDR formats;\n",
      "        .     3-channel (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding\n",
      "        .     (4 bytes per pixel)\n",
      "        .   - PNG images with an alpha channel can be saved using this function. To do this, create\n",
      "        .   8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels\n",
      "        .   should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n",
      "        .   \n",
      "        .   If the format, depth or channel order is different, use\n",
      "        .   Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O\n",
      "        .   functions to save the image to XML or YAML format.\n",
      "        .   \n",
      "        .   The sample below shows how to create a BGRA image and save it to a PNG file. It also demonstrates how to set custom\n",
      "        .   compression parameters:\n",
      "        .   @include snippets/imgcodecs_imwrite.cpp\n",
      "        .   @param filename Name of the file.\n",
      "        .   @param img Image to be saved.\n",
      "        .   @param params Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags\n",
      "    \n",
      "    inRange(...)\n",
      "        inRange(src, lowerb, upperb[, dst]) -> dst\n",
      "        .   @brief  Checks if array elements lie between the elements of two other arrays.\n",
      "        .   \n",
      "        .   The function checks the range as follows:\n",
      "        .   -   For every element of a single-channel input array:\n",
      "        .       \\f[\\texttt{dst} (I)= \\texttt{lowerb} (I)_0  \\leq \\texttt{src} (I)_0 \\leq  \\texttt{upperb} (I)_0\\f]\n",
      "        .   -   For two-channel arrays:\n",
      "        .       \\f[\\texttt{dst} (I)= \\texttt{lowerb} (I)_0  \\leq \\texttt{src} (I)_0 \\leq  \\texttt{upperb} (I)_0  \\land \\texttt{lowerb} (I)_1  \\leq \\texttt{src} (I)_1 \\leq  \\texttt{upperb} (I)_1\\f]\n",
      "        .   -   and so forth.\n",
      "        .   \n",
      "        .   That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the\n",
      "        .   specified 1D, 2D, 3D, ... box and 0 otherwise.\n",
      "        .   \n",
      "        .   When the lower and/or upper boundary parameters are scalars, the indexes\n",
      "        .   (I) at lowerb and upperb in the above formulas should be omitted.\n",
      "        .   @param src first input array.\n",
      "        .   @param lowerb inclusive lower boundary array or a scalar.\n",
      "        .   @param upperb inclusive upper boundary array or a scalar.\n",
      "        .   @param dst output array of the same size as src and CV_8U type.\n",
      "    \n",
      "    initCameraMatrix2D(...)\n",
      "        initCameraMatrix2D(objectPoints, imagePoints, imageSize[, aspectRatio]) -> retval\n",
      "        .   @brief Finds an initial camera matrix from 3D-2D point correspondences.\n",
      "        .   \n",
      "        .   @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern\n",
      "        .   coordinate space. In the old interface all the per-view vectors are concatenated. See\n",
      "        .   calibrateCamera for details.\n",
      "        .   @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the\n",
      "        .   old interface all the per-view vectors are concatenated.\n",
      "        .   @param imageSize Image size in pixels used to initialize the principal point.\n",
      "        .   @param aspectRatio If it is zero or negative, both \\f$f_x\\f$ and \\f$f_y\\f$ are estimated independently.\n",
      "        .   Otherwise, \\f$f_x = f_y * \\texttt{aspectRatio}\\f$ .\n",
      "        .   \n",
      "        .   The function estimates and returns an initial camera matrix for the camera calibration process.\n",
      "        .   Currently, the function only supports planar calibration patterns, which are patterns where each\n",
      "        .   object point has z-coordinate =0.\n",
      "    \n",
      "    initUndistortRectifyMap(...)\n",
      "        initUndistortRectifyMap(cameraMatrix, distCoeffs, R, newCameraMatrix, size, m1type[, map1[, map2]]) -> map1, map2\n",
      "        .   @brief Computes the undistortion and rectification transformation map.\n",
      "        .   \n",
      "        .   The function computes the joint undistortion and rectification transformation and represents the\n",
      "        .   result in the form of maps for remap. The undistorted image looks like original, as if it is\n",
      "        .   captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a\n",
      "        .   monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by\n",
      "        .   #getOptimalNewCameraMatrix for a better control over scaling. In case of a stereo camera,\n",
      "        .   newCameraMatrix is normally set to P1 or P2 computed by #stereoRectify .\n",
      "        .   \n",
      "        .   Also, this new camera is oriented differently in the coordinate space, according to R. That, for\n",
      "        .   example, helps to align two heads of a stereo camera so that the epipolar lines on both images\n",
      "        .   become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).\n",
      "        .   \n",
      "        .   The function actually builds the maps for the inverse mapping algorithm that is used by remap. That\n",
      "        .   is, for each pixel \\f$(u, v)\\f$ in the destination (corrected and rectified) image, the function\n",
      "        .   computes the corresponding coordinates in the source image (that is, in the original image from\n",
      "        .   camera). The following process is applied:\n",
      "        .   \\f[\n",
      "        .   \\begin{array}{l}\n",
      "        .   x  \\leftarrow (u - {c'}_x)/{f'}_x  \\\\\n",
      "        .   y  \\leftarrow (v - {c'}_y)/{f'}_y  \\\\\n",
      "        .   {[X\\,Y\\,W]} ^T  \\leftarrow R^{-1}*[x \\, y \\, 1]^T  \\\\\n",
      "        .   x'  \\leftarrow X/W  \\\\\n",
      "        .   y'  \\leftarrow Y/W  \\\\\n",
      "        .   r^2  \\leftarrow x'^2 + y'^2 \\\\\n",
      "        .   x''  \\leftarrow x' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6}\n",
      "        .   + 2p_1 x' y' + p_2(r^2 + 2 x'^2)  + s_1 r^2 + s_2 r^4\\\\\n",
      "        .   y''  \\leftarrow y' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6}\n",
      "        .   + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\\\\n",
      "        .   s\\vecthree{x'''}{y'''}{1} =\n",
      "        .   \\vecthreethree{R_{33}(\\tau_x, \\tau_y)}{0}{-R_{13}((\\tau_x, \\tau_y)}\n",
      "        .   {0}{R_{33}(\\tau_x, \\tau_y)}{-R_{23}(\\tau_x, \\tau_y)}\n",
      "        .   {0}{0}{1} R(\\tau_x, \\tau_y) \\vecthree{x''}{y''}{1}\\\\\n",
      "        .   map_x(u,v)  \\leftarrow x''' f_x + c_x  \\\\\n",
      "        .   map_y(u,v)  \\leftarrow y''' f_y + c_y\n",
      "        .   \\end{array}\n",
      "        .   \\f]\n",
      "        .   where \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$\n",
      "        .   are the distortion coefficients.\n",
      "        .   \n",
      "        .   In case of a stereo camera, this function is called twice: once for each camera head, after\n",
      "        .   stereoRectify, which in its turn is called after #stereoCalibrate. But if the stereo camera\n",
      "        .   was not calibrated, it is still possible to compute the rectification transformations directly from\n",
      "        .   the fundamental matrix using #stereoRectifyUncalibrated. For each camera, the function computes\n",
      "        .   homography H as the rectification transformation in a pixel domain, not a rotation matrix R in 3D\n",
      "        .   space. R can be computed from H as\n",
      "        .   \\f[\\texttt{R} = \\texttt{cameraMatrix} ^{-1} \\cdot \\texttt{H} \\cdot \\texttt{cameraMatrix}\\f]\n",
      "        .   where cameraMatrix can be chosen arbitrarily.\n",
      "        .   \n",
      "        .   @param cameraMatrix Input camera matrix \\f$A=\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$\n",
      "        .   of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.\n",
      "        .   @param R Optional rectification transformation in the object space (3x3 matrix). R1 or R2 ,\n",
      "        .   computed by #stereoRectify can be passed here. If the matrix is empty, the identity transformation\n",
      "        .   is assumed. In cvInitUndistortMap R assumed to be an identity matrix.\n",
      "        .   @param newCameraMatrix New camera matrix \\f$A'=\\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}\\f$.\n",
      "        .   @param size Undistorted image size.\n",
      "        .   @param m1type Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see #convertMaps\n",
      "        .   @param map1 The first output map.\n",
      "        .   @param map2 The second output map.\n",
      "    \n",
      "    inpaint(...)\n",
      "        inpaint(src, inpaintMask, inpaintRadius, flags[, dst]) -> dst\n",
      "        .   @brief Restores the selected region in an image using the region neighborhood.\n",
      "        .   \n",
      "        .   @param src Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.\n",
      "        .   @param inpaintMask Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that\n",
      "        .   needs to be inpainted.\n",
      "        .   @param dst Output image with the same size and type as src .\n",
      "        .   @param inpaintRadius Radius of a circular neighborhood of each point inpainted that is considered\n",
      "        .   by the algorithm.\n",
      "        .   @param flags Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA\n",
      "        .   \n",
      "        .   The function reconstructs the selected image area from the pixel near the area boundary. The\n",
      "        .   function may be used to remove dust and scratches from a scanned photo, or to remove undesirable\n",
      "        .   objects from still images or video. See <http://en.wikipedia.org/wiki/Inpainting> for more details.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      -   An example using the inpainting technique can be found at\n",
      "        .           opencv_source_code/samples/cpp/inpaint.cpp\n",
      "        .      -   (Python) An example using the inpainting technique can be found at\n",
      "        .           opencv_source_code/samples/python/inpaint.py\n",
      "    \n",
      "    insertChannel(...)\n",
      "        insertChannel(src, dst, coi) -> dst\n",
      "        .   @brief Inserts a single channel to dst (coi is 0-based index)\n",
      "        .   @param src input array\n",
      "        .   @param dst output array\n",
      "        .   @param coi index of channel for insertion\n",
      "        .   @sa mixChannels, merge\n",
      "    \n",
      "    integral(...)\n",
      "        integral(src[, sum[, sdepth]]) -> sum\n",
      "        .   @overload\n",
      "    \n",
      "    integral2(...)\n",
      "        integral2(src[, sum[, sqsum[, sdepth[, sqdepth]]]]) -> sum, sqsum\n",
      "        .   @overload\n",
      "    \n",
      "    integral3(...)\n",
      "        integral3(src[, sum[, sqsum[, tilted[, sdepth[, sqdepth]]]]]) -> sum, sqsum, tilted\n",
      "        .   @brief Calculates the integral of an image.\n",
      "        .   \n",
      "        .   The function calculates one or more integral images for the source image as follows:\n",
      "        .   \n",
      "        .   \\f[\\texttt{sum} (X,Y) =  \\sum _{x<X,y<Y}  \\texttt{image} (x,y)\\f]\n",
      "        .   \n",
      "        .   \\f[\\texttt{sqsum} (X,Y) =  \\sum _{x<X,y<Y}  \\texttt{image} (x,y)^2\\f]\n",
      "        .   \n",
      "        .   \\f[\\texttt{tilted} (X,Y) =  \\sum _{y<Y,abs(x-X+1) \\leq Y-y-1}  \\texttt{image} (x,y)\\f]\n",
      "        .   \n",
      "        .   Using these integral images, you can calculate sum, mean, and standard deviation over a specific\n",
      "        .   up-right or rotated rectangular region of the image in a constant time, for example:\n",
      "        .   \n",
      "        .   \\f[\\sum _{x_1 \\leq x < x_2,  \\, y_1  \\leq y < y_2}  \\texttt{image} (x,y) =  \\texttt{sum} (x_2,y_2)- \\texttt{sum} (x_1,y_2)- \\texttt{sum} (x_2,y_1)+ \\texttt{sum} (x_1,y_1)\\f]\n",
      "        .   \n",
      "        .   It makes possible to do a fast blurring or fast block correlation with a variable window size, for\n",
      "        .   example. In case of multi-channel images, sums for each channel are accumulated independently.\n",
      "        .   \n",
      "        .   As a practical example, the next figure shows the calculation of the integral of a straight\n",
      "        .   rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the\n",
      "        .   original image are shown, as well as the relative pixels in the integral images sum and tilted .\n",
      "        .   \n",
      "        .   ![integral calculation example](pics/integral.png)\n",
      "        .   \n",
      "        .   @param src input image as \\f$W \\times H\\f$, 8-bit or floating-point (32f or 64f).\n",
      "        .   @param sum integral image as \\f$(W+1)\\times (H+1)\\f$ , 32-bit integer or floating-point (32f or 64f).\n",
      "        .   @param sqsum integral image for squared pixel values; it is \\f$(W+1)\\times (H+1)\\f$, double-precision\n",
      "        .   floating-point (64f) array.\n",
      "        .   @param tilted integral for the image rotated by 45 degrees; it is \\f$(W+1)\\times (H+1)\\f$ array with\n",
      "        .   the same data type as sum.\n",
      "        .   @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or\n",
      "        .   CV_64F.\n",
      "        .   @param sqdepth desired depth of the integral image of squared pixel values, CV_32F or CV_64F.\n",
      "    \n",
      "    intersectConvexConvex(...)\n",
      "        intersectConvexConvex(_p1, _p2[, _p12[, handleNested]]) -> retval, _p12\n",
      "        .\n",
      "    \n",
      "    invert(...)\n",
      "        invert(src[, dst[, flags]]) -> retval, dst\n",
      "        .   @brief Finds the inverse or pseudo-inverse of a matrix.\n",
      "        .   \n",
      "        .   The function cv::invert inverts the matrix src and stores the result in dst\n",
      "        .   . When the matrix src is singular or non-square, the function calculates\n",
      "        .   the pseudo-inverse matrix (the dst matrix) so that norm(src\\*dst - I) is\n",
      "        .   minimal, where I is an identity matrix.\n",
      "        .   \n",
      "        .   In case of the #DECOMP_LU method, the function returns non-zero value if\n",
      "        .   the inverse has been successfully calculated and 0 if src is singular.\n",
      "        .   \n",
      "        .   In case of the #DECOMP_SVD method, the function returns the inverse\n",
      "        .   condition number of src (the ratio of the smallest singular value to the\n",
      "        .   largest singular value) and 0 if src is singular. The SVD method\n",
      "        .   calculates a pseudo-inverse matrix if src is singular.\n",
      "        .   \n",
      "        .   Similarly to #DECOMP_LU, the method #DECOMP_CHOLESKY works only with\n",
      "        .   non-singular square matrices that should also be symmetrical and\n",
      "        .   positively defined. In this case, the function stores the inverted\n",
      "        .   matrix in dst and returns non-zero. Otherwise, it returns 0.\n",
      "        .   \n",
      "        .   @param src input floating-point M x N matrix.\n",
      "        .   @param dst output matrix of N x M size and the same type as src.\n",
      "        .   @param flags inversion method (cv::DecompTypes)\n",
      "        .   @sa solve, SVD\n",
      "    \n",
      "    invertAffineTransform(...)\n",
      "        invertAffineTransform(M[, iM]) -> iM\n",
      "        .   @brief Inverts an affine transformation.\n",
      "        .   \n",
      "        .   The function computes an inverse affine transformation represented by \\f$2 \\times 3\\f$ matrix M:\n",
      "        .   \n",
      "        .   \\f[\\begin{bmatrix} a_{11} & a_{12} & b_1  \\\\ a_{21} & a_{22} & b_2 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   The result is also a \\f$2 \\times 3\\f$ matrix of the same type as M.\n",
      "        .   \n",
      "        .   @param M Original affine transformation.\n",
      "        .   @param iM Output reverse affine transformation.\n",
      "    \n",
      "    isContourConvex(...)\n",
      "        isContourConvex(contour) -> retval\n",
      "        .   @brief Tests a contour convexity.\n",
      "        .   \n",
      "        .   The function tests whether the input contour is convex or not. The contour must be simple, that is,\n",
      "        .   without self-intersections. Otherwise, the function output is undefined.\n",
      "        .   \n",
      "        .   @param contour Input vector of 2D points, stored in std::vector\\<\\> or Mat\n",
      "    \n",
      "    kmeans(...)\n",
      "        kmeans(data, K, bestLabels, criteria, attempts, flags[, centers]) -> retval, bestLabels, centers\n",
      "        .   @brief Finds centers of clusters and groups input samples around the clusters.\n",
      "        .   \n",
      "        .   The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters\n",
      "        .   and groups the input samples around the clusters. As an output, \\f$\\texttt{bestLabels}_i\\f$ contains a\n",
      "        .   0-based cluster index for the sample stored in the \\f$i^{th}\\f$ row of the samples matrix.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   -   (Python) An example on K-means clustering can be found at\n",
      "        .       opencv_source_code/samples/python/kmeans.py\n",
      "        .   @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.\n",
      "        .   Examples of this array can be:\n",
      "        .   -   Mat points(count, 2, CV_32F);\n",
      "        .   -   Mat points(count, 1, CV_32FC2);\n",
      "        .   -   Mat points(1, count, CV_32FC2);\n",
      "        .   -   std::vector\\<cv::Point2f\\> points(sampleCount);\n",
      "        .   @param K Number of clusters to split the set by.\n",
      "        .   @param bestLabels Input/output integer array that stores the cluster indices for every sample.\n",
      "        .   @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or\n",
      "        .   the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster\n",
      "        .   centers moves by less than criteria.epsilon on some iteration, the algorithm stops.\n",
      "        .   @param attempts Flag to specify the number of times the algorithm is executed using different\n",
      "        .   initial labellings. The algorithm returns the labels that yield the best compactness (see the last\n",
      "        .   function parameter).\n",
      "        .   @param flags Flag that can take values of cv::KmeansFlags\n",
      "        .   @param centers Output matrix of the cluster centers, one row per each cluster center.\n",
      "        .   @return The function returns the compactness measure that is computed as\n",
      "        .   \\f[\\sum _i  \\| \\texttt{samples} _i -  \\texttt{centers} _{ \\texttt{labels} _i} \\| ^2\\f]\n",
      "        .   after every attempt. The best (minimum) value is chosen and the corresponding labels and the\n",
      "        .   compactness value are returned by the function. Basically, you can use only the core of the\n",
      "        .   function, set the number of attempts to 1, initialize labels each time using a custom algorithm,\n",
      "        .   pass them with the ( flags = #KMEANS_USE_INITIAL_LABELS ) flag, and then choose the best\n",
      "        .   (most-compact) clustering.\n",
      "    \n",
      "    line(...)\n",
      "        line(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @brief Draws a line segment connecting two points.\n",
      "        .   \n",
      "        .   The function line draws the line segment between pt1 and pt2 points in the image. The line is\n",
      "        .   clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected\n",
      "        .   or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased\n",
      "        .   lines are drawn using Gaussian filtering.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param pt1 First point of the line segment.\n",
      "        .   @param pt2 Second point of the line segment.\n",
      "        .   @param color Line color.\n",
      "        .   @param thickness Line thickness.\n",
      "        .   @param lineType Type of the line. See #LineTypes.\n",
      "        .   @param shift Number of fractional bits in the point coordinates.\n",
      "    \n",
      "    linearPolar(...)\n",
      "        linearPolar(src, center, maxRadius, flags[, dst]) -> dst\n",
      "        .   @brief Remaps an image to polar coordinates space.\n",
      "        .   \n",
      "        .   @deprecated This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags)\n",
      "        .   \n",
      "        .   @internal\n",
      "        .   Transform the source image using the following transformation (See @ref polar_remaps_reference_image \"Polar remaps reference image c)\"):\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     dst( \\rho , \\phi ) = src(x,y) \\\\\n",
      "        .     dst.size() \\leftarrow src.size()\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     I = (dx,dy) = (x - center.x,y - center.y) \\\\\n",
      "        .     \\rho = Kmag \\cdot \\texttt{magnitude} (I) ,\\\\\n",
      "        .     \\phi = angle \\cdot \\texttt{angle} (I)\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   and\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     Kx = src.cols / maxRadius \\\\\n",
      "        .     Ky = src.rows / 2\\Pi\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   @param src Source image\n",
      "        .   @param dst Destination image. It will have same size and type as src.\n",
      "        .   @param center The transformation center;\n",
      "        .   @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude scale parameter too.\n",
      "        .   @param flags A combination of interpolation methods, see #InterpolationFlags\n",
      "        .   \n",
      "        .   @note\n",
      "        .   -   The function can not operate in-place.\n",
      "        .   -   To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.\n",
      "        .   \n",
      "        .   @sa cv::logPolar\n",
      "        .   @endinternal\n",
      "    \n",
      "    log(...)\n",
      "        log(src[, dst]) -> dst\n",
      "        .   @brief Calculates the natural logarithm of every array element.\n",
      "        .   \n",
      "        .   The function cv::log calculates the natural logarithm of every element of the input array:\n",
      "        .   \\f[\\texttt{dst} (I) =  \\log (\\texttt{src}(I)) \\f]\n",
      "        .   \n",
      "        .   Output on zero, negative and special (NaN, Inf) values is undefined.\n",
      "        .   \n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same size and type as src .\n",
      "        .   @sa exp, cartToPolar, polarToCart, phase, pow, sqrt, magnitude\n",
      "    \n",
      "    logPolar(...)\n",
      "        logPolar(src, center, M, flags[, dst]) -> dst\n",
      "        .   @brief Remaps an image to semilog-polar coordinates space.\n",
      "        .   \n",
      "        .   @deprecated This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags+WARP_POLAR_LOG);\n",
      "        .   \n",
      "        .   @internal\n",
      "        .   Transform the source image using the following transformation (See @ref polar_remaps_reference_image \"Polar remaps reference image d)\"):\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     dst( \\rho , \\phi ) = src(x,y) \\\\\n",
      "        .     dst.size() \\leftarrow src.size()\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     I = (dx,dy) = (x - center.x,y - center.y) \\\\\n",
      "        .     \\rho = M \\cdot log_e(\\texttt{magnitude} (I)) ,\\\\\n",
      "        .     \\phi = Kangle \\cdot \\texttt{angle} (I) \\\\\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   and\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .     M = src.cols / log_e(maxRadius) \\\\\n",
      "        .     Kangle = src.rows / 2\\Pi \\\\\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   The function emulates the human \"foveal\" vision and can be used for fast scale and\n",
      "        .   rotation-invariant template matching, for object tracking and so forth.\n",
      "        .   @param src Source image\n",
      "        .   @param dst Destination image. It will have same size and type as src.\n",
      "        .   @param center The transformation center; where the output precision is maximal\n",
      "        .   @param M Magnitude scale parameter. It determines the radius of the bounding circle to transform too.\n",
      "        .   @param flags A combination of interpolation methods, see #InterpolationFlags\n",
      "        .   \n",
      "        .   @note\n",
      "        .   -   The function can not operate in-place.\n",
      "        .   -   To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.\n",
      "        .   \n",
      "        .   @sa cv::linearPolar\n",
      "        .   @endinternal\n",
      "    \n",
      "    magnitude(...)\n",
      "        magnitude(x, y[, magnitude]) -> magnitude\n",
      "        .   @brief Calculates the magnitude of 2D vectors.\n",
      "        .   \n",
      "        .   The function cv::magnitude calculates the magnitude of 2D vectors formed\n",
      "        .   from the corresponding elements of x and y arrays:\n",
      "        .   \\f[\\texttt{dst} (I) =  \\sqrt{\\texttt{x}(I)^2 + \\texttt{y}(I)^2}\\f]\n",
      "        .   @param x floating-point array of x-coordinates of the vectors.\n",
      "        .   @param y floating-point array of y-coordinates of the vectors; it must\n",
      "        .   have the same size as x.\n",
      "        .   @param magnitude output array of the same size and type as x.\n",
      "        .   @sa cartToPolar, polarToCart, phase, sqrt\n",
      "    \n",
      "    matMulDeriv(...)\n",
      "        matMulDeriv(A, B[, dABdA[, dABdB]]) -> dABdA, dABdB\n",
      "        .   @brief Computes partial derivatives of the matrix product for each multiplied matrix.\n",
      "        .   \n",
      "        .   @param A First multiplied matrix.\n",
      "        .   @param B Second multiplied matrix.\n",
      "        .   @param dABdA First output derivative matrix d(A\\*B)/dA of size\n",
      "        .   \\f$\\texttt{A.rows*B.cols} \\times {A.rows*A.cols}\\f$ .\n",
      "        .   @param dABdB Second output derivative matrix d(A\\*B)/dB of size\n",
      "        .   \\f$\\texttt{A.rows*B.cols} \\times {B.rows*B.cols}\\f$ .\n",
      "        .   \n",
      "        .   The function computes partial derivatives of the elements of the matrix product \\f$A*B\\f$ with regard to\n",
      "        .   the elements of each of the two input matrices. The function is used to compute the Jacobian\n",
      "        .   matrices in stereoCalibrate but can also be used in any other similar optimization function.\n",
      "    \n",
      "    matchShapes(...)\n",
      "        matchShapes(contour1, contour2, method, parameter) -> retval\n",
      "        .   @brief Compares two shapes.\n",
      "        .   \n",
      "        .   The function compares two shapes. All three implemented methods use the Hu invariants (see #HuMoments)\n",
      "        .   \n",
      "        .   @param contour1 First contour or grayscale image.\n",
      "        .   @param contour2 Second contour or grayscale image.\n",
      "        .   @param method Comparison method, see #ShapeMatchModes\n",
      "        .   @param parameter Method-specific parameter (not supported now).\n",
      "    \n",
      "    matchTemplate(...)\n",
      "        matchTemplate(image, templ, method[, result[, mask]]) -> result\n",
      "        .   @brief Compares a template against overlapped image regions.\n",
      "        .   \n",
      "        .   The function slides through image , compares the overlapped patches of size \\f$w \\times h\\f$ against\n",
      "        .   templ using the specified method and stores the comparison results in result . Here are the formulae\n",
      "        .   for the available comparison methods ( \\f$I\\f$ denotes image, \\f$T\\f$ template, \\f$R\\f$ result ). The summation\n",
      "        .   is done over template and/or the image patch: \\f$x' = 0...w-1, y' = 0...h-1\\f$\n",
      "        .   \n",
      "        .   After the function finishes the comparison, the best matches can be found as global minimums (when\n",
      "        .   #TM_SQDIFF was used) or maximums (when #TM_CCORR or #TM_CCOEFF was used) using the\n",
      "        .   #minMaxLoc function. In case of a color image, template summation in the numerator and each sum in\n",
      "        .   the denominator is done over all of the channels and separate mean values are used for each channel.\n",
      "        .   That is, the function can take a color template and a color image. The result will still be a\n",
      "        .   single-channel image, which is easier to analyze.\n",
      "        .   \n",
      "        .   @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.\n",
      "        .   @param templ Searched template. It must be not greater than the source image and have the same\n",
      "        .   data type.\n",
      "        .   @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image\n",
      "        .   is \\f$W \\times H\\f$ and templ is \\f$w \\times h\\f$ , then result is \\f$(W-w+1) \\times (H-h+1)\\f$ .\n",
      "        .   @param method Parameter specifying the comparison method, see #TemplateMatchModes\n",
      "        .   @param mask Mask of searched template. It must have the same datatype and size with templ. It is\n",
      "        .   not set by default. Currently, only the #TM_SQDIFF and #TM_CCORR_NORMED methods are supported.\n",
      "    \n",
      "    max(...)\n",
      "        max(src1, src2[, dst]) -> dst\n",
      "        .   @brief Calculates per-element maximum of two arrays or an array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::max calculates the per-element maximum of two arrays:\n",
      "        .   \\f[\\texttt{dst} (I)= \\max ( \\texttt{src1} (I), \\texttt{src2} (I))\\f]\n",
      "        .   or array and a scalar:\n",
      "        .   \\f[\\texttt{dst} (I)= \\max ( \\texttt{src1} (I), \\texttt{value} )\\f]\n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size and type as src1 .\n",
      "        .   @param dst output array of the same size and type as src1.\n",
      "        .   @sa  min, compare, inRange, minMaxLoc, @ref MatrixExpressions\n",
      "    \n",
      "    mean(...)\n",
      "        mean(src[, mask]) -> retval\n",
      "        .   @brief Calculates an average (mean) of array elements.\n",
      "        .   \n",
      "        .   The function cv::mean calculates the mean value M of array elements,\n",
      "        .   independently for each channel, and return it:\n",
      "        .   \\f[\\begin{array}{l} N =  \\sum _{I: \\; \\texttt{mask} (I) \\ne 0} 1 \\\\ M_c =  \\left ( \\sum _{I: \\; \\texttt{mask} (I) \\ne 0}{ \\texttt{mtx} (I)_c} \\right )/N \\end{array}\\f]\n",
      "        .   When all the mask elements are 0's, the function returns Scalar::all(0)\n",
      "        .   @param src input array that should have from 1 to 4 channels so that the result can be stored in\n",
      "        .   Scalar_ .\n",
      "        .   @param mask optional operation mask.\n",
      "        .   @sa  countNonZero, meanStdDev, norm, minMaxLoc\n",
      "    \n",
      "    meanShift(...)\n",
      "        meanShift(probImage, window, criteria) -> retval, window\n",
      "        .   @brief Finds an object on a back projection image.\n",
      "        .   \n",
      "        .   @param probImage Back projection of the object histogram. See calcBackProject for details.\n",
      "        .   @param window Initial search window.\n",
      "        .   @param criteria Stop criteria for the iterative search algorithm.\n",
      "        .   returns\n",
      "        .   :   Number of iterations CAMSHIFT took to converge.\n",
      "        .   The function implements the iterative object search algorithm. It takes the input back projection of\n",
      "        .   an object and the initial position. The mass center in window of the back projection image is\n",
      "        .   computed and the search window center shifts to the mass center. The procedure is repeated until the\n",
      "        .   specified number of iterations criteria.maxCount is done or until the window center shifts by less\n",
      "        .   than criteria.epsilon. The algorithm is used inside CamShift and, unlike CamShift , the search\n",
      "        .   window size or orientation do not change during the search. You can simply pass the output of\n",
      "        .   calcBackProject to this function. But better results can be obtained if you pre-filter the back\n",
      "        .   projection and remove the noise. For example, you can do this by retrieving connected components\n",
      "        .   with findContours , throwing away contours with small area ( contourArea ), and rendering the\n",
      "        .   remaining contours with drawContours.\n",
      "    \n",
      "    meanStdDev(...)\n",
      "        meanStdDev(src[, mean[, stddev[, mask]]]) -> mean, stddev\n",
      "        .   Calculates a mean and standard deviation of array elements.\n",
      "        .   \n",
      "        .   The function cv::meanStdDev calculates the mean and the standard deviation M\n",
      "        .   of array elements independently for each channel and returns it via the\n",
      "        .   output parameters:\n",
      "        .   \\f[\\begin{array}{l} N =  \\sum _{I, \\texttt{mask} (I)  \\ne 0} 1 \\\\ \\texttt{mean} _c =  \\frac{\\sum_{ I: \\; \\texttt{mask}(I) \\ne 0} \\texttt{src} (I)_c}{N} \\\\ \\texttt{stddev} _c =  \\sqrt{\\frac{\\sum_{ I: \\; \\texttt{mask}(I) \\ne 0} \\left ( \\texttt{src} (I)_c -  \\texttt{mean} _c \\right )^2}{N}} \\end{array}\\f]\n",
      "        .   When all the mask elements are 0's, the function returns\n",
      "        .   mean=stddev=Scalar::all(0).\n",
      "        .   @note The calculated standard deviation is only the diagonal of the\n",
      "        .   complete normalized covariance matrix. If the full matrix is needed, you\n",
      "        .   can reshape the multi-channel array M x N to the single-channel array\n",
      "        .   M\\*N x mtx.channels() (only possible when the matrix is continuous) and\n",
      "        .   then pass the matrix to calcCovarMatrix .\n",
      "        .   @param src input array that should have from 1 to 4 channels so that the results can be stored in\n",
      "        .   Scalar_ 's.\n",
      "        .   @param mean output parameter: calculated mean value.\n",
      "        .   @param stddev output parameter: calculated standard deviation.\n",
      "        .   @param mask optional operation mask.\n",
      "        .   @sa  countNonZero, mean, norm, minMaxLoc, calcCovarMatrix\n",
      "    \n",
      "    medianBlur(...)\n",
      "        medianBlur(src, ksize[, dst]) -> dst\n",
      "        .   @brief Blurs an image using the median filter.\n",
      "        .   \n",
      "        .   The function smoothes an image using the median filter with the \\f$\\texttt{ksize} \\times\n",
      "        .   \\texttt{ksize}\\f$ aperture. Each channel of a multi-channel image is processed independently.\n",
      "        .   In-place operation is supported.\n",
      "        .   \n",
      "        .   @note The median filter uses #BORDER_REPLICATE internally to cope with border pixels, see #BorderTypes\n",
      "        .   \n",
      "        .   @param src input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be\n",
      "        .   CV_8U, CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.\n",
      "        .   @param dst destination array of the same size and type as src.\n",
      "        .   @param ksize aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...\n",
      "        .   @sa  bilateralFilter, blur, boxFilter, GaussianBlur\n",
      "    \n",
      "    merge(...)\n",
      "        merge(mv[, dst]) -> dst\n",
      "        .   @overload\n",
      "        .   @param mv input vector of matrices to be merged; all the matrices in mv must have the same\n",
      "        .   size and the same depth.\n",
      "        .   @param dst output array of the same size and the same depth as mv[0]; The number of channels will\n",
      "        .   be the total number of channels in the matrix array.\n",
      "    \n",
      "    min(...)\n",
      "        min(src1, src2[, dst]) -> dst\n",
      "        .   @brief Calculates per-element minimum of two arrays or an array and a scalar.\n",
      "        .   \n",
      "        .   The function cv::min calculates the per-element minimum of two arrays:\n",
      "        .   \\f[\\texttt{dst} (I)= \\min ( \\texttt{src1} (I), \\texttt{src2} (I))\\f]\n",
      "        .   or array and a scalar:\n",
      "        .   \\f[\\texttt{dst} (I)= \\min ( \\texttt{src1} (I), \\texttt{value} )\\f]\n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size and type as src1.\n",
      "        .   @param dst output array of the same size and type as src1.\n",
      "        .   @sa max, compare, inRange, minMaxLoc\n",
      "    \n",
      "    minAreaRect(...)\n",
      "        minAreaRect(points) -> retval\n",
      "        .   @brief Finds a rotated rectangle of the minimum area enclosing the input 2D point set.\n",
      "        .   \n",
      "        .   The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a\n",
      "        .   specified point set. Developer should keep in mind that the returned RotatedRect can contain negative\n",
      "        .   indices when data is close to the containing Mat element boundary.\n",
      "        .   \n",
      "        .   @param points Input vector of 2D points, stored in std::vector\\<\\> or Mat\n",
      "    \n",
      "    minEnclosingCircle(...)\n",
      "        minEnclosingCircle(points) -> center, radius\n",
      "        .   @brief Finds a circle of the minimum area enclosing a 2D point set.\n",
      "        .   \n",
      "        .   The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.\n",
      "        .   \n",
      "        .   @param points Input vector of 2D points, stored in std::vector\\<\\> or Mat\n",
      "        .   @param center Output center of the circle.\n",
      "        .   @param radius Output radius of the circle.\n",
      "    \n",
      "    minEnclosingTriangle(...)\n",
      "        minEnclosingTriangle(points[, triangle]) -> retval, triangle\n",
      "        .   @brief Finds a triangle of minimum area enclosing a 2D point set and returns its area.\n",
      "        .   \n",
      "        .   The function finds a triangle of minimum area enclosing the given set of 2D points and returns its\n",
      "        .   area. The output for a given 2D point set is shown in the image below. 2D points are depicted in\n",
      "        .   *red* and the enclosing triangle in *yellow*.\n",
      "        .   \n",
      "        .   ![Sample output of the minimum enclosing triangle function](pics/minenclosingtriangle.png)\n",
      "        .   \n",
      "        .   The implementation of the algorithm is based on O'Rourke's @cite ORourke86 and Klee and Laskowski's\n",
      "        .   @cite KleeLaskowski85 papers. O'Rourke provides a \\f$\\theta(n)\\f$ algorithm for finding the minimal\n",
      "        .   enclosing triangle of a 2D convex polygon with n vertices. Since the #minEnclosingTriangle function\n",
      "        .   takes a 2D point set as input an additional preprocessing step of computing the convex hull of the\n",
      "        .   2D point set is required. The complexity of the #convexHull function is \\f$O(n log(n))\\f$ which is higher\n",
      "        .   than \\f$\\theta(n)\\f$. Thus the overall complexity of the function is \\f$O(n log(n))\\f$.\n",
      "        .   \n",
      "        .   @param points Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector\\<\\> or Mat\n",
      "        .   @param triangle Output vector of three 2D points defining the vertices of the triangle. The depth\n",
      "        .   of the OutputArray must be CV_32F.\n",
      "    \n",
      "    minMaxLoc(...)\n",
      "        minMaxLoc(src[, mask]) -> minVal, maxVal, minLoc, maxLoc\n",
      "        .   @brief Finds the global minimum and maximum in an array.\n",
      "        .   \n",
      "        .   The function cv::minMaxLoc finds the minimum and maximum element values and their positions. The\n",
      "        .   extremums are searched across the whole array or, if mask is not an empty array, in the specified\n",
      "        .   array region.\n",
      "        .   \n",
      "        .   The function do not work with multi-channel arrays. If you need to find minimum or maximum\n",
      "        .   elements across all the channels, use Mat::reshape first to reinterpret the array as\n",
      "        .   single-channel. Or you may extract the particular channel using either extractImageCOI , or\n",
      "        .   mixChannels , or split .\n",
      "        .   @param src input single-channel array.\n",
      "        .   @param minVal pointer to the returned minimum value; NULL is used if not required.\n",
      "        .   @param maxVal pointer to the returned maximum value; NULL is used if not required.\n",
      "        .   @param minLoc pointer to the returned minimum location (in 2D case); NULL is used if not required.\n",
      "        .   @param maxLoc pointer to the returned maximum location (in 2D case); NULL is used if not required.\n",
      "        .   @param mask optional mask used to select a sub-array.\n",
      "        .   @sa max, min, compare, inRange, extractImageCOI, mixChannels, split, Mat::reshape\n",
      "    \n",
      "    mixChannels(...)\n",
      "        mixChannels(src, dst, fromTo) -> dst\n",
      "        .   @overload\n",
      "        .   @param src input array or vector of matrices; all of the matrices must have the same size and the\n",
      "        .   same depth.\n",
      "        .   @param dst output array or vector of matrices; all the matrices **must be allocated**; their size and\n",
      "        .   depth must be the same as in src[0].\n",
      "        .   @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k\\*2] is\n",
      "        .   a 0-based index of the input channel in src, fromTo[k\\*2+1] is an index of the output channel in\n",
      "        .   dst; the continuous channel numbering is used: the first input image channels are indexed from 0 to\n",
      "        .   src[0].channels()-1, the second input image channels are indexed from src[0].channels() to\n",
      "        .   src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image\n",
      "        .   channels; as a special case, when fromTo[k\\*2] is negative, the corresponding output channel is\n",
      "        .   filled with zero .\n",
      "    \n",
      "    moments(...)\n",
      "        moments(array[, binaryImage]) -> retval\n",
      "        .   @brief Calculates all of the moments up to the third order of a polygon or rasterized shape.\n",
      "        .   \n",
      "        .   The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The\n",
      "        .   results are returned in the structure cv::Moments.\n",
      "        .   \n",
      "        .   @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array (\n",
      "        .   \\f$1 \\times N\\f$ or \\f$N \\times 1\\f$ ) of 2D points (Point or Point2f ).\n",
      "        .   @param binaryImage If it is true, all non-zero image pixels are treated as 1's. The parameter is\n",
      "        .   used for images only.\n",
      "        .   @returns moments.\n",
      "        .   \n",
      "        .   @note Only applicable to contour moments calculations from Python bindings: Note that the numpy\n",
      "        .   type for the input array should be either np.int32 or np.float32.\n",
      "        .   \n",
      "        .   @sa  contourArea, arcLength\n",
      "    \n",
      "    morphologyEx(...)\n",
      "        morphologyEx(src, op, kernel[, dst[, anchor[, iterations[, borderType[, borderValue]]]]]) -> dst\n",
      "        .   @brief Performs advanced morphological transformations.\n",
      "        .   \n",
      "        .   The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as\n",
      "        .   basic operations.\n",
      "        .   \n",
      "        .   Any of the operations can be done in-place. In case of multi-channel images, each channel is\n",
      "        .   processed independently.\n",
      "        .   \n",
      "        .   @param src Source image. The number of channels can be arbitrary. The depth should be one of\n",
      "        .   CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
      "        .   @param dst Destination image of the same size and type as source image.\n",
      "        .   @param op Type of a morphological operation, see #MorphTypes\n",
      "        .   @param kernel Structuring element. It can be created using #getStructuringElement.\n",
      "        .   @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the\n",
      "        .   kernel center.\n",
      "        .   @param iterations Number of times erosion and dilation are applied.\n",
      "        .   @param borderType Pixel extrapolation method, see #BorderTypes\n",
      "        .   @param borderValue Border value in case of a constant border. The default value has a special\n",
      "        .   meaning.\n",
      "        .   @sa  dilate, erode, getStructuringElement\n",
      "        .   @note The number of iterations is the number of times erosion or dilatation operation will be applied.\n",
      "        .   For instance, an opening operation (#MORPH_OPEN) with two iterations is equivalent to apply\n",
      "        .   successively: erode -> erode -> dilate -> dilate (and not erode -> dilate -> erode -> dilate).\n",
      "    \n",
      "    moveWindow(...)\n",
      "        moveWindow(winname, x, y) -> None\n",
      "        .   @brief Moves window to the specified position\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param x The new x-coordinate of the window.\n",
      "        .   @param y The new y-coordinate of the window.\n",
      "    \n",
      "    mulSpectrums(...)\n",
      "        mulSpectrums(a, b, flags[, c[, conjB]]) -> c\n",
      "        .   @brief Performs the per-element multiplication of two Fourier spectrums.\n",
      "        .   \n",
      "        .   The function cv::mulSpectrums performs the per-element multiplication of the two CCS-packed or complex\n",
      "        .   matrices that are results of a real or complex Fourier transform.\n",
      "        .   \n",
      "        .   The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )\n",
      "        .   or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are\n",
      "        .   simply multiplied (per element) with an optional conjugation of the second-array elements. When the\n",
      "        .   arrays are real, they are assumed to be CCS-packed (see dft for details).\n",
      "        .   @param a first input array.\n",
      "        .   @param b second input array of the same size and type as src1 .\n",
      "        .   @param c output array of the same size and type as src1 .\n",
      "        .   @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates that\n",
      "        .   each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this flag, then simply add a `0` as value.\n",
      "        .   @param conjB optional flag that conjugates the second input array before the multiplication (true)\n",
      "        .   or not (false).\n",
      "    \n",
      "    mulTransposed(...)\n",
      "        mulTransposed(src, aTa[, dst[, delta[, scale[, dtype]]]]) -> dst\n",
      "        .   @brief Calculates the product of a matrix and its transposition.\n",
      "        .   \n",
      "        .   The function cv::mulTransposed calculates the product of src and its\n",
      "        .   transposition:\n",
      "        .   \\f[\\texttt{dst} = \\texttt{scale} ( \\texttt{src} - \\texttt{delta} )^T ( \\texttt{src} - \\texttt{delta} )\\f]\n",
      "        .   if aTa=true , and\n",
      "        .   \\f[\\texttt{dst} = \\texttt{scale} ( \\texttt{src} - \\texttt{delta} ) ( \\texttt{src} - \\texttt{delta} )^T\\f]\n",
      "        .   otherwise. The function is used to calculate the covariance matrix. With\n",
      "        .   zero delta, it can be used as a faster substitute for general matrix\n",
      "        .   product A\\*B when B=A'\n",
      "        .   @param src input single-channel matrix. Note that unlike gemm, the\n",
      "        .   function can multiply not only floating-point matrices.\n",
      "        .   @param dst output square matrix.\n",
      "        .   @param aTa Flag specifying the multiplication ordering. See the\n",
      "        .   description below.\n",
      "        .   @param delta Optional delta matrix subtracted from src before the\n",
      "        .   multiplication. When the matrix is empty ( delta=noArray() ), it is\n",
      "        .   assumed to be zero, that is, nothing is subtracted. If it has the same\n",
      "        .   size as src , it is simply subtracted. Otherwise, it is \"repeated\" (see\n",
      "        .   repeat ) to cover the full src and then subtracted. Type of the delta\n",
      "        .   matrix, when it is not empty, must be the same as the type of created\n",
      "        .   output matrix. See the dtype parameter description below.\n",
      "        .   @param scale Optional scale factor for the matrix product.\n",
      "        .   @param dtype Optional type of the output matrix. When it is negative,\n",
      "        .   the output matrix will have the same type as src . Otherwise, it will be\n",
      "        .   type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .\n",
      "        .   @sa calcCovarMatrix, gemm, repeat, reduce\n",
      "    \n",
      "    multiply(...)\n",
      "        multiply(src1, src2[, dst[, scale[, dtype]]]) -> dst\n",
      "        .   @brief Calculates the per-element scaled product of two arrays.\n",
      "        .   \n",
      "        .   The function multiply calculates the per-element product of two arrays:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (I)= \\texttt{saturate} ( \\texttt{scale} \\cdot \\texttt{src1} (I)  \\cdot \\texttt{src2} (I))\\f]\n",
      "        .   \n",
      "        .   There is also a @ref MatrixExpressions -friendly variant of the first function. See Mat::mul .\n",
      "        .   \n",
      "        .   For a not-per-element matrix product, see gemm .\n",
      "        .   \n",
      "        .   @note Saturation is not applied when the output array has the depth\n",
      "        .   CV_32S. You may even get result of an incorrect sign in the case of\n",
      "        .   overflow.\n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size and the same type as src1.\n",
      "        .   @param dst output array of the same size and type as src1.\n",
      "        .   @param scale optional scale factor.\n",
      "        .   @param dtype optional depth of the output array\n",
      "        .   @sa add, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare,\n",
      "        .   Mat::convertTo\n",
      "    \n",
      "    namedWindow(...)\n",
      "        namedWindow(winname[, flags]) -> None\n",
      "        .   @brief Creates a window.\n",
      "        .   \n",
      "        .   The function namedWindow creates a window that can be used as a placeholder for images and\n",
      "        .   trackbars. Created windows are referred to by their names.\n",
      "        .   \n",
      "        .   If a window with the same name already exists, the function does nothing.\n",
      "        .   \n",
      "        .   You can call cv::destroyWindow or cv::destroyAllWindows to close the window and de-allocate any associated\n",
      "        .   memory usage. For a simple program, you do not really have to call these functions because all the\n",
      "        .   resources and windows of the application are closed automatically by the operating system upon exit.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   Qt backend supports additional flags:\n",
      "        .    -   **WINDOW_NORMAL or WINDOW_AUTOSIZE:** WINDOW_NORMAL enables you to resize the\n",
      "        .        window, whereas WINDOW_AUTOSIZE adjusts automatically the window size to fit the\n",
      "        .        displayed image (see imshow ), and you cannot change the window size manually.\n",
      "        .    -   **WINDOW_FREERATIO or WINDOW_KEEPRATIO:** WINDOW_FREERATIO adjusts the image\n",
      "        .        with no respect to its ratio, whereas WINDOW_KEEPRATIO keeps the image ratio.\n",
      "        .    -   **WINDOW_GUI_NORMAL or WINDOW_GUI_EXPANDED:** WINDOW_GUI_NORMAL is the old way to draw the window\n",
      "        .        without statusbar and toolbar, whereas WINDOW_GUI_EXPANDED is a new enhanced GUI.\n",
      "        .   By default, flags == WINDOW_AUTOSIZE | WINDOW_KEEPRATIO | WINDOW_GUI_EXPANDED\n",
      "        .   \n",
      "        .   @param winname Name of the window in the window caption that may be used as a window identifier.\n",
      "        .   @param flags Flags of the window. The supported flags are: (cv::WindowFlags)\n",
      "    \n",
      "    norm(...)\n",
      "        norm(src1[, normType[, mask]]) -> retval\n",
      "        .   @brief Calculates the  absolute norm of an array.\n",
      "        .   \n",
      "        .   This version of #norm calculates the absolute norm of src1. The type of norm to calculate is specified using #NormTypes.\n",
      "        .   \n",
      "        .   As example for one array consider the function \\f$r(x)= \\begin{pmatrix} x \\\\ 1-x \\end{pmatrix}, x \\in [-1;1]\\f$.\n",
      "        .   The \\f$ L_{1}, L_{2} \\f$ and \\f$ L_{\\infty} \\f$ norm for the sample value \\f$r(-1) = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\\f$\n",
      "        .   is calculated as follows\n",
      "        .   \\f{align*}\n",
      "        .       \\| r(-1) \\|_{L_1} &= |-1| + |2| = 3 \\\\\n",
      "        .       \\| r(-1) \\|_{L_2} &= \\sqrt{(-1)^{2} + (2)^{2}} = \\sqrt{5} \\\\\n",
      "        .       \\| r(-1) \\|_{L_\\infty} &= \\max(|-1|,|2|) = 2\n",
      "        .   \\f}\n",
      "        .   and for \\f$r(0.5) = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}\\f$ the calculation is\n",
      "        .   \\f{align*}\n",
      "        .       \\| r(0.5) \\|_{L_1} &= |0.5| + |0.5| = 1 \\\\\n",
      "        .       \\| r(0.5) \\|_{L_2} &= \\sqrt{(0.5)^{2} + (0.5)^{2}} = \\sqrt{0.5} \\\\\n",
      "        .       \\| r(0.5) \\|_{L_\\infty} &= \\max(|0.5|,|0.5|) = 0.5.\n",
      "        .   \\f}\n",
      "        .   The following graphic shows all values for the three norm functions \\f$\\| r(x) \\|_{L_1}, \\| r(x) \\|_{L_2}\\f$ and \\f$\\| r(x) \\|_{L_\\infty}\\f$.\n",
      "        .   It is notable that the \\f$ L_{1} \\f$ norm forms the upper and the \\f$ L_{\\infty} \\f$ norm forms the lower border for the example function \\f$ r(x) \\f$.\n",
      "        .   ![Graphs for the different norm functions from the above example](pics/NormTypes_OneArray_1-2-INF.png)\n",
      "        .   \n",
      "        .   When the mask parameter is specified and it is not empty, the norm is\n",
      "        .   \n",
      "        .   If normType is not specified, #NORM_L2 is used.\n",
      "        .   calculated only over the region specified by the mask.\n",
      "        .   \n",
      "        .   Multi-channel input arrays are treated as single-channel arrays, that is,\n",
      "        .   the results for all channels are combined.\n",
      "        .   \n",
      "        .   Hamming norms can only be calculated with CV_8U depth arrays.\n",
      "        .   \n",
      "        .   @param src1 first input array.\n",
      "        .   @param normType type of the norm (see #NormTypes).\n",
      "        .   @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n",
      "        \n",
      "        \n",
      "        \n",
      "        norm(src1, src2[, normType[, mask]]) -> retval\n",
      "        .   @brief Calculates an absolute difference norm or a relative difference norm.\n",
      "        .   \n",
      "        .   This version of cv::norm calculates the absolute difference norm\n",
      "        .   or the relative difference norm of arrays src1 and src2.\n",
      "        .   The type of norm to calculate is specified using #NormTypes.\n",
      "        .   \n",
      "        .   @param src1 first input array.\n",
      "        .   @param src2 second input array of the same size and the same type as src1.\n",
      "        .   @param normType type of the norm (see #NormTypes).\n",
      "        .   @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.\n",
      "    \n",
      "    normalize(...)\n",
      "        normalize(src, dst[, alpha[, beta[, norm_type[, dtype[, mask]]]]]) -> dst\n",
      "        .   @brief Normalizes the norm or value range of an array.\n",
      "        .   \n",
      "        .   The function cv::normalize normalizes scale and shift the input array elements so that\n",
      "        .   \\f[\\| \\texttt{dst} \\| _{L_p}= \\texttt{alpha}\\f]\n",
      "        .   (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that\n",
      "        .   \\f[\\min _I  \\texttt{dst} (I)= \\texttt{alpha} , \\, \\, \\max _I  \\texttt{dst} (I)= \\texttt{beta}\\f]\n",
      "        .   \n",
      "        .   when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be\n",
      "        .   normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this\n",
      "        .   sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or\n",
      "        .   min-max but modify the whole array, you can use norm and Mat::convertTo.\n",
      "        .   \n",
      "        .   In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,\n",
      "        .   the range transformation for sparse matrices is not allowed since it can shift the zero level.\n",
      "        .   \n",
      "        .   Possible usage with some positive example data:\n",
      "        .   @code{.cpp}\n",
      "        .       vector<double> positiveData = { 2.0, 8.0, 10.0 };\n",
      "        .       vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;\n",
      "        .   \n",
      "        .       // Norm to probability (total count)\n",
      "        .       // sum(numbers) = 20.0\n",
      "        .       // 2.0      0.1     (2.0/20.0)\n",
      "        .       // 8.0      0.4     (8.0/20.0)\n",
      "        .       // 10.0     0.5     (10.0/20.0)\n",
      "        .       normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);\n",
      "        .   \n",
      "        .       // Norm to unit vector: ||positiveData|| = 1.0\n",
      "        .       // 2.0      0.15\n",
      "        .       // 8.0      0.62\n",
      "        .       // 10.0     0.77\n",
      "        .       normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);\n",
      "        .   \n",
      "        .       // Norm to max element\n",
      "        .       // 2.0      0.2     (2.0/10.0)\n",
      "        .       // 8.0      0.8     (8.0/10.0)\n",
      "        .       // 10.0     1.0     (10.0/10.0)\n",
      "        .       normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);\n",
      "        .   \n",
      "        .       // Norm to range [0.0;1.0]\n",
      "        .       // 2.0      0.0     (shift to left border)\n",
      "        .       // 8.0      0.75    (6.0/8.0)\n",
      "        .       // 10.0     1.0     (shift to right border)\n",
      "        .       normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same size as src .\n",
      "        .   @param alpha norm value to normalize to or the lower range boundary in case of the range\n",
      "        .   normalization.\n",
      "        .   @param beta upper range boundary in case of the range normalization; it is not used for the norm\n",
      "        .   normalization.\n",
      "        .   @param norm_type normalization type (see cv::NormTypes).\n",
      "        .   @param dtype when negative, the output array has the same type as src; otherwise, it has the same\n",
      "        .   number of channels as src and the depth =CV_MAT_DEPTH(dtype).\n",
      "        .   @param mask optional operation mask.\n",
      "        .   @sa norm, Mat::convertTo, SparseMat::convertTo\n",
      "    \n",
      "    patchNaNs(...)\n",
      "        patchNaNs(a[, val]) -> a\n",
      "        .   @brief converts NaN's to the given number\n",
      "    \n",
      "    pencilSketch(...)\n",
      "        pencilSketch(src[, dst1[, dst2[, sigma_s[, sigma_r[, shade_factor]]]]]) -> dst1, dst2\n",
      "        .   @brief Pencil-like non-photorealistic line drawing\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst1 Output 8-bit 1-channel image.\n",
      "        .   @param dst2 Output image with the same size and type as src.\n",
      "        .   @param sigma_s %Range between 0 to 200.\n",
      "        .   @param sigma_r %Range between 0 to 1.\n",
      "        .   @param shade_factor %Range between 0 to 0.1.\n",
      "    \n",
      "    perspectiveTransform(...)\n",
      "        perspectiveTransform(src, m[, dst]) -> dst\n",
      "        .   @brief Performs the perspective matrix transformation of vectors.\n",
      "        .   \n",
      "        .   The function cv::perspectiveTransform transforms every element of src by\n",
      "        .   treating it as a 2D or 3D vector, in the following way:\n",
      "        .   \\f[(x, y, z)  \\rightarrow (x'/w, y'/w, z'/w)\\f]\n",
      "        .   where\n",
      "        .   \\f[(x', y', z', w') =  \\texttt{mat} \\cdot \\begin{bmatrix} x & y & z & 1  \\end{bmatrix}\\f]\n",
      "        .   and\n",
      "        .   \\f[w =  \\fork{w'}{if \\(w' \\ne 0\\)}{\\infty}{otherwise}\\f]\n",
      "        .   \n",
      "        .   Here a 3D vector transformation is shown. In case of a 2D vector\n",
      "        .   transformation, the z component is omitted.\n",
      "        .   \n",
      "        .   @note The function transforms a sparse set of 2D or 3D vectors. If you\n",
      "        .   want to transform an image using perspective transformation, use\n",
      "        .   warpPerspective . If you have an inverse problem, that is, you want to\n",
      "        .   compute the most probable perspective transformation out of several\n",
      "        .   pairs of corresponding points, you can use getPerspectiveTransform or\n",
      "        .   findHomography .\n",
      "        .   @param src input two-channel or three-channel floating-point array; each\n",
      "        .   element is a 2D/3D vector to be transformed.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @param m 3x3 or 4x4 floating-point transformation matrix.\n",
      "        .   @sa  transform, warpPerspective, getPerspectiveTransform, findHomography\n",
      "    \n",
      "    phase(...)\n",
      "        phase(x, y[, angle[, angleInDegrees]]) -> angle\n",
      "        .   @brief Calculates the rotation angle of 2D vectors.\n",
      "        .   \n",
      "        .   The function cv::phase calculates the rotation angle of each 2D vector that\n",
      "        .   is formed from the corresponding elements of x and y :\n",
      "        .   \\f[\\texttt{angle} (I) =  \\texttt{atan2} ( \\texttt{y} (I), \\texttt{x} (I))\\f]\n",
      "        .   \n",
      "        .   The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 ,\n",
      "        .   the corresponding angle(I) is set to 0.\n",
      "        .   @param x input floating-point array of x-coordinates of 2D vectors.\n",
      "        .   @param y input array of y-coordinates of 2D vectors; it must have the\n",
      "        .   same size and the same type as x.\n",
      "        .   @param angle output array of vector angles; it has the same size and\n",
      "        .   same type as x .\n",
      "        .   @param angleInDegrees when true, the function calculates the angle in\n",
      "        .   degrees, otherwise, they are measured in radians.\n",
      "    \n",
      "    phaseCorrelate(...)\n",
      "        phaseCorrelate(src1, src2[, window]) -> retval, response\n",
      "        .   @brief The function is used to detect translational shifts that occur between two images.\n",
      "        .   \n",
      "        .   The operation takes advantage of the Fourier shift theorem for detecting the translational shift in\n",
      "        .   the frequency domain. It can be used for fast image registration as well as motion estimation. For\n",
      "        .   more information please see <http://en.wikipedia.org/wiki/Phase_correlation>\n",
      "        .   \n",
      "        .   Calculates the cross-power spectrum of two supplied source arrays. The arrays are padded if needed\n",
      "        .   with getOptimalDFTSize.\n",
      "        .   \n",
      "        .   The function performs the following equations:\n",
      "        .   - First it applies a Hanning window (see <http://en.wikipedia.org/wiki/Hann_function>) to each\n",
      "        .   image to remove possible edge effects. This window is cached until the array size changes to speed\n",
      "        .   up processing time.\n",
      "        .   - Next it computes the forward DFTs of each source array:\n",
      "        .   \\f[\\mathbf{G}_a = \\mathcal{F}\\{src_1\\}, \\; \\mathbf{G}_b = \\mathcal{F}\\{src_2\\}\\f]\n",
      "        .   where \\f$\\mathcal{F}\\f$ is the forward DFT.\n",
      "        .   - It then computes the cross-power spectrum of each frequency domain array:\n",
      "        .   \\f[R = \\frac{ \\mathbf{G}_a \\mathbf{G}_b^*}{|\\mathbf{G}_a \\mathbf{G}_b^*|}\\f]\n",
      "        .   - Next the cross-correlation is converted back into the time domain via the inverse DFT:\n",
      "        .   \\f[r = \\mathcal{F}^{-1}\\{R\\}\\f]\n",
      "        .   - Finally, it computes the peak location and computes a 5x5 weighted centroid around the peak to\n",
      "        .   achieve sub-pixel accuracy.\n",
      "        .   \\f[(\\Delta x, \\Delta y) = \\texttt{weightedCentroid} \\{\\arg \\max_{(x, y)}\\{r\\}\\}\\f]\n",
      "        .   - If non-zero, the response parameter is computed as the sum of the elements of r within the 5x5\n",
      "        .   centroid around the peak location. It is normalized to a maximum of 1 (meaning there is a single\n",
      "        .   peak) and will be smaller when there are multiple peaks.\n",
      "        .   \n",
      "        .   @param src1 Source floating point array (CV_32FC1 or CV_64FC1)\n",
      "        .   @param src2 Source floating point array (CV_32FC1 or CV_64FC1)\n",
      "        .   @param window Floating point array with windowing coefficients to reduce edge effects (optional).\n",
      "        .   @param response Signal power within the 5x5 centroid around the peak, between 0 and 1 (optional).\n",
      "        .   @returns detected phase shift (sub-pixel) between the two arrays.\n",
      "        .   \n",
      "        .   @sa dft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow\n",
      "    \n",
      "    pointPolygonTest(...)\n",
      "        pointPolygonTest(contour, pt, measureDist) -> retval\n",
      "        .   @brief Performs a point-in-contour test.\n",
      "        .   \n",
      "        .   The function determines whether the point is inside a contour, outside, or lies on an edge (or\n",
      "        .   coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge)\n",
      "        .   value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively.\n",
      "        .   Otherwise, the return value is a signed distance between the point and the nearest contour edge.\n",
      "        .   \n",
      "        .   See below a sample output of the function where each image pixel is tested against the contour:\n",
      "        .   \n",
      "        .   ![sample output](pics/pointpolygon.png)\n",
      "        .   \n",
      "        .   @param contour Input contour.\n",
      "        .   @param pt Point tested against the contour.\n",
      "        .   @param measureDist If true, the function estimates the signed distance from the point to the\n",
      "        .   nearest contour edge. Otherwise, the function only checks if the point is inside a contour or not.\n",
      "    \n",
      "    polarToCart(...)\n",
      "        polarToCart(magnitude, angle[, x[, y[, angleInDegrees]]]) -> x, y\n",
      "        .   @brief Calculates x and y coordinates of 2D vectors from their magnitude and angle.\n",
      "        .   \n",
      "        .   The function cv::polarToCart calculates the Cartesian coordinates of each 2D\n",
      "        .   vector represented by the corresponding elements of magnitude and angle:\n",
      "        .   \\f[\\begin{array}{l} \\texttt{x} (I) =  \\texttt{magnitude} (I) \\cos ( \\texttt{angle} (I)) \\\\ \\texttt{y} (I) =  \\texttt{magnitude} (I) \\sin ( \\texttt{angle} (I)) \\\\ \\end{array}\\f]\n",
      "        .   \n",
      "        .   The relative accuracy of the estimated coordinates is about 1e-6.\n",
      "        .   @param magnitude input floating-point array of magnitudes of 2D vectors;\n",
      "        .   it can be an empty matrix (=Mat()), in this case, the function assumes\n",
      "        .   that all the magnitudes are =1; if it is not empty, it must have the\n",
      "        .   same size and type as angle.\n",
      "        .   @param angle input floating-point array of angles of 2D vectors.\n",
      "        .   @param x output array of x-coordinates of 2D vectors; it has the same\n",
      "        .   size and type as angle.\n",
      "        .   @param y output array of y-coordinates of 2D vectors; it has the same\n",
      "        .   size and type as angle.\n",
      "        .   @param angleInDegrees when true, the input angles are measured in\n",
      "        .   degrees, otherwise, they are measured in radians.\n",
      "        .   @sa cartToPolar, magnitude, phase, exp, log, pow, sqrt\n",
      "    \n",
      "    polylines(...)\n",
      "        polylines(img, pts, isClosed, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @brief Draws several polygonal curves.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param pts Array of polygonal curves.\n",
      "        .   @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,\n",
      "        .   the function draws a line from the last vertex of each curve to its first vertex.\n",
      "        .   @param color Polyline color.\n",
      "        .   @param thickness Thickness of the polyline edges.\n",
      "        .   @param lineType Type of the line segments. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the vertex coordinates.\n",
      "        .   \n",
      "        .   The function cv::polylines draws one or more polygonal curves.\n",
      "    \n",
      "    pow(...)\n",
      "        pow(src, power[, dst]) -> dst\n",
      "        .   @brief Raises every array element to a power.\n",
      "        .   \n",
      "        .   The function cv::pow raises every element of the input array to power :\n",
      "        .   \\f[\\texttt{dst} (I) =  \\fork{\\texttt{src}(I)^{power}}{if \\(\\texttt{power}\\) is integer}{|\\texttt{src}(I)|^{power}}{otherwise}\\f]\n",
      "        .   \n",
      "        .   So, for a non-integer power exponent, the absolute values of input array\n",
      "        .   elements are used. However, it is possible to get true values for\n",
      "        .   negative values using some extra operations. In the example below,\n",
      "        .   computing the 5th root of array src shows:\n",
      "        .   @code{.cpp}\n",
      "        .       Mat mask = src < 0;\n",
      "        .       pow(src, 1./5, dst);\n",
      "        .       subtract(Scalar::all(0), dst, dst, mask);\n",
      "        .   @endcode\n",
      "        .   For some values of power, such as integer values, 0.5 and -0.5,\n",
      "        .   specialized faster algorithms are used.\n",
      "        .   \n",
      "        .   Special values (NaN, Inf) are not handled.\n",
      "        .   @param src input array.\n",
      "        .   @param power exponent of power.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @sa sqrt, exp, log, cartToPolar, polarToCart\n",
      "    \n",
      "    preCornerDetect(...)\n",
      "        preCornerDetect(src, ksize[, dst[, borderType]]) -> dst\n",
      "        .   @brief Calculates a feature map for corner detection.\n",
      "        .   \n",
      "        .   The function calculates the complex spatial derivative-based function of the source image\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} = (D_x  \\texttt{src} )^2  \\cdot D_{yy}  \\texttt{src} + (D_y  \\texttt{src} )^2  \\cdot D_{xx}  \\texttt{src} - 2 D_x  \\texttt{src} \\cdot D_y  \\texttt{src} \\cdot D_{xy}  \\texttt{src}\\f]\n",
      "        .   \n",
      "        .   where \\f$D_x\\f$,\\f$D_y\\f$ are the first image derivatives, \\f$D_{xx}\\f$,\\f$D_{yy}\\f$ are the second image\n",
      "        .   derivatives, and \\f$D_{xy}\\f$ is the mixed derivative.\n",
      "        .   \n",
      "        .   The corners can be found as local maximums of the functions, as shown below:\n",
      "        .   @code\n",
      "        .       Mat corners, dilated_corners;\n",
      "        .       preCornerDetect(image, corners, 3);\n",
      "        .       // dilation with 3x3 rectangular structuring element\n",
      "        .       dilate(corners, dilated_corners, Mat(), 1);\n",
      "        .       Mat corner_mask = corners == dilated_corners;\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @param src Source single-channel 8-bit of floating-point image.\n",
      "        .   @param dst Output image that has the type CV_32F and the same size as src .\n",
      "        .   @param ksize %Aperture size of the Sobel .\n",
      "        .   @param borderType Pixel extrapolation method. See #BorderTypes.\n",
      "    \n",
      "    projectPoints(...)\n",
      "        projectPoints(objectPoints, rvec, tvec, cameraMatrix, distCoeffs[, imagePoints[, jacobian[, aspectRatio]]]) -> imagePoints, jacobian\n",
      "        .   @brief Projects 3D points to an image plane.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points, 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or\n",
      "        .   vector\\<Point3f\\> ), where N is the number of points in the view.\n",
      "        .   @param rvec Rotation vector. See Rodrigues for details.\n",
      "        .   @param tvec Translation vector.\n",
      "        .   @param cameraMatrix Camera matrix \\f$A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is empty, the zero distortion coefficients are assumed.\n",
      "        .   @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or\n",
      "        .   vector\\<Point2f\\> .\n",
      "        .   @param jacobian Optional output 2Nx(10+\\<numDistCoeffs\\>) jacobian matrix of derivatives of image\n",
      "        .   points with respect to components of the rotation vector, translation vector, focal lengths,\n",
      "        .   coordinates of the principal point and the distortion coefficients. In the old interface different\n",
      "        .   components of the jacobian are returned via different output parameters.\n",
      "        .   @param aspectRatio Optional \"fixed aspect ratio\" parameter. If the parameter is not 0, the\n",
      "        .   function assumes that the aspect ratio (*fx/fy*) is fixed and correspondingly adjusts the jacobian\n",
      "        .   matrix.\n",
      "        .   \n",
      "        .   The function computes projections of 3D points to the image plane given intrinsic and extrinsic\n",
      "        .   camera parameters. Optionally, the function computes Jacobians - matrices of partial derivatives of\n",
      "        .   image points coordinates (as functions of all the input parameters) with respect to the particular\n",
      "        .   parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in\n",
      "        .   calibrateCamera, solvePnP, and stereoCalibrate . The function itself can also be used to compute a\n",
      "        .   re-projection error given the current intrinsic and extrinsic parameters.\n",
      "        .   \n",
      "        .   @note By setting rvec=tvec=(0,0,0) or by setting cameraMatrix to a 3x3 identity matrix, or by\n",
      "        .   passing zero distortion coefficients, you can get various useful partial cases of the function. This\n",
      "        .   means that you can compute the distorted coordinates for a sparse set of points or apply a\n",
      "        .   perspective transformation (and also compute the derivatives) in the ideal zero-distortion setup.\n",
      "    \n",
      "    putText(...)\n",
      "        putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]) -> img\n",
      "        .   @brief Draws a text string.\n",
      "        .   \n",
      "        .   The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered\n",
      "        .   using the specified font are replaced by question marks. See #getTextSize for a text rendering code\n",
      "        .   example.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param text Text string to be drawn.\n",
      "        .   @param org Bottom-left corner of the text string in the image.\n",
      "        .   @param fontFace Font type, see #HersheyFonts.\n",
      "        .   @param fontScale Font scale factor that is multiplied by the font-specific base size.\n",
      "        .   @param color Text color.\n",
      "        .   @param thickness Thickness of the lines used to draw a text.\n",
      "        .   @param lineType Line type. See #LineTypes\n",
      "        .   @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise,\n",
      "        .   it is at the top-left corner.\n",
      "    \n",
      "    pyrDown(...)\n",
      "        pyrDown(src[, dst[, dstsize[, borderType]]]) -> dst\n",
      "        .   @brief Blurs an image and downsamples it.\n",
      "        .   \n",
      "        .   By default, size of the output image is computed as `Size((src.cols+1)/2, (src.rows+1)/2)`, but in\n",
      "        .   any case, the following conditions should be satisfied:\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} | \\texttt{dstsize.width} *2-src.cols| \\leq 2 \\\\ | \\texttt{dstsize.height} *2-src.rows| \\leq 2 \\end{array}\\f]\n",
      "        .   \n",
      "        .   The function performs the downsampling step of the Gaussian pyramid construction. First, it\n",
      "        .   convolves the source image with the kernel:\n",
      "        .   \n",
      "        .   \\f[\\frac{1}{256} \\begin{bmatrix} 1 & 4 & 6 & 4 & 1  \\\\ 4 & 16 & 24 & 16 & 4  \\\\ 6 & 24 & 36 & 24 & 6  \\\\ 4 & 16 & 24 & 16 & 4  \\\\ 1 & 4 & 6 & 4 & 1 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .   Then, it downsamples the image by rejecting even rows and columns.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image; it has the specified size and the same type as src.\n",
      "        .   @param dstsize size of the output image.\n",
      "        .   @param borderType Pixel extrapolation method, see #BorderTypes (#BORDER_CONSTANT isn't supported)\n",
      "    \n",
      "    pyrMeanShiftFiltering(...)\n",
      "        pyrMeanShiftFiltering(src, sp, sr[, dst[, maxLevel[, termcrit]]]) -> dst\n",
      "        .   @brief Performs initial step of meanshift segmentation of an image.\n",
      "        .   \n",
      "        .   The function implements the filtering stage of meanshift segmentation, that is, the output of the\n",
      "        .   function is the filtered \"posterized\" image with color gradients and fine-grain texture flattened.\n",
      "        .   At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes\n",
      "        .   meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is\n",
      "        .   considered:\n",
      "        .   \n",
      "        .   \\f[(x,y): X- \\texttt{sp} \\le x  \\le X+ \\texttt{sp} , Y- \\texttt{sp} \\le y  \\le Y+ \\texttt{sp} , ||(R,G,B)-(r,g,b)||   \\le \\texttt{sr}\\f]\n",
      "        .   \n",
      "        .   where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively\n",
      "        .   (though, the algorithm does not depend on the color space used, so any 3-component color space can\n",
      "        .   be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector\n",
      "        .   (R',G',B') are found and they act as the neighborhood center on the next iteration:\n",
      "        .   \n",
      "        .   \\f[(X,Y)~(X',Y'), (R,G,B)~(R',G',B').\\f]\n",
      "        .   \n",
      "        .   After the iterations over, the color components of the initial pixel (that is, the pixel from where\n",
      "        .   the iterations started) are set to the final value (average color at the last iteration):\n",
      "        .   \n",
      "        .   \\f[I(X,Y) <- (R*,G*,B*)\\f]\n",
      "        .   \n",
      "        .   When maxLevel \\> 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is\n",
      "        .   run on the smallest layer first. After that, the results are propagated to the larger layer and the\n",
      "        .   iterations are run again only on those pixels where the layer colors differ by more than sr from the\n",
      "        .   lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the\n",
      "        .   results will be actually different from the ones obtained by running the meanshift procedure on the\n",
      "        .   whole original image (i.e. when maxLevel==0).\n",
      "        .   \n",
      "        .   @param src The source 8-bit, 3-channel image.\n",
      "        .   @param dst The destination image of the same format and the same size as the source.\n",
      "        .   @param sp The spatial window radius.\n",
      "        .   @param sr The color window radius.\n",
      "        .   @param maxLevel Maximum level of the pyramid for the segmentation.\n",
      "        .   @param termcrit Termination criteria: when to stop meanshift iterations.\n",
      "    \n",
      "    pyrUp(...)\n",
      "        pyrUp(src[, dst[, dstsize[, borderType]]]) -> dst\n",
      "        .   @brief Upsamples an image and then blurs it.\n",
      "        .   \n",
      "        .   By default, size of the output image is computed as `Size(src.cols\\*2, (src.rows\\*2)`, but in any\n",
      "        .   case, the following conditions should be satisfied:\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} | \\texttt{dstsize.width} -src.cols*2| \\leq  ( \\texttt{dstsize.width}   \\mod  2)  \\\\ | \\texttt{dstsize.height} -src.rows*2| \\leq  ( \\texttt{dstsize.height}   \\mod  2) \\end{array}\\f]\n",
      "        .   \n",
      "        .   The function performs the upsampling step of the Gaussian pyramid construction, though it can\n",
      "        .   actually be used to construct the Laplacian pyramid. First, it upsamples the source image by\n",
      "        .   injecting even zero rows and columns and then convolves the result with the same kernel as in\n",
      "        .   pyrDown multiplied by 4.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image. It has the specified size and the same type as src .\n",
      "        .   @param dstsize size of the output image.\n",
      "        .   @param borderType Pixel extrapolation method, see #BorderTypes (only #BORDER_DEFAULT is supported)\n",
      "    \n",
      "    randShuffle(...)\n",
      "        randShuffle(dst[, iterFactor]) -> dst\n",
      "        .   @brief Shuffles the array elements randomly.\n",
      "        .   \n",
      "        .   The function cv::randShuffle shuffles the specified 1D array by randomly choosing pairs of elements and\n",
      "        .   swapping them. The number of such swap operations will be dst.rows\\*dst.cols\\*iterFactor .\n",
      "        .   @param dst input/output numerical 1D array.\n",
      "        .   @param iterFactor scale factor that determines the number of random swap operations (see the details\n",
      "        .   below).\n",
      "        .   @param rng optional random number generator used for shuffling; if it is zero, theRNG () is used\n",
      "        .   instead.\n",
      "        .   @sa RNG, sort\n",
      "    \n",
      "    randn(...)\n",
      "        randn(dst, mean, stddev) -> dst\n",
      "        .   @brief Fills the array with normally distributed random numbers.\n",
      "        .   \n",
      "        .   The function cv::randn fills the matrix dst with normally distributed random numbers with the specified\n",
      "        .   mean vector and the standard deviation matrix. The generated random numbers are clipped to fit the\n",
      "        .   value range of the output array data type.\n",
      "        .   @param dst output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.\n",
      "        .   @param mean mean value (expectation) of the generated random numbers.\n",
      "        .   @param stddev standard deviation of the generated random numbers; it can be either a vector (in\n",
      "        .   which case a diagonal standard deviation matrix is assumed) or a square matrix.\n",
      "        .   @sa RNG, randu\n",
      "    \n",
      "    randu(...)\n",
      "        randu(dst, low, high) -> dst\n",
      "        .   @brief Generates a single uniformly-distributed random number or an array of random numbers.\n",
      "        .   \n",
      "        .   Non-template variant of the function fills the matrix dst with uniformly-distributed\n",
      "        .   random numbers from the specified range:\n",
      "        .   \\f[\\texttt{low} _c  \\leq \\texttt{dst} (I)_c <  \\texttt{high} _c\\f]\n",
      "        .   @param dst output array of random numbers; the array must be pre-allocated.\n",
      "        .   @param low inclusive lower boundary of the generated random numbers.\n",
      "        .   @param high exclusive upper boundary of the generated random numbers.\n",
      "        .   @sa RNG, randn, theRNG\n",
      "    \n",
      "    readOpticalFlow(...)\n",
      "        readOpticalFlow(path) -> retval\n",
      "        .   @brief Read a .flo file\n",
      "        .   \n",
      "        .    @param path Path to the file to be loaded\n",
      "        .   \n",
      "        .    The function readOpticalFlow loads a flow field from a file and returns it as a single matrix.\n",
      "        .    Resulting Mat has a type CV_32FC2 - floating-point, 2-channel. First channel corresponds to the\n",
      "        .    flow in the horizontal direction (u), second - vertical (v).\n",
      "    \n",
      "    recoverPose(...)\n",
      "        recoverPose(E, points1, points2, cameraMatrix[, R[, t[, mask]]]) -> retval, R, t, mask\n",
      "        .   @brief Recover relative camera rotation and translation from an estimated essential matrix and the\n",
      "        .   corresponding points in two images, using cheirality check. Returns the number of inliers which pass\n",
      "        .   the check.\n",
      "        .   \n",
      "        .   @param E The input essential matrix.\n",
      "        .   @param points1 Array of N 2D points from the first image. The point coordinates should be\n",
      "        .   floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1 .\n",
      "        .   @param cameraMatrix Camera matrix \\f$K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   Note that this function assumes that points1 and points2 are feature points from cameras with the\n",
      "        .   same camera matrix.\n",
      "        .   @param R Recovered relative rotation.\n",
      "        .   @param t Recovered relative translation.\n",
      "        .   @param mask Input/output mask for inliers in points1 and points2.\n",
      "        .   :   If it is not empty, then it marks inliers in points1 and points2 for then given essential\n",
      "        .   matrix E. Only these inliers will be used to recover pose. In the output mask only inliers\n",
      "        .   which pass the cheirality check.\n",
      "        .   This function decomposes an essential matrix using decomposeEssentialMat and then verifies possible\n",
      "        .   pose hypotheses by doing cheirality check. The cheirality check basically means that the\n",
      "        .   triangulated 3D points should have positive depth. Some details can be found in @cite Nister03 .\n",
      "        .   \n",
      "        .   This function can be used to process output E and mask from findEssentialMat. In this scenario,\n",
      "        .   points1 and points2 are the same input for findEssentialMat. :\n",
      "        .   @code\n",
      "        .       // Example. Estimation of fundamental matrix using the RANSAC algorithm\n",
      "        .       int point_count = 100;\n",
      "        .       vector<Point2f> points1(point_count);\n",
      "        .       vector<Point2f> points2(point_count);\n",
      "        .   \n",
      "        .       // initialize the points here ...\n",
      "        .       for( int i = 0; i < point_count; i++ )\n",
      "        .       {\n",
      "        .           points1[i] = ...;\n",
      "        .           points2[i] = ...;\n",
      "        .       }\n",
      "        .   \n",
      "        .       // cametra matrix with both focal lengths = 1, and principal point = (0, 0)\n",
      "        .       Mat cameraMatrix = Mat::eye(3, 3, CV_64F);\n",
      "        .   \n",
      "        .       Mat E, R, t, mask;\n",
      "        .   \n",
      "        .       E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);\n",
      "        .       recoverPose(E, points1, points2, cameraMatrix, R, t, mask);\n",
      "        .   @endcode\n",
      "        \n",
      "        \n",
      "        \n",
      "        recoverPose(E, points1, points2[, R[, t[, focal[, pp[, mask]]]]]) -> retval, R, t, mask\n",
      "        .   @overload\n",
      "        .   @param E The input essential matrix.\n",
      "        .   @param points1 Array of N 2D points from the first image. The point coordinates should be\n",
      "        .   floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1 .\n",
      "        .   @param R Recovered relative rotation.\n",
      "        .   @param t Recovered relative translation.\n",
      "        .   @param focal Focal length of the camera. Note that this function assumes that points1 and points2\n",
      "        .   are feature points from cameras with same focal length and principal point.\n",
      "        .   @param pp principal point of the camera.\n",
      "        .   @param mask Input/output mask for inliers in points1 and points2.\n",
      "        .   :   If it is not empty, then it marks inliers in points1 and points2 for then given essential\n",
      "        .   matrix E. Only these inliers will be used to recover pose. In the output mask only inliers\n",
      "        .   which pass the cheirality check.\n",
      "        .   \n",
      "        .   This function differs from the one above that it computes camera matrix from focal length and\n",
      "        .   principal point:\n",
      "        .   \n",
      "        .   \\f[K =\n",
      "        .   \\begin{bmatrix}\n",
      "        .   f & 0 & x_{pp}  \\\\\n",
      "        .   0 & f & y_{pp}  \\\\\n",
      "        .   0 & 0 & 1\n",
      "        .   \\end{bmatrix}\\f]\n",
      "        \n",
      "        \n",
      "        \n",
      "        recoverPose(E, points1, points2, cameraMatrix, distanceThresh[, R[, t[, mask[, triangulatedPoints]]]]) -> retval, R, t, mask, triangulatedPoints\n",
      "        .   @overload\n",
      "        .   @param E The input essential matrix.\n",
      "        .   @param points1 Array of N 2D points from the first image. The point coordinates should be\n",
      "        .   floating-point (single or double precision).\n",
      "        .   @param points2 Array of the second image points of the same size and format as points1.\n",
      "        .   @param cameraMatrix Camera matrix \\f$K = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   Note that this function assumes that points1 and points2 are feature points from cameras with the\n",
      "        .   same camera matrix.\n",
      "        .   @param R Recovered relative rotation.\n",
      "        .   @param t Recovered relative translation.\n",
      "        .   @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite points).\n",
      "        .   @param mask Input/output mask for inliers in points1 and points2.\n",
      "        .   :   If it is not empty, then it marks inliers in points1 and points2 for then given essential\n",
      "        .   matrix E. Only these inliers will be used to recover pose. In the output mask only inliers\n",
      "        .   which pass the cheirality check.\n",
      "        .   @param triangulatedPoints 3d points which were reconstructed by triangulation.\n",
      "    \n",
      "    rectangle(...)\n",
      "        rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @brief Draws a simple, thick, or filled up-right rectangle.\n",
      "        .   \n",
      "        .   The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners\n",
      "        .   are pt1 and pt2.\n",
      "        .   \n",
      "        .   @param img Image.\n",
      "        .   @param pt1 Vertex of the rectangle.\n",
      "        .   @param pt2 Vertex of the rectangle opposite to pt1 .\n",
      "        .   @param color Rectangle color or brightness (grayscale image).\n",
      "        .   @param thickness Thickness of lines that make up the rectangle. Negative values, like #FILLED,\n",
      "        .   mean that the function has to draw a filled rectangle.\n",
      "        .   @param lineType Type of the line. See #LineTypes\n",
      "        .   @param shift Number of fractional bits in the point coordinates.\n",
      "        \n",
      "        \n",
      "        \n",
      "        rectangle(img, rec, color[, thickness[, lineType[, shift]]]) -> img\n",
      "        .   @overload\n",
      "        .   \n",
      "        .   use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and\n",
      "        .   r.br()-Point(1,1)` are opposite corners\n",
      "    \n",
      "    rectify3Collinear(...)\n",
      "        rectify3Collinear(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, cameraMatrix3, distCoeffs3, imgpt1, imgpt3, imageSize, R12, T12, R13, T13, alpha, newImgSize, flags[, R1[, R2[, R3[, P1[, P2[, P3[, Q]]]]]]]) -> retval, R1, R2, R3, P1, P2, P3, Q, roi1, roi2\n",
      "        .\n",
      "    \n",
      "    redirectError(...)\n",
      "        redirectError(onError) -> None\n",
      "    \n",
      "    reduce(...)\n",
      "        reduce(src, dim, rtype[, dst[, dtype]]) -> dst\n",
      "        .   @brief Reduces a matrix to a vector.\n",
      "        .   \n",
      "        .   The function #reduce reduces the matrix to a vector by treating the matrix rows/columns as a set of\n",
      "        .   1D vectors and performing the specified operation on the vectors until a single row/column is\n",
      "        .   obtained. For example, the function can be used to compute horizontal and vertical projections of a\n",
      "        .   raster image. In case of #REDUCE_MAX and #REDUCE_MIN , the output image should have the same type as the source one.\n",
      "        .   In case of #REDUCE_SUM and #REDUCE_AVG , the output may have a larger element bit-depth to preserve accuracy.\n",
      "        .   And multi-channel arrays are also supported in these two reduction modes.\n",
      "        .   \n",
      "        .   The following code demonstrates its usage for a single channel matrix.\n",
      "        .   @snippet snippets/core_reduce.cpp example\n",
      "        .   \n",
      "        .   And the following code demonstrates its usage for a two-channel matrix.\n",
      "        .   @snippet snippets/core_reduce.cpp example2\n",
      "        .   \n",
      "        .   @param src input 2D matrix.\n",
      "        .   @param dst output vector. Its size and type is defined by dim and dtype parameters.\n",
      "        .   @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to\n",
      "        .   a single row. 1 means that the matrix is reduced to a single column.\n",
      "        .   @param rtype reduction operation that could be one of #ReduceTypes\n",
      "        .   @param dtype when negative, the output vector will have the same type as the input matrix,\n",
      "        .   otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).\n",
      "        .   @sa repeat\n",
      "    \n",
      "    remap(...)\n",
      "        remap(src, map1, map2, interpolation[, dst[, borderMode[, borderValue]]]) -> dst\n",
      "        .   @brief Applies a generic geometrical transformation to an image.\n",
      "        .   \n",
      "        .   The function remap transforms the source image using the specified map:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\texttt{src} (map_x(x,y),map_y(x,y))\\f]\n",
      "        .   \n",
      "        .   where values of pixels with non-integer coordinates are computed using one of available\n",
      "        .   interpolation methods. \\f$map_x\\f$ and \\f$map_y\\f$ can be encoded as separate floating-point maps\n",
      "        .   in \\f$map_1\\f$ and \\f$map_2\\f$ respectively, or interleaved floating-point maps of \\f$(x,y)\\f$ in\n",
      "        .   \\f$map_1\\f$, or fixed-point maps created by using convertMaps. The reason you might want to\n",
      "        .   convert from floating to fixed-point representations of a map is that they can yield much faster\n",
      "        .   (\\~2x) remapping operations. In the converted case, \\f$map_1\\f$ contains pairs (cvFloor(x),\n",
      "        .   cvFloor(y)) and \\f$map_2\\f$ contains indices in a table of interpolation coefficients.\n",
      "        .   \n",
      "        .   This function cannot operate in-place.\n",
      "        .   \n",
      "        .   @param src Source image.\n",
      "        .   @param dst Destination image. It has the same size as map1 and the same type as src .\n",
      "        .   @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,\n",
      "        .   CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point\n",
      "        .   representation to fixed-point for speed.\n",
      "        .   @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map\n",
      "        .   if map1 is (x,y) points), respectively.\n",
      "        .   @param interpolation Interpolation method (see #InterpolationFlags). The method #INTER_AREA is\n",
      "        .   not supported by this function.\n",
      "        .   @param borderMode Pixel extrapolation method (see #BorderTypes). When\n",
      "        .   borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image that\n",
      "        .   corresponds to the \"outliers\" in the source image are not modified by the function.\n",
      "        .   @param borderValue Value used in case of a constant border. By default, it is 0.\n",
      "        .   @note\n",
      "        .   Due to current implementation limitations the size of an input and output images should be less than 32767x32767.\n",
      "    \n",
      "    repeat(...)\n",
      "        repeat(src, ny, nx[, dst]) -> dst\n",
      "        .   @brief Fills the output array with repeated copies of the input array.\n",
      "        .   \n",
      "        .   The function cv::repeat duplicates the input array one or more times along each of the two axes:\n",
      "        .   \\f[\\texttt{dst} _{ij}= \\texttt{src} _{i\\mod src.rows, \\; j\\mod src.cols }\\f]\n",
      "        .   The second variant of the function is more convenient to use with @ref MatrixExpressions.\n",
      "        .   @param src input array to replicate.\n",
      "        .   @param ny Flag to specify how many times the `src` is repeated along the\n",
      "        .   vertical axis.\n",
      "        .   @param nx Flag to specify how many times the `src` is repeated along the\n",
      "        .   horizontal axis.\n",
      "        .   @param dst output array of the same type as `src`.\n",
      "        .   @sa cv::reduce\n",
      "    \n",
      "    reprojectImageTo3D(...)\n",
      "        reprojectImageTo3D(disparity, Q[, _3dImage[, handleMissingValues[, ddepth]]]) -> _3dImage\n",
      "        .   @brief Reprojects a disparity image to 3D space.\n",
      "        .   \n",
      "        .   @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit\n",
      "        .   floating-point disparity image. If 16-bit signed format is used, the values are assumed to have no\n",
      "        .   fractional bits.\n",
      "        .   @param _3dImage Output 3-channel floating-point image of the same size as disparity . Each\n",
      "        .   element of _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity\n",
      "        .   map.\n",
      "        .   @param Q \\f$4 \\times 4\\f$ perspective transformation matrix that can be obtained with stereoRectify.\n",
      "        .   @param handleMissingValues Indicates, whether the function should handle missing values (i.e.\n",
      "        .   points where the disparity was not computed). If handleMissingValues=true, then pixels with the\n",
      "        .   minimal disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed\n",
      "        .   to 3D points with a very large Z value (currently set to 10000).\n",
      "        .   @param ddepth The optional output array depth. If it is -1, the output image will have CV_32F\n",
      "        .   depth. ddepth can also be set to CV_16S, CV_32S or CV_32F.\n",
      "        .   \n",
      "        .   The function transforms a single-channel disparity map to a 3-channel image representing a 3D\n",
      "        .   surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it\n",
      "        .   computes:\n",
      "        .   \n",
      "        .   \\f[\\begin{array}{l} [X \\; Y \\; Z \\; W]^T =  \\texttt{Q} *[x \\; y \\; \\texttt{disparity} (x,y) \\; 1]^T  \\\\ \\texttt{\\_3dImage} (x,y) = (X/W, \\; Y/W, \\; Z/W) \\end{array}\\f]\n",
      "        .   \n",
      "        .   The matrix Q can be an arbitrary \\f$4 \\times 4\\f$ matrix (for example, the one computed by\n",
      "        .   stereoRectify). To reproject a sparse set of points {(x,y,d),...} to 3D space, use\n",
      "        .   perspectiveTransform .\n",
      "    \n",
      "    resize(...)\n",
      "        resize(src, dsize[, dst[, fx[, fy[, interpolation]]]]) -> dst\n",
      "        .   @brief Resizes an image.\n",
      "        .   \n",
      "        .   The function resize resizes the image src down to or up to the specified size. Note that the\n",
      "        .   initial dst type or size are not taken into account. Instead, the size and type are derived from\n",
      "        .   the `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst,\n",
      "        .   you may call the function as follows:\n",
      "        .   @code\n",
      "        .       // explicitly specify dsize=dst.size(); fx and fy will be computed from that.\n",
      "        .       resize(src, dst, dst.size(), 0, 0, interpolation);\n",
      "        .   @endcode\n",
      "        .   If you want to decimate the image by factor of 2 in each direction, you can call the function this\n",
      "        .   way:\n",
      "        .   @code\n",
      "        .       // specify fx and fy and let the function compute the destination image size.\n",
      "        .       resize(src, dst, Size(), 0.5, 0.5, interpolation);\n",
      "        .   @endcode\n",
      "        .   To shrink an image, it will generally look best with #INTER_AREA interpolation, whereas to\n",
      "        .   enlarge an image, it will generally look best with c#INTER_CUBIC (slow) or #INTER_LINEAR\n",
      "        .   (faster but still looks OK).\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image; it has the size dsize (when it is non-zero) or the size computed from\n",
      "        .   src.size(), fx, and fy; the type of dst is the same as of src.\n",
      "        .   @param dsize output image size; if it equals zero, it is computed as:\n",
      "        .    \\f[\\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\\f]\n",
      "        .    Either dsize or both fx and fy must be non-zero.\n",
      "        .   @param fx scale factor along the horizontal axis; when it equals 0, it is computed as\n",
      "        .   \\f[\\texttt{(double)dsize.width/src.cols}\\f]\n",
      "        .   @param fy scale factor along the vertical axis; when it equals 0, it is computed as\n",
      "        .   \\f[\\texttt{(double)dsize.height/src.rows}\\f]\n",
      "        .   @param interpolation interpolation method, see #InterpolationFlags\n",
      "        .   \n",
      "        .   @sa  warpAffine, warpPerspective, remap\n",
      "    \n",
      "    resizeWindow(...)\n",
      "        resizeWindow(winname, width, height) -> None\n",
      "        .   @brief Resizes window to the specified size\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   -   The specified window size is for the image area. Toolbars are not counted.\n",
      "        .   -   Only windows created without cv::WINDOW_AUTOSIZE flag can be resized.\n",
      "        .   \n",
      "        .   @param winname Window name.\n",
      "        .   @param width The new window width.\n",
      "        .   @param height The new window height.\n",
      "        \n",
      "        \n",
      "        \n",
      "        resizeWindow(winname, size) -> None\n",
      "        .   @overload\n",
      "        .   @param winname Window name.\n",
      "        .   @param size The new window size.\n",
      "    \n",
      "    rotate(...)\n",
      "        rotate(src, rotateCode[, dst]) -> dst\n",
      "        .   @brief Rotates a 2D array in multiples of 90 degrees.\n",
      "        .   The function cv::rotate rotates the array in one of three different ways:\n",
      "        .   *   Rotate by 90 degrees clockwise (rotateCode = ROTATE_90_CLOCKWISE).\n",
      "        .   *   Rotate by 180 degrees clockwise (rotateCode = ROTATE_180).\n",
      "        .   *   Rotate by 270 degrees clockwise (rotateCode = ROTATE_90_COUNTERCLOCKWISE).\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same type as src.  The size is the same with ROTATE_180,\n",
      "        .   and the rows and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.\n",
      "        .   @param rotateCode an enum to specify how to rotate the array; see the enum #RotateFlags\n",
      "        .   @sa transpose , repeat , completeSymm, flip, RotateFlags\n",
      "    \n",
      "    rotatedRectangleIntersection(...)\n",
      "        rotatedRectangleIntersection(rect1, rect2[, intersectingRegion]) -> retval, intersectingRegion\n",
      "        .   @brief Finds out if there is any intersection between two rotated rectangles.\n",
      "        .   \n",
      "        .   If there is then the vertices of the intersecting region are returned as well.\n",
      "        .   \n",
      "        .   Below are some examples of intersection configurations. The hatched pattern indicates the\n",
      "        .   intersecting region and the red vertices are returned by the function.\n",
      "        .   \n",
      "        .   ![intersection examples](pics/intersection.png)\n",
      "        .   \n",
      "        .   @param rect1 First rectangle\n",
      "        .   @param rect2 Second rectangle\n",
      "        .   @param intersectingRegion The output array of the vertices of the intersecting region. It returns\n",
      "        .   at most 8 vertices. Stored as std::vector\\<cv::Point2f\\> or cv::Mat as Mx1 of type CV_32FC2.\n",
      "        .   @returns One of #RectanglesIntersectTypes\n",
      "    \n",
      "    sampsonDistance(...)\n",
      "        sampsonDistance(pt1, pt2, F) -> retval\n",
      "        .   @brief Calculates the Sampson Distance between two points.\n",
      "        .   \n",
      "        .   The function cv::sampsonDistance calculates and returns the first order approximation of the geometric error as:\n",
      "        .   \\f[\n",
      "        .   sd( \\texttt{pt1} , \\texttt{pt2} )=\n",
      "        .   \\frac{(\\texttt{pt2}^t \\cdot \\texttt{F} \\cdot \\texttt{pt1})^2}\n",
      "        .   {((\\texttt{F} \\cdot \\texttt{pt1})(0))^2 +\n",
      "        .   ((\\texttt{F} \\cdot \\texttt{pt1})(1))^2 +\n",
      "        .   ((\\texttt{F}^t \\cdot \\texttt{pt2})(0))^2 +\n",
      "        .   ((\\texttt{F}^t \\cdot \\texttt{pt2})(1))^2}\n",
      "        .   \\f]\n",
      "        .   The fundamental matrix may be calculated using the cv::findFundamentalMat function. See @cite HartleyZ00 11.4.3 for details.\n",
      "        .   @param pt1 first homogeneous 2d point\n",
      "        .   @param pt2 second homogeneous 2d point\n",
      "        .   @param F fundamental matrix\n",
      "        .   @return The computed Sampson distance.\n",
      "    \n",
      "    scaleAdd(...)\n",
      "        scaleAdd(src1, alpha, src2[, dst]) -> dst\n",
      "        .   @brief Calculates the sum of a scaled array and another array.\n",
      "        .   \n",
      "        .   The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY\n",
      "        .   or SAXPY in [BLAS](http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). It calculates\n",
      "        .   the sum of a scaled array and another array:\n",
      "        .   \\f[\\texttt{dst} (I)= \\texttt{scale} \\cdot \\texttt{src1} (I) +  \\texttt{src2} (I)\\f]\n",
      "        .   The function can also be emulated with a matrix expression, for example:\n",
      "        .   @code{.cpp}\n",
      "        .       Mat A(3, 3, CV_64F);\n",
      "        .       ...\n",
      "        .       A.row(0) = A.row(1)*2 + A.row(2);\n",
      "        .   @endcode\n",
      "        .   @param src1 first input array.\n",
      "        .   @param alpha scale factor for the first array.\n",
      "        .   @param src2 second input array of the same size and type as src1.\n",
      "        .   @param dst output array of the same size and type as src1.\n",
      "        .   @sa add, addWeighted, subtract, Mat::dot, Mat::convertTo\n",
      "    \n",
      "    seamlessClone(...)\n",
      "        seamlessClone(src, dst, mask, p, flags[, blend]) -> blend\n",
      "        .   @brief Image editing tasks concern either global changes (color/intensity corrections, filters,\n",
      "        .   deformations) or local changes concerned to a selection. Here we are interested in achieving local\n",
      "        .   changes, ones that are restricted to a region manually selected (ROI), in a seamless and effortless\n",
      "        .   manner. The extent of the changes ranges from slight distortions to complete replacement by novel\n",
      "        .   content @cite PM03 .\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst Input 8-bit 3-channel image.\n",
      "        .   @param mask Input 8-bit 1 or 3-channel image.\n",
      "        .   @param p Point in dst image where object is placed.\n",
      "        .   @param blend Output image with the same size and type as dst.\n",
      "        .   @param flags Cloning method that could be cv::NORMAL_CLONE, cv::MIXED_CLONE or cv::MONOCHROME_TRANSFER\n",
      "    \n",
      "    selectROI(...)\n",
      "        selectROI(windowName, img[, showCrosshair[, fromCenter]]) -> retval\n",
      "        .   @brief Selects ROI on the given image.\n",
      "        .   Function creates a window and allows user to select a ROI using mouse.\n",
      "        .   Controls: use `space` or `enter` to finish selection, use key `c` to cancel selection (function will return the zero cv::Rect).\n",
      "        .   \n",
      "        .   @param windowName name of the window where selection process will be shown.\n",
      "        .   @param img image to select a ROI.\n",
      "        .   @param showCrosshair if true crosshair of selection rectangle will be shown.\n",
      "        .   @param fromCenter if true center of selection will match initial mouse position. In opposite case a corner of\n",
      "        .   selection rectangle will correspont to the initial mouse position.\n",
      "        .   @return selected ROI or empty rect if selection canceled.\n",
      "        .   \n",
      "        .   @note The function sets it's own mouse callback for specified window using cv::setMouseCallback(windowName, ...).\n",
      "        .   After finish of work an empty callback will be set for the used window.\n",
      "        \n",
      "        \n",
      "        \n",
      "        selectROI(img[, showCrosshair[, fromCenter]]) -> retval\n",
      "        .   @overload\n",
      "    \n",
      "    selectROIs(...)\n",
      "        selectROIs(windowName, img[, showCrosshair[, fromCenter]]) -> boundingBoxes\n",
      "        .   @brief Selects ROIs on the given image.\n",
      "        .   Function creates a window and allows user to select a ROIs using mouse.\n",
      "        .   Controls: use `space` or `enter` to finish current selection and start a new one,\n",
      "        .   use `esc` to terminate multiple ROI selection process.\n",
      "        .   \n",
      "        .   @param windowName name of the window where selection process will be shown.\n",
      "        .   @param img image to select a ROI.\n",
      "        .   @param boundingBoxes selected ROIs.\n",
      "        .   @param showCrosshair if true crosshair of selection rectangle will be shown.\n",
      "        .   @param fromCenter if true center of selection will match initial mouse position. In opposite case a corner of\n",
      "        .   selection rectangle will correspont to the initial mouse position.\n",
      "        .   \n",
      "        .   @note The function sets it's own mouse callback for specified window using cv::setMouseCallback(windowName, ...).\n",
      "        .   After finish of work an empty callback will be set for the used window.\n",
      "    \n",
      "    sepFilter2D(...)\n",
      "        sepFilter2D(src, ddepth, kernelX, kernelY[, dst[, anchor[, delta[, borderType]]]]) -> dst\n",
      "        .   @brief Applies a separable linear filter to an image.\n",
      "        .   \n",
      "        .   The function applies a separable linear filter to the image. That is, first, every row of src is\n",
      "        .   filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D\n",
      "        .   kernel kernelY. The final result shifted by delta is stored in dst .\n",
      "        .   \n",
      "        .   @param src Source image.\n",
      "        .   @param dst Destination image of the same size and the same number of channels as src .\n",
      "        .   @param ddepth Destination image depth, see @ref filter_depths \"combinations\"\n",
      "        .   @param kernelX Coefficients for filtering each row.\n",
      "        .   @param kernelY Coefficients for filtering each column.\n",
      "        .   @param anchor Anchor position within the kernel. The default value \\f$(-1,-1)\\f$ means that the anchor\n",
      "        .   is at the kernel center.\n",
      "        .   @param delta Value added to the filtered results before storing them.\n",
      "        .   @param borderType Pixel extrapolation method, see #BorderTypes\n",
      "        .   @sa  filter2D, Sobel, GaussianBlur, boxFilter, blur\n",
      "    \n",
      "    setIdentity(...)\n",
      "        setIdentity(mtx[, s]) -> mtx\n",
      "        .   @brief Initializes a scaled identity matrix.\n",
      "        .   \n",
      "        .   The function cv::setIdentity initializes a scaled identity matrix:\n",
      "        .   \\f[\\texttt{mtx} (i,j)= \\fork{\\texttt{value}}{ if \\(i=j\\)}{0}{otherwise}\\f]\n",
      "        .   \n",
      "        .   The function can also be emulated using the matrix initializers and the\n",
      "        .   matrix expressions:\n",
      "        .   @code\n",
      "        .       Mat A = Mat::eye(4, 3, CV_32F)*5;\n",
      "        .       // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]\n",
      "        .   @endcode\n",
      "        .   @param mtx matrix to initialize (not necessarily square).\n",
      "        .   @param s value to assign to diagonal elements.\n",
      "        .   @sa Mat::zeros, Mat::ones, Mat::setTo, Mat::operator=\n",
      "    \n",
      "    setMouseCallback(...)\n",
      "        setMouseCallback(windowName, onMouse [, param]) -> None\n",
      "    \n",
      "    setNumThreads(...)\n",
      "        setNumThreads(nthreads) -> None\n",
      "        .   @brief OpenCV will try to set the number of threads for the next parallel region.\n",
      "        .   \n",
      "        .   If threads == 0, OpenCV will disable threading optimizations and run all it's functions\n",
      "        .   sequentially. Passing threads \\< 0 will reset threads number to system default. This function must\n",
      "        .   be called outside of parallel region.\n",
      "        .   \n",
      "        .   OpenCV will try to run its functions with specified threads number, but some behaviour differs from\n",
      "        .   framework:\n",
      "        .   -   `TBB` - User-defined parallel constructions will run with the same threads number, if\n",
      "        .       another is not specified. If later on user creates his own scheduler, OpenCV will use it.\n",
      "        .   -   `OpenMP` - No special defined behaviour.\n",
      "        .   -   `Concurrency` - If threads == 1, OpenCV will disable threading optimizations and run its\n",
      "        .       functions sequentially.\n",
      "        .   -   `GCD` - Supports only values \\<= 0.\n",
      "        .   -   `C=` - No special defined behaviour.\n",
      "        .   @param nthreads Number of threads used by OpenCV.\n",
      "        .   @sa getNumThreads, getThreadNum\n",
      "    \n",
      "    setRNGSeed(...)\n",
      "        setRNGSeed(seed) -> None\n",
      "        .   @brief Sets state of default random number generator.\n",
      "        .   \n",
      "        .   The function cv::setRNGSeed sets state of default random number generator to custom value.\n",
      "        .   @param seed new state for default random number generator\n",
      "        .   @sa RNG, randu, randn\n",
      "    \n",
      "    setTrackbarMax(...)\n",
      "        setTrackbarMax(trackbarname, winname, maxval) -> None\n",
      "        .   @brief Sets the trackbar maximum position.\n",
      "        .   \n",
      "        .   The function sets the maximum position of the specified trackbar in the specified window.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   [__Qt Backend Only__] winname can be empty if the trackbar is attached to the control\n",
      "        .   panel.\n",
      "        .   \n",
      "        .   @param trackbarname Name of the trackbar.\n",
      "        .   @param winname Name of the window that is the parent of trackbar.\n",
      "        .   @param maxval New maximum position.\n",
      "    \n",
      "    setTrackbarMin(...)\n",
      "        setTrackbarMin(trackbarname, winname, minval) -> None\n",
      "        .   @brief Sets the trackbar minimum position.\n",
      "        .   \n",
      "        .   The function sets the minimum position of the specified trackbar in the specified window.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   [__Qt Backend Only__] winname can be empty if the trackbar is attached to the control\n",
      "        .   panel.\n",
      "        .   \n",
      "        .   @param trackbarname Name of the trackbar.\n",
      "        .   @param winname Name of the window that is the parent of trackbar.\n",
      "        .   @param minval New minimum position.\n",
      "    \n",
      "    setTrackbarPos(...)\n",
      "        setTrackbarPos(trackbarname, winname, pos) -> None\n",
      "        .   @brief Sets the trackbar position.\n",
      "        .   \n",
      "        .   The function sets the position of the specified trackbar in the specified window.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   [__Qt Backend Only__] winname can be empty if the trackbar is attached to the control\n",
      "        .   panel.\n",
      "        .   \n",
      "        .   @param trackbarname Name of the trackbar.\n",
      "        .   @param winname Name of the window that is the parent of trackbar.\n",
      "        .   @param pos New position.\n",
      "    \n",
      "    setUseOpenVX(...)\n",
      "        setUseOpenVX(flag) -> None\n",
      "        .\n",
      "    \n",
      "    setUseOptimized(...)\n",
      "        setUseOptimized(onoff) -> None\n",
      "        .   @brief Enables or disables the optimized code.\n",
      "        .   \n",
      "        .   The function can be used to dynamically turn on and off optimized dispatched code (code that uses SSE4.2, AVX/AVX2,\n",
      "        .   and other instructions on the platforms that support it). It sets a global flag that is further\n",
      "        .   checked by OpenCV functions. Since the flag is not checked in the inner OpenCV loops, it is only\n",
      "        .   safe to call the function on the very top level in your application where you can be sure that no\n",
      "        .   other OpenCV function is currently executed.\n",
      "        .   \n",
      "        .   By default, the optimized code is enabled unless you disable it in CMake. The current status can be\n",
      "        .   retrieved using useOptimized.\n",
      "        .   @param onoff The boolean flag specifying whether the optimized code should be used (onoff=true)\n",
      "        .   or not (onoff=false).\n",
      "    \n",
      "    setWindowProperty(...)\n",
      "        setWindowProperty(winname, prop_id, prop_value) -> None\n",
      "        .   @brief Changes parameters of a window dynamically.\n",
      "        .   \n",
      "        .   The function setWindowProperty enables changing properties of a window.\n",
      "        .   \n",
      "        .   @param winname Name of the window.\n",
      "        .   @param prop_id Window property to edit. The supported operation flags are: (cv::WindowPropertyFlags)\n",
      "        .   @param prop_value New value of the window property. The supported flags are: (cv::WindowFlags)\n",
      "    \n",
      "    setWindowTitle(...)\n",
      "        setWindowTitle(winname, title) -> None\n",
      "        .   @brief Updates window title\n",
      "        .   @param winname Name of the window.\n",
      "        .   @param title New title.\n",
      "    \n",
      "    solve(...)\n",
      "        solve(src1, src2[, dst[, flags]]) -> retval, dst\n",
      "        .   @brief Solves one or more linear systems or least-squares problems.\n",
      "        .   \n",
      "        .   The function cv::solve solves a linear system or least-squares problem (the\n",
      "        .   latter is possible with SVD or QR methods, or by specifying the flag\n",
      "        .   #DECOMP_NORMAL ):\n",
      "        .   \\f[\\texttt{dst} =  \\arg \\min _X \\| \\texttt{src1} \\cdot \\texttt{X} -  \\texttt{src2} \\|\\f]\n",
      "        .   \n",
      "        .   If #DECOMP_LU or #DECOMP_CHOLESKY method is used, the function returns 1\n",
      "        .   if src1 (or \\f$\\texttt{src1}^T\\texttt{src1}\\f$ ) is non-singular. Otherwise,\n",
      "        .   it returns 0. In the latter case, dst is not valid. Other methods find a\n",
      "        .   pseudo-solution in case of a singular left-hand side part.\n",
      "        .   \n",
      "        .   @note If you want to find a unity-norm solution of an under-defined\n",
      "        .   singular system \\f$\\texttt{src1}\\cdot\\texttt{dst}=0\\f$ , the function solve\n",
      "        .   will not do the work. Use SVD::solveZ instead.\n",
      "        .   \n",
      "        .   @param src1 input matrix on the left-hand side of the system.\n",
      "        .   @param src2 input matrix on the right-hand side of the system.\n",
      "        .   @param dst output solution.\n",
      "        .   @param flags solution (matrix inversion) method (#DecompTypes)\n",
      "        .   @sa invert, SVD, eigen\n",
      "    \n",
      "    solveCubic(...)\n",
      "        solveCubic(coeffs[, roots]) -> retval, roots\n",
      "        .   @brief Finds the real roots of a cubic equation.\n",
      "        .   \n",
      "        .   The function solveCubic finds the real roots of a cubic equation:\n",
      "        .   -   if coeffs is a 4-element vector:\n",
      "        .   \\f[\\texttt{coeffs} [0] x^3 +  \\texttt{coeffs} [1] x^2 +  \\texttt{coeffs} [2] x +  \\texttt{coeffs} [3] = 0\\f]\n",
      "        .   -   if coeffs is a 3-element vector:\n",
      "        .   \\f[x^3 +  \\texttt{coeffs} [0] x^2 +  \\texttt{coeffs} [1] x +  \\texttt{coeffs} [2] = 0\\f]\n",
      "        .   \n",
      "        .   The roots are stored in the roots array.\n",
      "        .   @param coeffs equation coefficients, an array of 3 or 4 elements.\n",
      "        .   @param roots output array of real roots that has 1 or 3 elements.\n",
      "        .   @return number of real roots. It can be 0, 1 or 2.\n",
      "    \n",
      "    solveLP(...)\n",
      "        solveLP(Func, Constr[, z]) -> retval, z\n",
      "        .   @brief Solve given (non-integer) linear programming problem using the Simplex Algorithm (Simplex Method).\n",
      "        .   \n",
      "        .   What we mean here by \"linear programming problem\" (or LP problem, for short) can be formulated as:\n",
      "        .   \n",
      "        .   \\f[\\mbox{Maximize } c\\cdot x\\\\\n",
      "        .    \\mbox{Subject to:}\\\\\n",
      "        .    Ax\\leq b\\\\\n",
      "        .    x\\geq 0\\f]\n",
      "        .   \n",
      "        .   Where \\f$c\\f$ is fixed `1`-by-`n` row-vector, \\f$A\\f$ is fixed `m`-by-`n` matrix, \\f$b\\f$ is fixed `m`-by-`1`\n",
      "        .   column vector and \\f$x\\f$ is an arbitrary `n`-by-`1` column vector, which satisfies the constraints.\n",
      "        .   \n",
      "        .   Simplex algorithm is one of many algorithms that are designed to handle this sort of problems\n",
      "        .   efficiently. Although it is not optimal in theoretical sense (there exist algorithms that can solve\n",
      "        .   any problem written as above in polynomial time, while simplex method degenerates to exponential\n",
      "        .   time for some special cases), it is well-studied, easy to implement and is shown to work well for\n",
      "        .   real-life purposes.\n",
      "        .   \n",
      "        .   The particular implementation is taken almost verbatim from **Introduction to Algorithms, third\n",
      "        .   edition** by T. H. Cormen, C. E. Leiserson, R. L. Rivest and Clifford Stein. In particular, the\n",
      "        .   Bland's rule <http://en.wikipedia.org/wiki/Bland%27s_rule> is used to prevent cycling.\n",
      "        .   \n",
      "        .   @param Func This row-vector corresponds to \\f$c\\f$ in the LP problem formulation (see above). It should\n",
      "        .   contain 32- or 64-bit floating point numbers. As a convenience, column-vector may be also submitted,\n",
      "        .   in the latter case it is understood to correspond to \\f$c^T\\f$.\n",
      "        .   @param Constr `m`-by-`n+1` matrix, whose rightmost column corresponds to \\f$b\\f$ in formulation above\n",
      "        .   and the remaining to \\f$A\\f$. It should contain 32- or 64-bit floating point numbers.\n",
      "        .   @param z The solution will be returned here as a column-vector - it corresponds to \\f$c\\f$ in the\n",
      "        .   formulation above. It will contain 64-bit floating point numbers.\n",
      "        .   @return One of cv::SolveLPResult\n",
      "    \n",
      "    solveP3P(...)\n",
      "        solveP3P(objectPoints, imagePoints, cameraMatrix, distCoeffs, flags[, rvecs[, tvecs]]) -> retval, rvecs, tvecs\n",
      "        .   @brief Finds an object pose from 3 3D-2D point correspondences.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, 3x3 1-channel or\n",
      "        .   1x3/3x1 3-channel. vector\\<Point3f\\> can be also passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, 3x2 1-channel or 1x3/3x1 2-channel.\n",
      "        .    vector\\<Point2f\\> can be also passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvecs Output rotation vectors (see @ref Rodrigues ) that, together with tvecs, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system. A P3P problem has up to 4 solutions.\n",
      "        .   @param tvecs Output translation vectors.\n",
      "        .   @param flags Method for solving a P3P problem:\n",
      "        .   -   **SOLVEPNP_P3P** Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang\n",
      "        .   \"Complete Solution Classification for the Perspective-Three-Point Problem\" (@cite gao2003complete).\n",
      "        .   -   **SOLVEPNP_AP3P** Method is based on the paper of T. Ke and S. Roumeliotis.\n",
      "        .   \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\" (@cite Ke17).\n",
      "        .   \n",
      "        .   The function estimates the object pose given 3 object points, their corresponding image\n",
      "        .   projections, as well as the camera matrix and the distortion coefficients.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   The solutions are sorted by reprojection errors (lowest to highest).\n",
      "    \n",
      "    solvePnP(...)\n",
      "        solvePnP(objectPoints, imagePoints, cameraMatrix, distCoeffs[, rvec[, tvec[, useExtrinsicGuess[, flags]]]]) -> retval, rvec, tvec\n",
      "        .   @brief Finds an object pose from 3D-2D point correspondences.\n",
      "        .   This function returns the rotation and the translation vectors that transform a 3D point expressed in the object\n",
      "        .   coordinate frame to the camera coordinate frame, using different methods:\n",
      "        .   - P3P methods (@ref SOLVEPNP_P3P, @ref SOLVEPNP_AP3P): need 4 input points to return a unique solution.\n",
      "        .   - @ref SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar.\n",
      "        .   - @ref SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.\n",
      "        .   Number of input points must be 4. Object points must be defined in the following order:\n",
      "        .     - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .     - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "        .   - for all the other flags, number of input points must be >= 4 and object points can be in any configuration.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n",
      "        .   1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3f\\> can be also passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n",
      "        .   where N is the number of points. vector\\<Point2f\\> can be also passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvec Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system.\n",
      "        .   @param tvec Output translation vector.\n",
      "        .   @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses\n",
      "        .   the provided rvec and tvec values as initial approximations of the rotation and translation\n",
      "        .   vectors, respectively, and further optimizes them.\n",
      "        .   @param flags Method for solving a PnP problem:\n",
      "        .   -   **SOLVEPNP_ITERATIVE** Iterative method is based on a Levenberg-Marquardt optimization. In\n",
      "        .   this case the function finds such a pose that minimizes reprojection error, that is the sum\n",
      "        .   of squared distances between the observed projections imagePoints and the projected (using\n",
      "        .   projectPoints ) objectPoints .\n",
      "        .   -   **SOLVEPNP_P3P** Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang\n",
      "        .   \"Complete Solution Classification for the Perspective-Three-Point Problem\" (@cite gao2003complete).\n",
      "        .   In this case the function requires exactly four object and image points.\n",
      "        .   -   **SOLVEPNP_AP3P** Method is based on the paper of T. Ke, S. Roumeliotis\n",
      "        .   \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\" (@cite Ke17).\n",
      "        .   In this case the function requires exactly four object and image points.\n",
      "        .   -   **SOLVEPNP_EPNP** Method has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the\n",
      "        .   paper \"EPnP: Efficient Perspective-n-Point Camera Pose Estimation\" (@cite lepetit2009epnp).\n",
      "        .   -   **SOLVEPNP_DLS** Method is based on the paper of J. Hesch and S. Roumeliotis.\n",
      "        .   \"A Direct Least-Squares (DLS) Method for PnP\" (@cite hesch2011direct).\n",
      "        .   -   **SOLVEPNP_UPNP** Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,\n",
      "        .   F. Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n",
      "        .   Estimation\" (@cite penate2013exhaustive). In this case the function also estimates the parameters \\f$f_x\\f$ and \\f$f_y\\f$\n",
      "        .   assuming that both have the same value. Then the cameraMatrix is updated with the estimated\n",
      "        .   focal length.\n",
      "        .   -   **SOLVEPNP_IPPE** Method is based on the paper of T. Collins and A. Bartoli.\n",
      "        .   \"Infinitesimal Plane-Based Pose Estimation\" (@cite Collins14). This method requires coplanar object points.\n",
      "        .   -   **SOLVEPNP_IPPE_SQUARE** Method is based on the paper of Toby Collins and Adrien Bartoli.\n",
      "        .   \"Infinitesimal Plane-Based Pose Estimation\" (@cite Collins14). This method is suitable for marker pose estimation.\n",
      "        .   It requires 4 coplanar object points defined in the following order:\n",
      "        .     - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .     - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "        .   \n",
      "        .   The function estimates the object pose given a set of object points, their corresponding image\n",
      "        .   projections, as well as the camera matrix and the distortion coefficients, see the figure below\n",
      "        .   (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward\n",
      "        .   and the Z-axis forward).\n",
      "        .   \n",
      "        .   ![](pnp.jpg)\n",
      "        .   \n",
      "        .   Points expressed in the world frame \\f$ \\bf{X}_w \\f$ are projected into the image plane \\f$ \\left[ u, v \\right] \\f$\n",
      "        .   using the perspective projection model \\f$ \\Pi \\f$ and the camera intrinsic parameters matrix \\f$ \\bf{A} \\f$:\n",
      "        .   \n",
      "        .   \\f[\n",
      "        .     \\begin{align*}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     u \\\\\n",
      "        .     v \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\bf{A} \\hspace{0.1em} \\Pi \\hspace{0.2em} ^{c}\\bf{M}_w\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} \\\\\n",
      "        .     \\begin{bmatrix}\n",
      "        .     u \\\\\n",
      "        .     v \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\begin{bmatrix}\n",
      "        .     f_x & 0 & c_x \\\\\n",
      "        .     0 & f_y & c_y \\\\\n",
      "        .     0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     1 & 0 & 0 & 0 \\\\\n",
      "        .     0 & 1 & 0 & 0 \\\\\n",
      "        .     0 & 0 & 1 & 0\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     r_{11} & r_{12} & r_{13} & t_x \\\\\n",
      "        .     r_{21} & r_{22} & r_{23} & t_y \\\\\n",
      "        .     r_{31} & r_{32} & r_{33} & t_z \\\\\n",
      "        .     0 & 0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\end{align*}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow transforming\n",
      "        .   a 3D point expressed in the world frame into the camera frame:\n",
      "        .   \n",
      "        .   \\f[\n",
      "        .     \\begin{align*}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_c \\\\\n",
      "        .     Y_c \\\\\n",
      "        .     Z_c \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\hspace{0.2em} ^{c}\\bf{M}_w\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} \\\\\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_c \\\\\n",
      "        .     Y_c \\\\\n",
      "        .     Z_c \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\begin{bmatrix}\n",
      "        .     r_{11} & r_{12} & r_{13} & t_x \\\\\n",
      "        .     r_{21} & r_{22} & r_{23} & t_y \\\\\n",
      "        .     r_{31} & r_{32} & r_{33} & t_z \\\\\n",
      "        .     0 & 0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\end{align*}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   @note\n",
      "        .      -   An example of how to use solvePnP for planar augmented reality can be found at\n",
      "        .           opencv_source_code/samples/python/plane_ar.py\n",
      "        .      -   If you are using Python:\n",
      "        .           - Numpy array slices won't work as input because solvePnP requires contiguous\n",
      "        .           arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of\n",
      "        .           modules/calib3d/src/solvepnp.cpp version 2.4.9)\n",
      "        .           - The P3P algorithm requires image points to be in an array of shape (N,1,2) due\n",
      "        .           to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)\n",
      "        .           which requires 2-channel information.\n",
      "        .           - Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of\n",
      "        .           it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n",
      "        .           np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n",
      "        .      -   The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n",
      "        .          unstable and sometimes give completely wrong results. If you pass one of these two\n",
      "        .          flags, **SOLVEPNP_EPNP** method will be used instead.\n",
      "        .      -   The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and **SOLVEPNP_AP3P**\n",
      "        .          methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions\n",
      "        .          of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).\n",
      "        .      -   With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3 (3 points\n",
      "        .          are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the\n",
      "        .          global solution to converge.\n",
      "        .      -   With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n",
      "        .      -   With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation.\n",
      "        .          Number of input points must be 4. Object points must be defined in the following order:\n",
      "        .            - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .            - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .            - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .            - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "    \n",
      "    solvePnPGeneric(...)\n",
      "        solvePnPGeneric(objectPoints, imagePoints, cameraMatrix, distCoeffs[, rvecs[, tvecs[, useExtrinsicGuess[, flags[, rvec[, tvec[, reprojectionError]]]]]]]) -> retval, rvecs, tvecs, reprojectionError\n",
      "        .   @brief Finds an object pose from 3D-2D point correspondences.\n",
      "        .   This function returns a list of all the possible solutions (a solution is a <rotation vector, translation vector>\n",
      "        .   couple), depending on the number of input points and the chosen method:\n",
      "        .   - P3P methods (@ref SOLVEPNP_P3P, @ref SOLVEPNP_AP3P): 3 or 4 input points. Number of returned solutions can be between 0 and 4 with 3 input points.\n",
      "        .   - @ref SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar. Returns 2 solutions.\n",
      "        .   - @ref SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation.\n",
      "        .   Number of input points must be 4 and 2 solutions are returned. Object points must be defined in the following order:\n",
      "        .     - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .     - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "        .   - for all the other flags, number of input points must be >= 4 and object points can be in any configuration.\n",
      "        .   Only 1 solution is returned.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n",
      "        .   1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3f\\> can be also passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n",
      "        .   where N is the number of points. vector\\<Point2f\\> can be also passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvecs Vector of output rotation vectors (see @ref Rodrigues ) that, together with tvecs, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system.\n",
      "        .   @param tvecs Vector of output translation vectors.\n",
      "        .   @param useExtrinsicGuess Parameter used for #SOLVEPNP_ITERATIVE. If true (1), the function uses\n",
      "        .   the provided rvec and tvec values as initial approximations of the rotation and translation\n",
      "        .   vectors, respectively, and further optimizes them.\n",
      "        .   @param flags Method for solving a PnP problem:\n",
      "        .   -   **SOLVEPNP_ITERATIVE** Iterative method is based on a Levenberg-Marquardt optimization. In\n",
      "        .   this case the function finds such a pose that minimizes reprojection error, that is the sum\n",
      "        .   of squared distances between the observed projections imagePoints and the projected (using\n",
      "        .   projectPoints ) objectPoints .\n",
      "        .   -   **SOLVEPNP_P3P** Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang\n",
      "        .   \"Complete Solution Classification for the Perspective-Three-Point Problem\" (@cite gao2003complete).\n",
      "        .   In this case the function requires exactly four object and image points.\n",
      "        .   -   **SOLVEPNP_AP3P** Method is based on the paper of T. Ke, S. Roumeliotis\n",
      "        .   \"An Efficient Algebraic Solution to the Perspective-Three-Point Problem\" (@cite Ke17).\n",
      "        .   In this case the function requires exactly four object and image points.\n",
      "        .   -   **SOLVEPNP_EPNP** Method has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the\n",
      "        .   paper \"EPnP: Efficient Perspective-n-Point Camera Pose Estimation\" (@cite lepetit2009epnp).\n",
      "        .   -   **SOLVEPNP_DLS** Method is based on the paper of Joel A. Hesch and Stergios I. Roumeliotis.\n",
      "        .   \"A Direct Least-Squares (DLS) Method for PnP\" (@cite hesch2011direct).\n",
      "        .   -   **SOLVEPNP_UPNP** Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,\n",
      "        .   F.Moreno-Noguer. \"Exhaustive Linearization for Robust Camera Pose and Focal Length\n",
      "        .   Estimation\" (@cite penate2013exhaustive). In this case the function also estimates the parameters \\f$f_x\\f$ and \\f$f_y\\f$\n",
      "        .   assuming that both have the same value. Then the cameraMatrix is updated with the estimated\n",
      "        .   focal length.\n",
      "        .   -   **SOLVEPNP_IPPE** Method is based on the paper of T. Collins and A. Bartoli.\n",
      "        .   \"Infinitesimal Plane-Based Pose Estimation\" (@cite Collins14). This method requires coplanar object points.\n",
      "        .   -   **SOLVEPNP_IPPE_SQUARE** Method is based on the paper of Toby Collins and Adrien Bartoli.\n",
      "        .   \"Infinitesimal Plane-Based Pose Estimation\" (@cite Collins14). This method is suitable for marker pose estimation.\n",
      "        .   It requires 4 coplanar object points defined in the following order:\n",
      "        .     - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .     - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .     - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "        .   @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE\n",
      "        .   and useExtrinsicGuess is set to true.\n",
      "        .   @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag is SOLVEPNP_ITERATIVE\n",
      "        .   and useExtrinsicGuess is set to true.\n",
      "        .   @param reprojectionError Optional vector of reprojection error, that is the RMS error\n",
      "        .   (\\f$ \\text{RMSE} = \\sqrt{\\frac{\\sum_{i}^{N} \\left ( \\hat{y_i} - y_i \\right )^2}{N}} \\f$) between the input image points\n",
      "        .   and the 3D object points projected with the estimated pose.\n",
      "        .   \n",
      "        .   The function estimates the object pose given a set of object points, their corresponding image\n",
      "        .   projections, as well as the camera matrix and the distortion coefficients, see the figure below\n",
      "        .   (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward\n",
      "        .   and the Z-axis forward).\n",
      "        .   \n",
      "        .   ![](pnp.jpg)\n",
      "        .   \n",
      "        .   Points expressed in the world frame \\f$ \\bf{X}_w \\f$ are projected into the image plane \\f$ \\left[ u, v \\right] \\f$\n",
      "        .   using the perspective projection model \\f$ \\Pi \\f$ and the camera intrinsic parameters matrix \\f$ \\bf{A} \\f$:\n",
      "        .   \n",
      "        .   \\f[\n",
      "        .     \\begin{align*}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     u \\\\\n",
      "        .     v \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\bf{A} \\hspace{0.1em} \\Pi \\hspace{0.2em} ^{c}\\bf{M}_w\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} \\\\\n",
      "        .     \\begin{bmatrix}\n",
      "        .     u \\\\\n",
      "        .     v \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\begin{bmatrix}\n",
      "        .     f_x & 0 & c_x \\\\\n",
      "        .     0 & f_y & c_y \\\\\n",
      "        .     0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     1 & 0 & 0 & 0 \\\\\n",
      "        .     0 & 1 & 0 & 0 \\\\\n",
      "        .     0 & 0 & 1 & 0\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     r_{11} & r_{12} & r_{13} & t_x \\\\\n",
      "        .     r_{21} & r_{22} & r_{23} & t_y \\\\\n",
      "        .     r_{31} & r_{32} & r_{33} & t_z \\\\\n",
      "        .     0 & 0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\end{align*}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow transforming\n",
      "        .   a 3D point expressed in the world frame into the camera frame:\n",
      "        .   \n",
      "        .   \\f[\n",
      "        .     \\begin{align*}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_c \\\\\n",
      "        .     Y_c \\\\\n",
      "        .     Z_c \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\hspace{0.2em} ^{c}\\bf{M}_w\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} \\\\\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_c \\\\\n",
      "        .     Y_c \\\\\n",
      "        .     Z_c \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix} &=\n",
      "        .     \\begin{bmatrix}\n",
      "        .     r_{11} & r_{12} & r_{13} & t_x \\\\\n",
      "        .     r_{21} & r_{22} & r_{23} & t_y \\\\\n",
      "        .     r_{31} & r_{32} & r_{33} & t_z \\\\\n",
      "        .     0 & 0 & 0 & 1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\begin{bmatrix}\n",
      "        .     X_{w} \\\\\n",
      "        .     Y_{w} \\\\\n",
      "        .     Z_{w} \\\\\n",
      "        .     1\n",
      "        .     \\end{bmatrix}\n",
      "        .     \\end{align*}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   @note\n",
      "        .      -   An example of how to use solvePnP for planar augmented reality can be found at\n",
      "        .           opencv_source_code/samples/python/plane_ar.py\n",
      "        .      -   If you are using Python:\n",
      "        .           - Numpy array slices won't work as input because solvePnP requires contiguous\n",
      "        .           arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of\n",
      "        .           modules/calib3d/src/solvepnp.cpp version 2.4.9)\n",
      "        .           - The P3P algorithm requires image points to be in an array of shape (N,1,2) due\n",
      "        .           to its calling of cv::undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9)\n",
      "        .           which requires 2-channel information.\n",
      "        .           - Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of\n",
      "        .           it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints =\n",
      "        .           np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n",
      "        .      -   The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are\n",
      "        .          unstable and sometimes give completely wrong results. If you pass one of these two\n",
      "        .          flags, **SOLVEPNP_EPNP** method will be used instead.\n",
      "        .      -   The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and **SOLVEPNP_AP3P**\n",
      "        .          methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions\n",
      "        .          of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).\n",
      "        .      -   With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3 (3 points\n",
      "        .          are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the\n",
      "        .          global solution to converge.\n",
      "        .      -   With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.\n",
      "        .      -   With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation.\n",
      "        .          Number of input points must be 4. Object points must be defined in the following order:\n",
      "        .            - point 0: [-squareLength / 2,  squareLength / 2, 0]\n",
      "        .            - point 1: [ squareLength / 2,  squareLength / 2, 0]\n",
      "        .            - point 2: [ squareLength / 2, -squareLength / 2, 0]\n",
      "        .            - point 3: [-squareLength / 2, -squareLength / 2, 0]\n",
      "    \n",
      "    solvePnPRansac(...)\n",
      "        solvePnPRansac(objectPoints, imagePoints, cameraMatrix, distCoeffs[, rvec[, tvec[, useExtrinsicGuess[, iterationsCount[, reprojectionError[, confidence[, inliers[, flags]]]]]]]]) -> retval, rvec, tvec, inliers\n",
      "        .   @brief Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or\n",
      "        .   1xN/Nx1 3-channel, where N is the number of points. vector\\<Point3f\\> can be also passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n",
      "        .   where N is the number of points. vector\\<Point2f\\> can be also passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvec Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system.\n",
      "        .   @param tvec Output translation vector.\n",
      "        .   @param useExtrinsicGuess Parameter used for @ref SOLVEPNP_ITERATIVE. If true (1), the function uses\n",
      "        .   the provided rvec and tvec values as initial approximations of the rotation and translation\n",
      "        .   vectors, respectively, and further optimizes them.\n",
      "        .   @param iterationsCount Number of iterations.\n",
      "        .   @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value\n",
      "        .   is the maximum allowed distance between the observed and computed point projections to consider it\n",
      "        .   an inlier.\n",
      "        .   @param confidence The probability that the algorithm produces a useful result.\n",
      "        .   @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .\n",
      "        .   @param flags Method for solving a PnP problem (see @ref solvePnP ).\n",
      "        .   \n",
      "        .   The function estimates an object pose given a set of object points, their corresponding image\n",
      "        .   projections, as well as the camera matrix and the distortion coefficients. This function finds such\n",
      "        .   a pose that minimizes reprojection error, that is, the sum of squared distances between the observed\n",
      "        .   projections imagePoints and the projected (using @ref projectPoints ) objectPoints. The use of RANSAC\n",
      "        .   makes the function resistant to outliers.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      -   An example of how to use solvePNPRansac for object detection can be found at\n",
      "        .           opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/\n",
      "        .      -   The default method used to estimate the camera pose for the Minimal Sample Sets step\n",
      "        .          is #SOLVEPNP_EPNP. Exceptions are:\n",
      "        .            - if you choose #SOLVEPNP_P3P or #SOLVEPNP_AP3P, these methods will be used.\n",
      "        .            - if the number of input points is equal to 4, #SOLVEPNP_P3P is used.\n",
      "        .      -   The method used to estimate the camera pose using all the inliers is defined by the\n",
      "        .          flags parameters unless it is equal to #SOLVEPNP_P3P or #SOLVEPNP_AP3P. In this case,\n",
      "        .          the method #SOLVEPNP_EPNP will be used instead.\n",
      "    \n",
      "    solvePnPRefineLM(...)\n",
      "        solvePnPRefineLM(objectPoints, imagePoints, cameraMatrix, distCoeffs, rvec, tvec[, criteria]) -> rvec, tvec\n",
      "        .   @brief Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame\n",
      "        .   to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,\n",
      "        .   where N is the number of points. vector\\<Point3f\\> can also be passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n",
      "        .   where N is the number of points. vector\\<Point2f\\> can also be passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvec Input/Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system. Input values are used as an initial solution.\n",
      "        .   @param tvec Input/Output translation vector. Input values are used as an initial solution.\n",
      "        .   @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n",
      "        .   \n",
      "        .   The function refines the object pose given at least 3 object points, their corresponding image\n",
      "        .   projections, an initial solution for the rotation and translation vector,\n",
      "        .   as well as the camera matrix and the distortion coefficients.\n",
      "        .   The function minimizes the projection error with respect to the rotation and the translation vectors, according\n",
      "        .   to a Levenberg-Marquardt iterative minimization @cite Madsen04 @cite Eade13 process.\n",
      "    \n",
      "    solvePnPRefineVVS(...)\n",
      "        solvePnPRefineVVS(objectPoints, imagePoints, cameraMatrix, distCoeffs, rvec, tvec[, criteria[, VVSlambda]]) -> rvec, tvec\n",
      "        .   @brief Refine a pose (the translation and the rotation that transform a 3D point expressed in the object coordinate frame\n",
      "        .   to the camera coordinate frame) from a 3D-2D point correspondences and starting from an initial solution.\n",
      "        .   \n",
      "        .   @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel,\n",
      "        .   where N is the number of points. vector\\<Point3f\\> can also be passed here.\n",
      "        .   @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel,\n",
      "        .   where N is the number of points. vector\\<Point2f\\> can also be passed here.\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are\n",
      "        .   assumed.\n",
      "        .   @param rvec Input/Output rotation vector (see @ref Rodrigues ) that, together with tvec, brings points from\n",
      "        .   the model coordinate system to the camera coordinate system. Input values are used as an initial solution.\n",
      "        .   @param tvec Input/Output translation vector. Input values are used as an initial solution.\n",
      "        .   @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.\n",
      "        .   @param VVSlambda Gain for the virtual visual servoing control law, equivalent to the \\f$\\alpha\\f$\n",
      "        .   gain in the Damped Gauss-Newton formulation.\n",
      "        .   \n",
      "        .   The function refines the object pose given at least 3 object points, their corresponding image\n",
      "        .   projections, an initial solution for the rotation and translation vector,\n",
      "        .   as well as the camera matrix and the distortion coefficients.\n",
      "        .   The function minimizes the projection error with respect to the rotation and the translation vectors, using a\n",
      "        .   virtual visual servoing (VVS) @cite Chaumette06 @cite Marchand16 scheme.\n",
      "    \n",
      "    solvePoly(...)\n",
      "        solvePoly(coeffs[, roots[, maxIters]]) -> retval, roots\n",
      "        .   @brief Finds the real or complex roots of a polynomial equation.\n",
      "        .   \n",
      "        .   The function cv::solvePoly finds real and complex roots of a polynomial equation:\n",
      "        .   \\f[\\texttt{coeffs} [n] x^{n} +  \\texttt{coeffs} [n-1] x^{n-1} + ... +  \\texttt{coeffs} [1] x +  \\texttt{coeffs} [0] = 0\\f]\n",
      "        .   @param coeffs array of polynomial coefficients.\n",
      "        .   @param roots output (complex) array of roots.\n",
      "        .   @param maxIters maximum number of iterations the algorithm does.\n",
      "    \n",
      "    sort(...)\n",
      "        sort(src, flags[, dst]) -> dst\n",
      "        .   @brief Sorts each row or each column of a matrix.\n",
      "        .   \n",
      "        .   The function cv::sort sorts each matrix row or each matrix column in\n",
      "        .   ascending or descending order. So you should pass two operation flags to\n",
      "        .   get desired behaviour. If you want to sort matrix rows or columns\n",
      "        .   lexicographically, you can use STL std::sort generic function with the\n",
      "        .   proper comparison predicate.\n",
      "        .   \n",
      "        .   @param src input single-channel array.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "        .   @param flags operation flags, a combination of #SortFlags\n",
      "        .   @sa sortIdx, randShuffle\n",
      "    \n",
      "    sortIdx(...)\n",
      "        sortIdx(src, flags[, dst]) -> dst\n",
      "        .   @brief Sorts each row or each column of a matrix.\n",
      "        .   \n",
      "        .   The function cv::sortIdx sorts each matrix row or each matrix column in the\n",
      "        .   ascending or descending order. So you should pass two operation flags to\n",
      "        .   get desired behaviour. Instead of reordering the elements themselves, it\n",
      "        .   stores the indices of sorted elements in the output array. For example:\n",
      "        .   @code\n",
      "        .       Mat A = Mat::eye(3,3,CV_32F), B;\n",
      "        .       sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);\n",
      "        .       // B will probably contain\n",
      "        .       // (because of equal elements in A some permutations are possible):\n",
      "        .       // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]\n",
      "        .   @endcode\n",
      "        .   @param src input single-channel array.\n",
      "        .   @param dst output integer array of the same size as src.\n",
      "        .   @param flags operation flags that could be a combination of cv::SortFlags\n",
      "        .   @sa sort, randShuffle\n",
      "    \n",
      "    spatialGradient(...)\n",
      "        spatialGradient(src[, dx[, dy[, ksize[, borderType]]]]) -> dx, dy\n",
      "        .   @brief Calculates the first order image derivative in both x and y using a Sobel operator\n",
      "        .   \n",
      "        .   Equivalent to calling:\n",
      "        .   \n",
      "        .   @code\n",
      "        .   Sobel( src, dx, CV_16SC1, 1, 0, 3 );\n",
      "        .   Sobel( src, dy, CV_16SC1, 0, 1, 3 );\n",
      "        .   @endcode\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dx output image with first-order derivative in x.\n",
      "        .   @param dy output image with first-order derivative in y.\n",
      "        .   @param ksize size of Sobel kernel. It must be 3.\n",
      "        .   @param borderType pixel extrapolation method, see #BorderTypes\n",
      "        .   \n",
      "        .   @sa Sobel\n",
      "    \n",
      "    split(...)\n",
      "        split(m[, mv]) -> mv\n",
      "        .   @overload\n",
      "        .   @param m input multi-channel array.\n",
      "        .   @param mv output vector of arrays; the arrays themselves are reallocated, if needed.\n",
      "    \n",
      "    sqrBoxFilter(...)\n",
      "        sqrBoxFilter(src, ddepth, ksize[, dst[, anchor[, normalize[, borderType]]]]) -> dst\n",
      "        .   @brief Calculates the normalized sum of squares of the pixel values overlapping the filter.\n",
      "        .   \n",
      "        .   For every pixel \\f$ (x, y) \\f$ in the source image, the function calculates the sum of squares of those neighboring\n",
      "        .   pixel values which overlap the filter placed over the pixel \\f$ (x, y) \\f$.\n",
      "        .   \n",
      "        .   The unnormalized square box filter can be useful in computing local image statistics such as the the local\n",
      "        .   variance and standard deviation around the neighborhood of a pixel.\n",
      "        .   \n",
      "        .   @param src input image\n",
      "        .   @param dst output image of the same size and type as _src\n",
      "        .   @param ddepth the output image depth (-1 to use src.depth())\n",
      "        .   @param ksize kernel size\n",
      "        .   @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at the kernel\n",
      "        .   center.\n",
      "        .   @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.\n",
      "        .   @param borderType border mode used to extrapolate pixels outside of the image, see #BorderTypes\n",
      "        .   @sa boxFilter\n",
      "    \n",
      "    sqrt(...)\n",
      "        sqrt(src[, dst]) -> dst\n",
      "        .   @brief Calculates a square root of array elements.\n",
      "        .   \n",
      "        .   The function cv::sqrt calculates a square root of each input array element.\n",
      "        .   In case of multi-channel arrays, each channel is processed\n",
      "        .   independently. The accuracy is approximately the same as of the built-in\n",
      "        .   std::sqrt .\n",
      "        .   @param src input floating-point array.\n",
      "        .   @param dst output array of the same size and type as src.\n",
      "    \n",
      "    startWindowThread(...)\n",
      "        startWindowThread() -> retval\n",
      "        .\n",
      "    \n",
      "    stereoCalibrate(...)\n",
      "        stereoCalibrate(objectPoints, imagePoints1, imagePoints2, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize[, R[, T[, E[, F[, flags[, criteria]]]]]]) -> retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F\n",
      "        .\n",
      "    \n",
      "    stereoCalibrateExtended(...)\n",
      "        stereoCalibrateExtended(objectPoints, imagePoints1, imagePoints2, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T[, E[, F[, perViewErrors[, flags[, criteria]]]]]) -> retval, cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, R, T, E, F, perViewErrors\n",
      "        .   @brief Calibrates the stereo camera.\n",
      "        .   \n",
      "        .   @param objectPoints Vector of vectors of the calibration pattern points.\n",
      "        .   @param imagePoints1 Vector of vectors of the projections of the calibration pattern points,\n",
      "        .   observed by the first camera.\n",
      "        .   @param imagePoints2 Vector of vectors of the projections of the calibration pattern points,\n",
      "        .   observed by the second camera.\n",
      "        .   @param cameraMatrix1 Input/output first camera matrix:\n",
      "        .   \\f$\\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\\f$ , \\f$j = 0,\\, 1\\f$ . If\n",
      "        .   any of CALIB_USE_INTRINSIC_GUESS , CALIB_FIX_ASPECT_RATIO ,\n",
      "        .   CALIB_FIX_INTRINSIC , or CALIB_FIX_FOCAL_LENGTH are specified, some or all of the\n",
      "        .   matrix components must be initialized. See the flags description for details.\n",
      "        .   @param distCoeffs1 Input/output vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$ of\n",
      "        .   4, 5, 8, 12 or 14 elements. The output vector length depends on the flags.\n",
      "        .   @param cameraMatrix2 Input/output second camera matrix. The parameter is similar to cameraMatrix1\n",
      "        .   @param distCoeffs2 Input/output lens distortion coefficients for the second camera. The parameter\n",
      "        .   is similar to distCoeffs1 .\n",
      "        .   @param imageSize Size of the image used only to initialize intrinsic camera matrix.\n",
      "        .   @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.\n",
      "        .   @param T Output translation vector between the coordinate systems of the cameras.\n",
      "        .   @param E Output essential matrix.\n",
      "        .   @param F Output fundamental matrix.\n",
      "        .   @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.\n",
      "        .   @param flags Different flags that may be zero or a combination of the following values:\n",
      "        .   -   **CALIB_FIX_INTRINSIC** Fix cameraMatrix? and distCoeffs? so that only R, T, E , and F\n",
      "        .   matrices are estimated.\n",
      "        .   -   **CALIB_USE_INTRINSIC_GUESS** Optimize some or all of the intrinsic parameters\n",
      "        .   according to the specified flags. Initial values are provided by the user.\n",
      "        .   -   **CALIB_USE_EXTRINSIC_GUESS** R, T contain valid initial values that are optimized further.\n",
      "        .   Otherwise R, T are initialized to the median value of the pattern views (each dimension separately).\n",
      "        .   -   **CALIB_FIX_PRINCIPAL_POINT** Fix the principal points during the optimization.\n",
      "        .   -   **CALIB_FIX_FOCAL_LENGTH** Fix \\f$f^{(j)}_x\\f$ and \\f$f^{(j)}_y\\f$ .\n",
      "        .   -   **CALIB_FIX_ASPECT_RATIO** Optimize \\f$f^{(j)}_y\\f$ . Fix the ratio \\f$f^{(j)}_x/f^{(j)}_y\\f$\n",
      "        .   .\n",
      "        .   -   **CALIB_SAME_FOCAL_LENGTH** Enforce \\f$f^{(0)}_x=f^{(1)}_x\\f$ and \\f$f^{(0)}_y=f^{(1)}_y\\f$ .\n",
      "        .   -   **CALIB_ZERO_TANGENT_DIST** Set tangential distortion coefficients for each camera to\n",
      "        .   zeros and fix there.\n",
      "        .   -   **CALIB_FIX_K1,...,CALIB_FIX_K6** Do not change the corresponding radial\n",
      "        .   distortion coefficient during the optimization. If CALIB_USE_INTRINSIC_GUESS is set,\n",
      "        .   the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   -   **CALIB_RATIONAL_MODEL** Enable coefficients k4, k5, and k6. To provide the backward\n",
      "        .   compatibility, this extra flag should be explicitly specified to make the calibration\n",
      "        .   function use the rational model and return 8 coefficients. If the flag is not set, the\n",
      "        .   function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_THIN_PRISM_MODEL** Coefficients s1, s2, s3 and s4 are enabled. To provide the\n",
      "        .   backward compatibility, this extra flag should be explicitly specified to make the\n",
      "        .   calibration function use the thin prism model and return 12 coefficients. If the flag is not\n",
      "        .   set, the function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_FIX_S1_S2_S3_S4** The thin prism distortion coefficients are not changed during\n",
      "        .   the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n",
      "        .   supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   -   **CALIB_TILTED_MODEL** Coefficients tauX and tauY are enabled. To provide the\n",
      "        .   backward compatibility, this extra flag should be explicitly specified to make the\n",
      "        .   calibration function use the tilted sensor model and return 14 coefficients. If the flag is not\n",
      "        .   set, the function computes and returns only 5 distortion coefficients.\n",
      "        .   -   **CALIB_FIX_TAUX_TAUY** The coefficients of the tilted sensor model are not changed during\n",
      "        .   the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the\n",
      "        .   supplied distCoeffs matrix is used. Otherwise, it is set to 0.\n",
      "        .   @param criteria Termination criteria for the iterative optimization algorithm.\n",
      "        .   \n",
      "        .   The function estimates transformation between two cameras making a stereo pair. If you have a stereo\n",
      "        .   camera where the relative position and orientation of two cameras is fixed, and if you computed\n",
      "        .   poses of an object relative to the first camera and to the second camera, (R1, T1) and (R2, T2),\n",
      "        .   respectively (this can be done with solvePnP ), then those poses definitely relate to each other.\n",
      "        .   This means that, given ( \\f$R_1\\f$,\\f$T_1\\f$ ), it should be possible to compute ( \\f$R_2\\f$,\\f$T_2\\f$ ). You only\n",
      "        .   need to know the position and orientation of the second camera relative to the first camera. This is\n",
      "        .   what the described function does. It computes ( \\f$R\\f$,\\f$T\\f$ ) so that:\n",
      "        .   \n",
      "        .   \\f[R_2=R*R_1\\f]\n",
      "        .   \\f[T_2=R*T_1 + T,\\f]\n",
      "        .   \n",
      "        .   Optionally, it computes the essential matrix E:\n",
      "        .   \n",
      "        .   \\f[E= \\vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} *R\\f]\n",
      "        .   \n",
      "        .   where \\f$T_i\\f$ are components of the translation vector \\f$T\\f$ : \\f$T=[T_0, T_1, T_2]^T\\f$ . And the function\n",
      "        .   can also compute the fundamental matrix F:\n",
      "        .   \n",
      "        .   \\f[F = cameraMatrix2^{-T} E cameraMatrix1^{-1}\\f]\n",
      "        .   \n",
      "        .   Besides the stereo-related information, the function can also perform a full calibration of each of\n",
      "        .   two cameras. However, due to the high dimensionality of the parameter space and noise in the input\n",
      "        .   data, the function can diverge from the correct solution. If the intrinsic parameters can be\n",
      "        .   estimated with high accuracy for each of the cameras individually (for example, using\n",
      "        .   calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the\n",
      "        .   function along with the computed intrinsic parameters. Otherwise, if all the parameters are\n",
      "        .   estimated at once, it makes sense to restrict some parameters, for example, pass\n",
      "        .   CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a\n",
      "        .   reasonable assumption.\n",
      "        .   \n",
      "        .   Similarly to calibrateCamera , the function minimizes the total re-projection error for all the\n",
      "        .   points in all the available views from both cameras. The function returns the final value of the\n",
      "        .   re-projection error.\n",
      "    \n",
      "    stereoRectify(...)\n",
      "        stereoRectify(cameraMatrix1, distCoeffs1, cameraMatrix2, distCoeffs2, imageSize, R, T[, R1[, R2[, P1[, P2[, Q[, flags[, alpha[, newImageSize]]]]]]]]) -> R1, R2, P1, P2, Q, validPixROI1, validPixROI2\n",
      "        .   @brief Computes rectification transforms for each head of a calibrated stereo camera.\n",
      "        .   \n",
      "        .   @param cameraMatrix1 First camera matrix.\n",
      "        .   @param distCoeffs1 First camera distortion parameters.\n",
      "        .   @param cameraMatrix2 Second camera matrix.\n",
      "        .   @param distCoeffs2 Second camera distortion parameters.\n",
      "        .   @param imageSize Size of the image used for stereo calibration.\n",
      "        .   @param R Rotation matrix between the coordinate systems of the first and the second cameras.\n",
      "        .   @param T Translation vector between coordinate systems of the cameras.\n",
      "        .   @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.\n",
      "        .   @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.\n",
      "        .   @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first\n",
      "        .   camera.\n",
      "        .   @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second\n",
      "        .   camera.\n",
      "        .   @param Q Output \\f$4 \\times 4\\f$ disparity-to-depth mapping matrix (see reprojectImageTo3D ).\n",
      "        .   @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set,\n",
      "        .   the function makes the principal points of each camera have the same pixel coordinates in the\n",
      "        .   rectified views. And if the flag is not set, the function may still shift the images in the\n",
      "        .   horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the\n",
      "        .   useful image area.\n",
      "        .   @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default\n",
      "        .   scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified\n",
      "        .   images are zoomed and shifted so that only valid pixels are visible (no black areas after\n",
      "        .   rectification). alpha=1 means that the rectified image is decimated and shifted so that all the\n",
      "        .   pixels from the original images from the cameras are retained in the rectified images (no source\n",
      "        .   image pixels are lost). Obviously, any intermediate value yields an intermediate result between\n",
      "        .   those two extreme cases.\n",
      "        .   @param newImageSize New image resolution after rectification. The same size should be passed to\n",
      "        .   initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0)\n",
      "        .   is passed (default), it is set to the original imageSize . Setting it to larger value can help you\n",
      "        .   preserve details in the original image, especially when there is a big radial distortion.\n",
      "        .   @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels\n",
      "        .   are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller\n",
      "        .   (see the picture below).\n",
      "        .   @param validPixROI2 Optional output rectangles inside the rectified images where all the pixels\n",
      "        .   are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller\n",
      "        .   (see the picture below).\n",
      "        .   \n",
      "        .   The function computes the rotation matrices for each camera that (virtually) make both camera image\n",
      "        .   planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies\n",
      "        .   the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate\n",
      "        .   as input. As output, it provides two rotation matrices and also two projection matrices in the new\n",
      "        .   coordinates. The function distinguishes the following two cases:\n",
      "        .   \n",
      "        .   -   **Horizontal stereo**: the first and the second camera views are shifted relative to each other\n",
      "        .       mainly along the x axis (with possible small vertical shift). In the rectified images, the\n",
      "        .       corresponding epipolar lines in the left and right cameras are horizontal and have the same\n",
      "        .       y-coordinate. P1 and P2 look like:\n",
      "        .   \n",
      "        .       \\f[\\texttt{P1} = \\begin{bmatrix} f & 0 & cx_1 & 0 \\\\ 0 & f & cy & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .       \\f[\\texttt{P2} = \\begin{bmatrix} f & 0 & cx_2 & T_x*f \\\\ 0 & f & cy & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} ,\\f]\n",
      "        .   \n",
      "        .       where \\f$T_x\\f$ is a horizontal shift between the cameras and \\f$cx_1=cx_2\\f$ if\n",
      "        .       CALIB_ZERO_DISPARITY is set.\n",
      "        .   \n",
      "        .   -   **Vertical stereo**: the first and the second camera views are shifted relative to each other\n",
      "        .       mainly in vertical direction (and probably a bit in the horizontal direction too). The epipolar\n",
      "        .       lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:\n",
      "        .   \n",
      "        .       \\f[\\texttt{P1} = \\begin{bmatrix} f & 0 & cx & 0 \\\\ 0 & f & cy_1 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}\\f]\n",
      "        .   \n",
      "        .       \\f[\\texttt{P2} = \\begin{bmatrix} f & 0 & cx & 0 \\\\ 0 & f & cy_2 & T_y*f \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} ,\\f]\n",
      "        .   \n",
      "        .       where \\f$T_y\\f$ is a vertical shift between the cameras and \\f$cy_1=cy_2\\f$ if CALIB_ZERO_DISPARITY is\n",
      "        .       set.\n",
      "        .   \n",
      "        .   As you can see, the first three columns of P1 and P2 will effectively be the new \"rectified\" camera\n",
      "        .   matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to\n",
      "        .   initialize the rectification map for each camera.\n",
      "        .   \n",
      "        .   See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through\n",
      "        .   the corresponding image regions. This means that the images are well rectified, which is what most\n",
      "        .   stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that\n",
      "        .   their interiors are all valid pixels.\n",
      "        .   \n",
      "        .   ![image](pics/stereo_undistort.jpg)\n",
      "    \n",
      "    stereoRectifyUncalibrated(...)\n",
      "        stereoRectifyUncalibrated(points1, points2, F, imgSize[, H1[, H2[, threshold]]]) -> retval, H1, H2\n",
      "        .   @brief Computes a rectification transform for an uncalibrated stereo camera.\n",
      "        .   \n",
      "        .   @param points1 Array of feature points in the first image.\n",
      "        .   @param points2 The corresponding points in the second image. The same formats as in\n",
      "        .   findFundamentalMat are supported.\n",
      "        .   @param F Input fundamental matrix. It can be computed from the same set of point pairs using\n",
      "        .   findFundamentalMat .\n",
      "        .   @param imgSize Size of the image.\n",
      "        .   @param H1 Output rectification homography matrix for the first image.\n",
      "        .   @param H2 Output rectification homography matrix for the second image.\n",
      "        .   @param threshold Optional threshold used to filter out the outliers. If the parameter is greater\n",
      "        .   than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points\n",
      "        .   for which \\f$|\\texttt{points2[i]}^T*\\texttt{F}*\\texttt{points1[i]}|>\\texttt{threshold}\\f$ ) are\n",
      "        .   rejected prior to computing the homographies. Otherwise, all the points are considered inliers.\n",
      "        .   \n",
      "        .   The function computes the rectification transformations without knowing intrinsic parameters of the\n",
      "        .   cameras and their relative position in the space, which explains the suffix \"uncalibrated\". Another\n",
      "        .   related difference from stereoRectify is that the function outputs not the rectification\n",
      "        .   transformations in the object (3D) space, but the planar perspective transformations encoded by the\n",
      "        .   homography matrices H1 and H2 . The function implements the algorithm @cite Hartley99 .\n",
      "        .   \n",
      "        .   @note\n",
      "        .      While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily\n",
      "        .       depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion,\n",
      "        .       it would be better to correct it before computing the fundamental matrix and calling this\n",
      "        .       function. For example, distortion coefficients can be estimated for each head of stereo camera\n",
      "        .       separately by using calibrateCamera . Then, the images can be corrected using undistort , or\n",
      "        .       just the point coordinates can be corrected with undistortPoints .\n",
      "    \n",
      "    stylization(...)\n",
      "        stylization(src[, dst[, sigma_s[, sigma_r]]]) -> dst\n",
      "        .   @brief Stylization aims to produce digital imagery with a wide variety of effects not focused on\n",
      "        .   photorealism. Edge-aware filters are ideal for stylization, as they can abstract regions of low\n",
      "        .   contrast while preserving, or enhancing, high-contrast features.\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src.\n",
      "        .   @param sigma_s %Range between 0 to 200.\n",
      "        .   @param sigma_r %Range between 0 to 1.\n",
      "    \n",
      "    subtract(...)\n",
      "        subtract(src1, src2[, dst[, mask[, dtype]]]) -> dst\n",
      "        .   @brief Calculates the per-element difference between two arrays or array and a scalar.\n",
      "        .   \n",
      "        .   The function subtract calculates:\n",
      "        .   - Difference between two arrays, when both input arrays have the same size and the same number of\n",
      "        .   channels:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1}(I) -  \\texttt{src2}(I)) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   - Difference between an array and a scalar, when src2 is constructed from Scalar or has the same\n",
      "        .   number of elements as `src1.channels()`:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1}(I) -  \\texttt{src2} ) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   - Difference between a scalar and an array, when src1 is constructed from Scalar or has the same\n",
      "        .   number of elements as `src2.channels()`:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src1} -  \\texttt{src2}(I) ) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   - The reverse difference between a scalar and an array in the case of `SubRS`:\n",
      "        .       \\f[\\texttt{dst}(I) =  \\texttt{saturate} ( \\texttt{src2} -  \\texttt{src1}(I) ) \\quad \\texttt{if mask}(I) \\ne0\\f]\n",
      "        .   where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each\n",
      "        .   channel is processed independently.\n",
      "        .   \n",
      "        .   The first function in the list above can be replaced with matrix expressions:\n",
      "        .   @code{.cpp}\n",
      "        .       dst = src1 - src2;\n",
      "        .       dst -= src1; // equivalent to subtract(dst, src1, dst);\n",
      "        .   @endcode\n",
      "        .   The input arrays and the output array can all have the same or different depths. For example, you\n",
      "        .   can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of\n",
      "        .   the output array is determined by dtype parameter. In the second and third cases above, as well as\n",
      "        .   in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this\n",
      "        .   case the output array will have the same depth as the input array, be it src1, src2 or both.\n",
      "        .   @note Saturation is not applied when the output array has the depth CV_32S. You may even get\n",
      "        .   result of an incorrect sign in the case of overflow.\n",
      "        .   @param src1 first input array or a scalar.\n",
      "        .   @param src2 second input array or a scalar.\n",
      "        .   @param dst output array of the same size and the same number of channels as the input array.\n",
      "        .   @param mask optional operation mask; this is an 8-bit single channel array that specifies elements\n",
      "        .   of the output array to be changed.\n",
      "        .   @param dtype optional depth of the output array\n",
      "        .   @sa  add, addWeighted, scaleAdd, Mat::convertTo\n",
      "    \n",
      "    sumElems(...)\n",
      "        sumElems(src) -> retval\n",
      "        .   @brief Calculates the sum of array elements.\n",
      "        .   \n",
      "        .   The function cv::sum calculates and returns the sum of array elements,\n",
      "        .   independently for each channel.\n",
      "        .   @param src input array that must have from 1 to 4 channels.\n",
      "        .   @sa  countNonZero, mean, meanStdDev, norm, minMaxLoc, reduce\n",
      "    \n",
      "    textureFlattening(...)\n",
      "        textureFlattening(src, mask[, dst[, low_threshold[, high_threshold[, kernel_size]]]]) -> dst\n",
      "        .   @brief By retaining only the gradients at edge locations, before integrating with the Poisson solver, one\n",
      "        .   washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge %Detector is used.\n",
      "        .   \n",
      "        .   @param src Input 8-bit 3-channel image.\n",
      "        .   @param mask Input 8-bit 1 or 3-channel image.\n",
      "        .   @param dst Output image with the same size and type as src.\n",
      "        .   @param low_threshold %Range from 0 to 100.\n",
      "        .   @param high_threshold Value \\> 100.\n",
      "        .   @param kernel_size The size of the Sobel kernel to be used.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   The algorithm assumes that the color of the source image is close to that of the destination. This\n",
      "        .   assumption means that when the colors don't match, the source image color gets tinted toward the\n",
      "        .   color of the destination image.\n",
      "    \n",
      "    threshold(...)\n",
      "        threshold(src, thresh, maxval, type[, dst]) -> retval, dst\n",
      "        .   @brief Applies a fixed-level threshold to each array element.\n",
      "        .   \n",
      "        .   The function applies fixed-level thresholding to a multiple-channel array. The function is typically\n",
      "        .   used to get a bi-level (binary) image out of a grayscale image ( #compare could be also used for\n",
      "        .   this purpose) or for removing a noise, that is, filtering out pixels with too small or too large\n",
      "        .   values. There are several types of thresholding supported by the function. They are determined by\n",
      "        .   type parameter.\n",
      "        .   \n",
      "        .   Also, the special values #THRESH_OTSU or #THRESH_TRIANGLE may be combined with one of the\n",
      "        .   above values. In these cases, the function determines the optimal threshold value using the Otsu's\n",
      "        .   or Triangle algorithm and uses it instead of the specified thresh.\n",
      "        .   \n",
      "        .   @note Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.\n",
      "        .   \n",
      "        .   @param src input array (multiple-channel, 8-bit or 32-bit floating point).\n",
      "        .   @param dst output array of the same size  and type and the same number of channels as src.\n",
      "        .   @param thresh threshold value.\n",
      "        .   @param maxval maximum value to use with the #THRESH_BINARY and #THRESH_BINARY_INV thresholding\n",
      "        .   types.\n",
      "        .   @param type thresholding type (see #ThresholdTypes).\n",
      "        .   @return the computed threshold value if Otsu's or Triangle methods used.\n",
      "        .   \n",
      "        .   @sa  adaptiveThreshold, findContours, compare, min, max\n",
      "    \n",
      "    trace(...)\n",
      "        trace(mtx) -> retval\n",
      "        .   @brief Returns the trace of a matrix.\n",
      "        .   \n",
      "        .   The function cv::trace returns the sum of the diagonal elements of the\n",
      "        .   matrix mtx .\n",
      "        .   \\f[\\mathrm{tr} ( \\texttt{mtx} ) =  \\sum _i  \\texttt{mtx} (i,i)\\f]\n",
      "        .   @param mtx input matrix.\n",
      "    \n",
      "    transform(...)\n",
      "        transform(src, m[, dst]) -> dst\n",
      "        .   @brief Performs the matrix transformation of every array element.\n",
      "        .   \n",
      "        .   The function cv::transform performs the matrix transformation of every\n",
      "        .   element of the array src and stores the results in dst :\n",
      "        .   \\f[\\texttt{dst} (I) =  \\texttt{m} \\cdot \\texttt{src} (I)\\f]\n",
      "        .   (when m.cols=src.channels() ), or\n",
      "        .   \\f[\\texttt{dst} (I) =  \\texttt{m} \\cdot [ \\texttt{src} (I); 1]\\f]\n",
      "        .   (when m.cols=src.channels()+1 )\n",
      "        .   \n",
      "        .   Every element of the N -channel array src is interpreted as N -element\n",
      "        .   vector that is transformed using the M x N or M x (N+1) matrix m to\n",
      "        .   M-element vector - the corresponding element of the output array dst .\n",
      "        .   \n",
      "        .   The function may be used for geometrical transformation of\n",
      "        .   N -dimensional points, arbitrary linear color space transformation (such\n",
      "        .   as various kinds of RGB to YUV transforms), shuffling the image\n",
      "        .   channels, and so forth.\n",
      "        .   @param src input array that must have as many channels (1 to 4) as\n",
      "        .   m.cols or m.cols-1.\n",
      "        .   @param dst output array of the same size and depth as src; it has as\n",
      "        .   many channels as m.rows.\n",
      "        .   @param m transformation 2x2 or 2x3 floating-point matrix.\n",
      "        .   @sa perspectiveTransform, getAffineTransform, estimateAffine2D, warpAffine, warpPerspective\n",
      "    \n",
      "    transpose(...)\n",
      "        transpose(src[, dst]) -> dst\n",
      "        .   @brief Transposes a matrix.\n",
      "        .   \n",
      "        .   The function cv::transpose transposes the matrix src :\n",
      "        .   \\f[\\texttt{dst} (i,j) =  \\texttt{src} (j,i)\\f]\n",
      "        .   @note No complex conjugation is done in case of a complex matrix. It\n",
      "        .   should be done separately if needed.\n",
      "        .   @param src input array.\n",
      "        .   @param dst output array of the same type as src.\n",
      "    \n",
      "    triangulatePoints(...)\n",
      "        triangulatePoints(projMatr1, projMatr2, projPoints1, projPoints2[, points4D]) -> points4D\n",
      "        .   @brief Reconstructs points by triangulation.\n",
      "        .   \n",
      "        .   @param projMatr1 3x4 projection matrix of the first camera.\n",
      "        .   @param projMatr2 3x4 projection matrix of the second camera.\n",
      "        .   @param projPoints1 2xN array of feature points in the first image. In case of c++ version it can\n",
      "        .   be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n",
      "        .   @param projPoints2 2xN array of corresponding points in the second image. In case of c++ version\n",
      "        .   it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.\n",
      "        .   @param points4D 4xN array of reconstructed points in homogeneous coordinates.\n",
      "        .   \n",
      "        .   The function reconstructs 3-dimensional points (in homogeneous coordinates) by using their\n",
      "        .   observations with a stereo camera. Projections matrices can be obtained from stereoRectify.\n",
      "        .   \n",
      "        .   @note\n",
      "        .      Keep in mind that all input data should be of float type in order for this function to work.\n",
      "        .   \n",
      "        .   @sa\n",
      "        .      reprojectImageTo3D\n",
      "    \n",
      "    undistort(...)\n",
      "        undistort(src, cameraMatrix, distCoeffs[, dst[, newCameraMatrix]]) -> dst\n",
      "        .   @brief Transforms an image to compensate for lens distortion.\n",
      "        .   \n",
      "        .   The function transforms an image to compensate radial and tangential lens distortion.\n",
      "        .   \n",
      "        .   The function is simply a combination of #initUndistortRectifyMap (with unity R ) and #remap\n",
      "        .   (with bilinear interpolation). See the former function for details of the transformation being\n",
      "        .   performed.\n",
      "        .   \n",
      "        .   Those pixels in the destination image, for which there is no correspondent pixels in the source\n",
      "        .   image, are filled with zeros (black color).\n",
      "        .   \n",
      "        .   A particular subset of the source image that will be visible in the corrected image can be regulated\n",
      "        .   by newCameraMatrix. You can use #getOptimalNewCameraMatrix to compute the appropriate\n",
      "        .   newCameraMatrix depending on your requirements.\n",
      "        .   \n",
      "        .   The camera matrix and the distortion parameters can be determined using #calibrateCamera. If\n",
      "        .   the resolution of images is different from the resolution used at the calibration stage, \\f$f_x,\n",
      "        .   f_y, c_x\\f$ and \\f$c_y\\f$ need to be scaled accordingly, while the distortion coefficients remain\n",
      "        .   the same.\n",
      "        .   \n",
      "        .   @param src Input (distorted) image.\n",
      "        .   @param dst Output (corrected) image that has the same size and type as src .\n",
      "        .   @param cameraMatrix Input camera matrix \\f$A = \\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$\n",
      "        .   of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.\n",
      "        .   @param newCameraMatrix Camera matrix of the distorted image. By default, it is the same as\n",
      "        .   cameraMatrix but you may additionally scale and shift the result by using a different matrix.\n",
      "    \n",
      "    undistortPoints(...)\n",
      "        undistortPoints(src, cameraMatrix, distCoeffs[, dst[, R[, P]]]) -> dst\n",
      "        .   @brief Computes the ideal point coordinates from the observed point coordinates.\n",
      "        .   \n",
      "        .   The function is similar to #undistort and #initUndistortRectifyMap but it operates on a\n",
      "        .   sparse set of points instead of a raster image. Also the function performs a reverse transformation\n",
      "        .   to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a\n",
      "        .   planar object, it does, up to a translation vector, if the proper R is specified.\n",
      "        .   \n",
      "        .   For each observed point coordinate \\f$(u, v)\\f$ the function computes:\n",
      "        .   \\f[\n",
      "        .   \\begin{array}{l}\n",
      "        .   x^{\"}  \\leftarrow (u - c_x)/f_x  \\\\\n",
      "        .   y^{\"}  \\leftarrow (v - c_y)/f_y  \\\\\n",
      "        .   (x',y') = undistort(x^{\"},y^{\"}, \\texttt{distCoeffs}) \\\\\n",
      "        .   {[X\\,Y\\,W]} ^T  \\leftarrow R*[x' \\, y' \\, 1]^T  \\\\\n",
      "        .   x  \\leftarrow X/W  \\\\\n",
      "        .   y  \\leftarrow Y/W  \\\\\n",
      "        .   \\text{only performed if P is specified:} \\\\\n",
      "        .   u'  \\leftarrow x {f'}_x + {c'}_x  \\\\\n",
      "        .   v'  \\leftarrow y {f'}_y + {c'}_y\n",
      "        .   \\end{array}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   where *undistort* is an approximate iterative algorithm that estimates the normalized original\n",
      "        .   point coordinates out of the normalized distorted point coordinates (\"normalized\" means that the\n",
      "        .   coordinates do not depend on the camera matrix).\n",
      "        .   \n",
      "        .   The function can be used for both a stereo camera head or a monocular camera (when R is empty).\n",
      "        .   @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2) (or\n",
      "        .   vector\\<Point2f\\> ).\n",
      "        .   @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector\\<Point2f\\> ) after undistortion and reverse perspective\n",
      "        .   transformation. If matrix P is identity or omitted, dst will contain normalized point coordinates.\n",
      "        .   @param cameraMatrix Camera matrix \\f$\\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\\f$ .\n",
      "        .   @param distCoeffs Input vector of distortion coefficients\n",
      "        .   \\f$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \\tau_x, \\tau_y]]]])\\f$\n",
      "        .   of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.\n",
      "        .   @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by\n",
      "        .   #stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.\n",
      "        .   @param P New camera matrix (3x3) or new projection matrix (3x4) \\f$\\begin{bmatrix} {f'}_x & 0 & {c'}_x & t_x \\\\ 0 & {f'}_y & {c'}_y & t_y \\\\ 0 & 0 & 1 & t_z \\end{bmatrix}\\f$. P1 or P2 computed by\n",
      "        .   #stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.\n",
      "    \n",
      "    undistortPointsIter(...)\n",
      "        undistortPointsIter(src, cameraMatrix, distCoeffs, R, P, criteria[, dst]) -> dst\n",
      "        .   @overload\n",
      "        .       @note Default version of #undistortPoints does 5 iterations to compute undistorted points.\n",
      "    \n",
      "    useOpenVX(...)\n",
      "        useOpenVX() -> retval\n",
      "        .\n",
      "    \n",
      "    useOptimized(...)\n",
      "        useOptimized() -> retval\n",
      "        .   @brief Returns the status of optimized code usage.\n",
      "        .   \n",
      "        .   The function returns true if the optimized code is enabled. Otherwise, it returns false.\n",
      "    \n",
      "    validateDisparity(...)\n",
      "        validateDisparity(disparity, cost, minDisparity, numberOfDisparities[, disp12MaxDisp]) -> disparity\n",
      "        .\n",
      "    \n",
      "    vconcat(...)\n",
      "        vconcat(src[, dst]) -> dst\n",
      "        .   @overload\n",
      "        .    @code{.cpp}\n",
      "        .       std::vector<cv::Mat> matrices = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),\n",
      "        .                                         cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),\n",
      "        .                                         cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};\n",
      "        .   \n",
      "        .       cv::Mat out;\n",
      "        .       cv::vconcat( matrices, out );\n",
      "        .       //out:\n",
      "        .       //[1,   1,   1,   1;\n",
      "        .       // 2,   2,   2,   2;\n",
      "        .       // 3,   3,   3,   3]\n",
      "        .    @endcode\n",
      "        .    @param src input array or vector of matrices. all of the matrices must have the same number of cols and the same depth\n",
      "        .    @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of the src.\n",
      "        .   same depth.\n",
      "    \n",
      "    waitKey(...)\n",
      "        waitKey([, delay]) -> retval\n",
      "        .   @brief Waits for a pressed key.\n",
      "        .   \n",
      "        .   The function waitKey waits for a key event infinitely (when \\f$\\texttt{delay}\\leq 0\\f$ ) or for delay\n",
      "        .   milliseconds, when it is positive. Since the OS has a minimum time between switching threads, the\n",
      "        .   function will not wait exactly delay ms, it will wait at least delay ms, depending on what else is\n",
      "        .   running on your computer at that time. It returns the code of the pressed key or -1 if no key was\n",
      "        .   pressed before the specified time had elapsed.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   This function is the only method in HighGUI that can fetch and handle events, so it needs to be\n",
      "        .   called periodically for normal event processing unless HighGUI is used within an environment that\n",
      "        .   takes care of event processing.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   The function only works if there is at least one HighGUI window created and the window is active.\n",
      "        .   If there are several HighGUI windows, any of them can be active.\n",
      "        .   \n",
      "        .   @param delay Delay in milliseconds. 0 is the special value that means \"forever\".\n",
      "    \n",
      "    waitKeyEx(...)\n",
      "        waitKeyEx([, delay]) -> retval\n",
      "        .   @brief Similar to #waitKey, but returns full key code.\n",
      "        .   \n",
      "        .   @note\n",
      "        .   \n",
      "        .   Key code is implementation specific and depends on used backend: QT/GTK/Win32/etc\n",
      "    \n",
      "    warpAffine(...)\n",
      "        warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]) -> dst\n",
      "        .   @brief Applies an affine transformation to an image.\n",
      "        .   \n",
      "        .   The function warpAffine transforms the source image using the specified matrix:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\texttt{src} ( \\texttt{M} _{11} x +  \\texttt{M} _{12} y +  \\texttt{M} _{13}, \\texttt{M} _{21} x +  \\texttt{M} _{22} y +  \\texttt{M} _{23})\\f]\n",
      "        .   \n",
      "        .   when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted\n",
      "        .   with #invertAffineTransform and then put in the formula above instead of M. The function cannot\n",
      "        .   operate in-place.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image that has the size dsize and the same type as src .\n",
      "        .   @param M \\f$2\\times 3\\f$ transformation matrix.\n",
      "        .   @param dsize size of the output image.\n",
      "        .   @param flags combination of interpolation methods (see #InterpolationFlags) and the optional\n",
      "        .   flag #WARP_INVERSE_MAP that means that M is the inverse transformation (\n",
      "        .   \\f$\\texttt{dst}\\rightarrow\\texttt{src}\\f$ ).\n",
      "        .   @param borderMode pixel extrapolation method (see #BorderTypes); when\n",
      "        .   borderMode=#BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to\n",
      "        .   the \"outliers\" in the source image are not modified by the function.\n",
      "        .   @param borderValue value used in case of a constant border; by default, it is 0.\n",
      "        .   \n",
      "        .   @sa  warpPerspective, resize, remap, getRectSubPix, transform\n",
      "    \n",
      "    warpPerspective(...)\n",
      "        warpPerspective(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]) -> dst\n",
      "        .   @brief Applies a perspective transformation to an image.\n",
      "        .   \n",
      "        .   The function warpPerspective transforms the source image using the specified matrix:\n",
      "        .   \n",
      "        .   \\f[\\texttt{dst} (x,y) =  \\texttt{src} \\left ( \\frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,\n",
      "        .        \\frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \\right )\\f]\n",
      "        .   \n",
      "        .   when the flag #WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert\n",
      "        .   and then put in the formula above instead of M. The function cannot operate in-place.\n",
      "        .   \n",
      "        .   @param src input image.\n",
      "        .   @param dst output image that has the size dsize and the same type as src .\n",
      "        .   @param M \\f$3\\times 3\\f$ transformation matrix.\n",
      "        .   @param dsize size of the output image.\n",
      "        .   @param flags combination of interpolation methods (#INTER_LINEAR or #INTER_NEAREST) and the\n",
      "        .   optional flag #WARP_INVERSE_MAP, that sets M as the inverse transformation (\n",
      "        .   \\f$\\texttt{dst}\\rightarrow\\texttt{src}\\f$ ).\n",
      "        .   @param borderMode pixel extrapolation method (#BORDER_CONSTANT or #BORDER_REPLICATE).\n",
      "        .   @param borderValue value used in case of a constant border; by default, it equals 0.\n",
      "        .   \n",
      "        .   @sa  warpAffine, resize, remap, getRectSubPix, perspectiveTransform\n",
      "    \n",
      "    warpPolar(...)\n",
      "        warpPolar(src, dsize, center, maxRadius, flags[, dst]) -> dst\n",
      "        .   \\brief Remaps an image to polar or semilog-polar coordinates space\n",
      "        .   \n",
      "        .   @anchor polar_remaps_reference_image\n",
      "        .   ![Polar remaps reference](pics/polar_remap_doc.png)\n",
      "        .   \n",
      "        .   Transform the source image using the following transformation:\n",
      "        .   \\f[\n",
      "        .   dst(\\rho , \\phi ) = src(x,y)\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   where\n",
      "        .   \\f[\n",
      "        .   \\begin{array}{l}\n",
      "        .   \\vec{I} = (x - center.x, \\;y - center.y) \\\\\n",
      "        .   \\phi = Kangle \\cdot \\texttt{angle} (\\vec{I}) \\\\\n",
      "        .   \\rho = \\left\\{\\begin{matrix}\n",
      "        .   Klin \\cdot \\texttt{magnitude} (\\vec{I}) & default \\\\\n",
      "        .   Klog \\cdot log_e(\\texttt{magnitude} (\\vec{I})) & if \\; semilog \\\\\n",
      "        .   \\end{matrix}\\right.\n",
      "        .   \\end{array}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   and\n",
      "        .   \\f[\n",
      "        .   \\begin{array}{l}\n",
      "        .   Kangle = dsize.height / 2\\Pi \\\\\n",
      "        .   Klin = dsize.width / maxRadius \\\\\n",
      "        .   Klog = dsize.width / log_e(maxRadius) \\\\\n",
      "        .   \\end{array}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   \\par Linear vs semilog mapping\n",
      "        .   \n",
      "        .   Polar mapping can be linear or semi-log. Add one of #WarpPolarMode to `flags` to specify the polar mapping mode.\n",
      "        .   \n",
      "        .   Linear is the default mode.\n",
      "        .   \n",
      "        .   The semilog mapping emulates the human \"foveal\" vision that permit very high acuity on the line of sight (central vision)\n",
      "        .   in contrast to peripheral vision where acuity is minor.\n",
      "        .   \n",
      "        .   \\par Option on `dsize`:\n",
      "        .   \n",
      "        .   - if both values in `dsize <=0 ` (default),\n",
      "        .   the destination image will have (almost) same area of source bounding circle:\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .   dsize.area  \\leftarrow (maxRadius^2 \\cdot \\Pi) \\\\\n",
      "        .   dsize.width = \\texttt{cvRound}(maxRadius) \\\\\n",
      "        .   dsize.height = \\texttt{cvRound}(maxRadius \\cdot \\Pi) \\\\\n",
      "        .   \\end{array}\\f]\n",
      "        .   \n",
      "        .   \n",
      "        .   - if only `dsize.height <= 0`,\n",
      "        .   the destination image area will be proportional to the bounding circle area but scaled by `Kx * Kx`:\n",
      "        .   \\f[\\begin{array}{l}\n",
      "        .   dsize.height = \\texttt{cvRound}(dsize.width \\cdot \\Pi) \\\\\n",
      "        .   \\end{array}\n",
      "        .   \\f]\n",
      "        .   \n",
      "        .   - if both values in `dsize > 0 `,\n",
      "        .   the destination image will have the given size therefore the area of the bounding circle will be scaled to `dsize`.\n",
      "        .   \n",
      "        .   \n",
      "        .   \\par Reverse mapping\n",
      "        .   \n",
      "        .   You can get reverse mapping adding #WARP_INVERSE_MAP to `flags`\n",
      "        .   \\snippet polar_transforms.cpp InverseMap\n",
      "        .   \n",
      "        .   In addiction, to calculate the original coordinate from a polar mapped coordinate \\f$(rho, phi)->(x, y)\\f$:\n",
      "        .   \\snippet polar_transforms.cpp InverseCoordinate\n",
      "        .   \n",
      "        .   @param src Source image.\n",
      "        .   @param dst Destination image. It will have same type as src.\n",
      "        .   @param dsize The destination image size (see description for valid options).\n",
      "        .   @param center The transformation center.\n",
      "        .   @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude scale parameter too.\n",
      "        .   @param flags A combination of interpolation methods, #InterpolationFlags + #WarpPolarMode.\n",
      "        .               - Add #WARP_POLAR_LINEAR to select linear polar mapping (default)\n",
      "        .               - Add #WARP_POLAR_LOG to select semilog polar mapping\n",
      "        .               - Add #WARP_INVERSE_MAP for reverse mapping.\n",
      "        .   @note\n",
      "        .   -  The function can not operate in-place.\n",
      "        .   -  To calculate magnitude and angle in degrees #cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.\n",
      "        .   -  This function uses #remap. Due to current implementation limitations the size of an input and output images should be less than 32767x32767.\n",
      "        .   \n",
      "        .   @sa cv::remap\n",
      "    \n",
      "    watershed(...)\n",
      "        watershed(image, markers) -> markers\n",
      "        .   @brief Performs a marker-based image segmentation using the watershed algorithm.\n",
      "        .   \n",
      "        .   The function implements one of the variants of watershed, non-parametric marker-based segmentation\n",
      "        .   algorithm, described in @cite Meyer92 .\n",
      "        .   \n",
      "        .   Before passing the image to the function, you have to roughly outline the desired regions in the\n",
      "        .   image markers with positive (\\>0) indices. So, every region is represented as one or more connected\n",
      "        .   components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary\n",
      "        .   mask using #findContours and #drawContours (see the watershed.cpp demo). The markers are \"seeds\" of\n",
      "        .   the future image regions. All the other pixels in markers , whose relation to the outlined regions\n",
      "        .   is not known and should be defined by the algorithm, should be set to 0's. In the function output,\n",
      "        .   each pixel in markers is set to a value of the \"seed\" components or to -1 at boundaries between the\n",
      "        .   regions.\n",
      "        .   \n",
      "        .   @note Any two neighbor connected components are not necessarily separated by a watershed boundary\n",
      "        .   (-1's pixels); for example, they can touch each other in the initial marker image passed to the\n",
      "        .   function.\n",
      "        .   \n",
      "        .   @param image Input 8-bit 3-channel image.\n",
      "        .   @param markers Input/output 32-bit single-channel image (map) of markers. It should have the same\n",
      "        .   size as image .\n",
      "        .   \n",
      "        .   @sa findContours\n",
      "        .   \n",
      "        .   @ingroup imgproc_misc\n",
      "    \n",
      "    writeOpticalFlow(...)\n",
      "        writeOpticalFlow(path, flow) -> retval\n",
      "        .   @brief Write a .flo to disk\n",
      "        .   \n",
      "        .    @param path Path to the file to be written\n",
      "        .    @param flow Flow field to be stored\n",
      "        .   \n",
      "        .    The function stores a flow field in a file, returns true on success, false otherwise.\n",
      "        .    The flow field must be a 2-channel, floating-point matrix (CV_32FC2). First channel corresponds\n",
      "        .    to the flow in the horizontal direction (u), second - vertical (v).\n",
      "\n",
      "DATA\n",
      "    ACCESS_FAST = 67108864\n",
      "    ACCESS_MASK = 50331648\n",
      "    ACCESS_READ = 16777216\n",
      "    ACCESS_RW = 50331648\n",
      "    ACCESS_WRITE = 33554432\n",
      "    ADAPTIVE_THRESH_GAUSSIAN_C = 1\n",
      "    ADAPTIVE_THRESH_MEAN_C = 0\n",
      "    AGAST_FEATURE_DETECTOR_AGAST_5_8 = 0\n",
      "    AGAST_FEATURE_DETECTOR_AGAST_7_12D = 1\n",
      "    AGAST_FEATURE_DETECTOR_AGAST_7_12S = 2\n",
      "    AGAST_FEATURE_DETECTOR_NONMAX_SUPPRESSION = 10001\n",
      "    AGAST_FEATURE_DETECTOR_OAST_9_16 = 3\n",
      "    AGAST_FEATURE_DETECTOR_THRESHOLD = 10000\n",
      "    AKAZE_DESCRIPTOR_KAZE = 3\n",
      "    AKAZE_DESCRIPTOR_KAZE_UPRIGHT = 2\n",
      "    AKAZE_DESCRIPTOR_MLDB = 5\n",
      "    AKAZE_DESCRIPTOR_MLDB_UPRIGHT = 4\n",
      "    AgastFeatureDetector_AGAST_5_8 = 0\n",
      "    AgastFeatureDetector_AGAST_7_12d = 1\n",
      "    AgastFeatureDetector_AGAST_7_12s = 2\n",
      "    AgastFeatureDetector_NONMAX_SUPPRESSION = 10001\n",
      "    AgastFeatureDetector_OAST_9_16 = 3\n",
      "    AgastFeatureDetector_THRESHOLD = 10000\n",
      "    BORDER_CONSTANT = 0\n",
      "    BORDER_DEFAULT = 4\n",
      "    BORDER_ISOLATED = 16\n",
      "    BORDER_REFLECT = 2\n",
      "    BORDER_REFLECT101 = 4\n",
      "    BORDER_REFLECT_101 = 4\n",
      "    BORDER_REPLICATE = 1\n",
      "    BORDER_TRANSPARENT = 5\n",
      "    BORDER_WRAP = 3\n",
      "    CALIB_CB_ACCURACY = 32\n",
      "    CALIB_CB_ADAPTIVE_THRESH = 1\n",
      "    CALIB_CB_ASYMMETRIC_GRID = 2\n",
      "    CALIB_CB_CLUSTERING = 4\n",
      "    CALIB_CB_EXHAUSTIVE = 16\n",
      "    CALIB_CB_FAST_CHECK = 8\n",
      "    CALIB_CB_FILTER_QUADS = 4\n",
      "    CALIB_CB_NORMALIZE_IMAGE = 2\n",
      "    CALIB_CB_SYMMETRIC_GRID = 1\n",
      "    CALIB_FIX_ASPECT_RATIO = 2\n",
      "    CALIB_FIX_FOCAL_LENGTH = 16\n",
      "    CALIB_FIX_INTRINSIC = 256\n",
      "    CALIB_FIX_K1 = 32\n",
      "    CALIB_FIX_K2 = 64\n",
      "    CALIB_FIX_K3 = 128\n",
      "    CALIB_FIX_K4 = 2048\n",
      "    CALIB_FIX_K5 = 4096\n",
      "    CALIB_FIX_K6 = 8192\n",
      "    CALIB_FIX_PRINCIPAL_POINT = 4\n",
      "    CALIB_FIX_S1_S2_S3_S4 = 65536\n",
      "    CALIB_FIX_TANGENT_DIST = 2097152\n",
      "    CALIB_FIX_TAUX_TAUY = 524288\n",
      "    CALIB_HAND_EYE_ANDREFF = 3\n",
      "    CALIB_HAND_EYE_DANIILIDIS = 4\n",
      "    CALIB_HAND_EYE_HORAUD = 2\n",
      "    CALIB_HAND_EYE_PARK = 1\n",
      "    CALIB_HAND_EYE_TSAI = 0\n",
      "    CALIB_NINTRINSIC = 18\n",
      "    CALIB_RATIONAL_MODEL = 16384\n",
      "    CALIB_SAME_FOCAL_LENGTH = 512\n",
      "    CALIB_THIN_PRISM_MODEL = 32768\n",
      "    CALIB_TILTED_MODEL = 262144\n",
      "    CALIB_USE_EXTRINSIC_GUESS = 4194304\n",
      "    CALIB_USE_INTRINSIC_GUESS = 1\n",
      "    CALIB_USE_LU = 131072\n",
      "    CALIB_USE_QR = 1048576\n",
      "    CALIB_ZERO_DISPARITY = 1024\n",
      "    CALIB_ZERO_TANGENT_DIST = 8\n",
      "    CAP_ANDROID = 1000\n",
      "    CAP_ANY = 0\n",
      "    CAP_ARAVIS = 2100\n",
      "    CAP_AVFOUNDATION = 1200\n",
      "    CAP_CMU1394 = 300\n",
      "    CAP_DC1394 = 300\n",
      "    CAP_DSHOW = 700\n",
      "    CAP_FFMPEG = 1900\n",
      "    CAP_FIREWARE = 300\n",
      "    CAP_FIREWIRE = 300\n",
      "    CAP_GIGANETIX = 1300\n",
      "    CAP_GPHOTO2 = 1700\n",
      "    CAP_GSTREAMER = 1800\n",
      "    CAP_IEEE1394 = 300\n",
      "    CAP_IMAGES = 2000\n",
      "    CAP_INTELPERC = 1500\n",
      "    CAP_INTELPERC_DEPTH_GENERATOR = 536870912\n",
      "    CAP_INTELPERC_DEPTH_MAP = 0\n",
      "    CAP_INTELPERC_GENERATORS_MASK = 939524096\n",
      "    CAP_INTELPERC_IMAGE = 3\n",
      "    CAP_INTELPERC_IMAGE_GENERATOR = 268435456\n",
      "    CAP_INTELPERC_IR_GENERATOR = 134217728\n",
      "    CAP_INTELPERC_IR_MAP = 2\n",
      "    CAP_INTELPERC_UVDEPTH_MAP = 1\n",
      "    CAP_INTEL_MFX = 2300\n",
      "    CAP_MSMF = 1400\n",
      "    CAP_OPENCV_MJPEG = 2200\n",
      "    CAP_OPENNI = 900\n",
      "    CAP_OPENNI2 = 1600\n",
      "    CAP_OPENNI2_ASUS = 1610\n",
      "    CAP_OPENNI_ASUS = 910\n",
      "    CAP_OPENNI_BGR_IMAGE = 5\n",
      "    CAP_OPENNI_DEPTH_GENERATOR = -2147483648\n",
      "    CAP_OPENNI_DEPTH_GENERATOR_BASELINE = -2147483546\n",
      "    CAP_OPENNI_DEPTH_GENERATOR_FOCAL_LENGTH = -2147483545\n",
      "    CAP_OPENNI_DEPTH_GENERATOR_PRESENT = -2147483539\n",
      "    CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION = -2147483544\n",
      "    CAP_OPENNI_DEPTH_GENERATOR_REGISTRATION_ON = -2147483544\n",
      "    CAP_OPENNI_DEPTH_MAP = 0\n",
      "    CAP_OPENNI_DISPARITY_MAP = 2\n",
      "    CAP_OPENNI_DISPARITY_MAP_32F = 3\n",
      "    CAP_OPENNI_GENERATORS_MASK = -536870912\n",
      "    CAP_OPENNI_GRAY_IMAGE = 6\n",
      "    CAP_OPENNI_IMAGE_GENERATOR = 1073741824\n",
      "    CAP_OPENNI_IMAGE_GENERATOR_OUTPUT_MODE = 1073741924\n",
      "    CAP_OPENNI_IMAGE_GENERATOR_PRESENT = 1073741933\n",
      "    CAP_OPENNI_IR_GENERATOR = 536870912\n",
      "    CAP_OPENNI_IR_GENERATOR_PRESENT = 536871021\n",
      "    CAP_OPENNI_IR_IMAGE = 7\n",
      "    CAP_OPENNI_POINT_CLOUD_MAP = 1\n",
      "    CAP_OPENNI_QVGA_30HZ = 3\n",
      "    CAP_OPENNI_QVGA_60HZ = 4\n",
      "    CAP_OPENNI_SXGA_15HZ = 1\n",
      "    CAP_OPENNI_SXGA_30HZ = 2\n",
      "    CAP_OPENNI_VALID_DEPTH_MASK = 4\n",
      "    CAP_OPENNI_VGA_30HZ = 0\n",
      "    CAP_PROP_APERTURE = 17008\n",
      "    CAP_PROP_AUTOFOCUS = 39\n",
      "    CAP_PROP_AUTO_EXPOSURE = 21\n",
      "    CAP_PROP_AUTO_WB = 44\n",
      "    CAP_PROP_BACKEND = 42\n",
      "    CAP_PROP_BACKLIGHT = 32\n",
      "    CAP_PROP_BRIGHTNESS = 10\n",
      "    CAP_PROP_BUFFERSIZE = 38\n",
      "    CAP_PROP_CHANNEL = 43\n",
      "    CAP_PROP_CONTRAST = 11\n",
      "    CAP_PROP_CONVERT_RGB = 16\n",
      "    CAP_PROP_DC1394_MAX = 31\n",
      "    CAP_PROP_DC1394_MODE_AUTO = -2\n",
      "    CAP_PROP_DC1394_MODE_MANUAL = -3\n",
      "    CAP_PROP_DC1394_MODE_ONE_PUSH_AUTO = -1\n",
      "    CAP_PROP_DC1394_OFF = -4\n",
      "    CAP_PROP_EXPOSURE = 15\n",
      "    CAP_PROP_EXPOSUREPROGRAM = 17009\n",
      "    CAP_PROP_FOCUS = 28\n",
      "    CAP_PROP_FORMAT = 8\n",
      "    CAP_PROP_FOURCC = 6\n",
      "    CAP_PROP_FPS = 5\n",
      "    CAP_PROP_FRAME_COUNT = 7\n",
      "    CAP_PROP_FRAME_HEIGHT = 4\n",
      "    CAP_PROP_FRAME_WIDTH = 3\n",
      "    CAP_PROP_GAIN = 14\n",
      "    CAP_PROP_GAMMA = 22\n",
      "    CAP_PROP_GIGA_FRAME_HEIGH_MAX = 10004\n",
      "    CAP_PROP_GIGA_FRAME_OFFSET_X = 10001\n",
      "    CAP_PROP_GIGA_FRAME_OFFSET_Y = 10002\n",
      "    CAP_PROP_GIGA_FRAME_SENS_HEIGH = 10006\n",
      "    CAP_PROP_GIGA_FRAME_SENS_WIDTH = 10005\n",
      "    CAP_PROP_GIGA_FRAME_WIDTH_MAX = 10003\n",
      "    CAP_PROP_GPHOTO2_COLLECT_MSGS = 17005\n",
      "    CAP_PROP_GPHOTO2_FLUSH_MSGS = 17006\n",
      "    CAP_PROP_GPHOTO2_PREVIEW = 17001\n",
      "    CAP_PROP_GPHOTO2_RELOAD_CONFIG = 17003\n",
      "    CAP_PROP_GPHOTO2_RELOAD_ON_CHANGE = 17004\n",
      "    CAP_PROP_GPHOTO2_WIDGET_ENUMERATE = 17002\n",
      "    CAP_PROP_GSTREAMER_QUEUE_LENGTH = 200\n",
      "    CAP_PROP_GUID = 29\n",
      "    CAP_PROP_HUE = 13\n",
      "    CAP_PROP_IMAGES_BASE = 18000\n",
      "    CAP_PROP_IMAGES_LAST = 19000\n",
      "    CAP_PROP_INTELPERC_DEPTH_CONFIDENCE_THRESHOLD = 11005\n",
      "    CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_HORZ = 11006\n",
      "    CAP_PROP_INTELPERC_DEPTH_FOCAL_LENGTH_VERT = 11007\n",
      "    CAP_PROP_INTELPERC_DEPTH_LOW_CONFIDENCE_VALUE = 11003\n",
      "    CAP_PROP_INTELPERC_DEPTH_SATURATION_VALUE = 11004\n",
      "    CAP_PROP_INTELPERC_PROFILE_COUNT = 11001\n",
      "    CAP_PROP_INTELPERC_PROFILE_IDX = 11002\n",
      "    CAP_PROP_IOS_DEVICE_EXPOSURE = 9002\n",
      "    CAP_PROP_IOS_DEVICE_FLASH = 9003\n",
      "    CAP_PROP_IOS_DEVICE_FOCUS = 9001\n",
      "    CAP_PROP_IOS_DEVICE_TORCH = 9005\n",
      "    CAP_PROP_IOS_DEVICE_WHITEBALANCE = 9004\n",
      "    CAP_PROP_IRIS = 36\n",
      "    CAP_PROP_ISO_SPEED = 30\n",
      "    CAP_PROP_MODE = 9\n",
      "    CAP_PROP_MONOCHROME = 19\n",
      "    CAP_PROP_OPENNI2_MIRROR = 111\n",
      "    CAP_PROP_OPENNI2_SYNC = 110\n",
      "    CAP_PROP_OPENNI_APPROX_FRAME_SYNC = 105\n",
      "    CAP_PROP_OPENNI_BASELINE = 102\n",
      "    CAP_PROP_OPENNI_CIRCLE_BUFFER = 107\n",
      "    CAP_PROP_OPENNI_FOCAL_LENGTH = 103\n",
      "    CAP_PROP_OPENNI_FRAME_MAX_DEPTH = 101\n",
      "    CAP_PROP_OPENNI_GENERATOR_PRESENT = 109\n",
      "    CAP_PROP_OPENNI_MAX_BUFFER_SIZE = 106\n",
      "    CAP_PROP_OPENNI_MAX_TIME_DURATION = 108\n",
      "    CAP_PROP_OPENNI_OUTPUT_MODE = 100\n",
      "    CAP_PROP_OPENNI_REGISTRATION = 104\n",
      "    CAP_PROP_OPENNI_REGISTRATION_ON = 104\n",
      "    CAP_PROP_PAN = 33\n",
      "    CAP_PROP_POS_AVI_RATIO = 2\n",
      "    CAP_PROP_POS_FRAMES = 1\n",
      "    CAP_PROP_POS_MSEC = 0\n",
      "    CAP_PROP_PVAPI_BINNINGX = 304\n",
      "    CAP_PROP_PVAPI_BINNINGY = 305\n",
      "    CAP_PROP_PVAPI_DECIMATIONHORIZONTAL = 302\n",
      "    CAP_PROP_PVAPI_DECIMATIONVERTICAL = 303\n",
      "    CAP_PROP_PVAPI_FRAMESTARTTRIGGERMODE = 301\n",
      "    CAP_PROP_PVAPI_MULTICASTIP = 300\n",
      "    CAP_PROP_PVAPI_PIXELFORMAT = 306\n",
      "    CAP_PROP_RECTIFICATION = 18\n",
      "    CAP_PROP_ROLL = 35\n",
      "    CAP_PROP_SAR_DEN = 41\n",
      "    CAP_PROP_SAR_NUM = 40\n",
      "    CAP_PROP_SATURATION = 12\n",
      "    CAP_PROP_SETTINGS = 37\n",
      "    CAP_PROP_SHARPNESS = 20\n",
      "    CAP_PROP_SPEED = 17007\n",
      "    CAP_PROP_TEMPERATURE = 23\n",
      "    CAP_PROP_TILT = 34\n",
      "    CAP_PROP_TRIGGER = 24\n",
      "    CAP_PROP_TRIGGER_DELAY = 25\n",
      "    CAP_PROP_VIEWFINDER = 17010\n",
      "    CAP_PROP_WB_TEMPERATURE = 45\n",
      "    CAP_PROP_WHITE_BALANCE_BLUE_U = 17\n",
      "    CAP_PROP_WHITE_BALANCE_RED_V = 26\n",
      "    CAP_PROP_XI_ACQ_BUFFER_SIZE = 548\n",
      "    CAP_PROP_XI_ACQ_BUFFER_SIZE_UNIT = 549\n",
      "    CAP_PROP_XI_ACQ_FRAME_BURST_COUNT = 499\n",
      "    CAP_PROP_XI_ACQ_TIMING_MODE = 538\n",
      "    CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_COMMIT = 552\n",
      "    CAP_PROP_XI_ACQ_TRANSPORT_BUFFER_SIZE = 550\n",
      "    CAP_PROP_XI_AEAG = 415\n",
      "    CAP_PROP_XI_AEAG_LEVEL = 419\n",
      "    CAP_PROP_XI_AEAG_ROI_HEIGHT = 442\n",
      "    CAP_PROP_XI_AEAG_ROI_OFFSET_X = 439\n",
      "    CAP_PROP_XI_AEAG_ROI_OFFSET_Y = 440\n",
      "    CAP_PROP_XI_AEAG_ROI_WIDTH = 441\n",
      "    CAP_PROP_XI_AE_MAX_LIMIT = 417\n",
      "    CAP_PROP_XI_AG_MAX_LIMIT = 418\n",
      "    CAP_PROP_XI_APPLY_CMS = 471\n",
      "    CAP_PROP_XI_AUTO_BANDWIDTH_CALCULATION = 573\n",
      "    CAP_PROP_XI_AUTO_WB = 414\n",
      "    CAP_PROP_XI_AVAILABLE_BANDWIDTH = 539\n",
      "    CAP_PROP_XI_BINNING_HORIZONTAL = 429\n",
      "    CAP_PROP_XI_BINNING_PATTERN = 430\n",
      "    CAP_PROP_XI_BINNING_SELECTOR = 427\n",
      "    CAP_PROP_XI_BINNING_VERTICAL = 428\n",
      "    CAP_PROP_XI_BPC = 445\n",
      "    CAP_PROP_XI_BUFFERS_QUEUE_SIZE = 551\n",
      "    CAP_PROP_XI_BUFFER_POLICY = 540\n",
      "    CAP_PROP_XI_CC_MATRIX_00 = 479\n",
      "    CAP_PROP_XI_CC_MATRIX_01 = 480\n",
      "    CAP_PROP_XI_CC_MATRIX_02 = 481\n",
      "    CAP_PROP_XI_CC_MATRIX_03 = 482\n",
      "    CAP_PROP_XI_CC_MATRIX_10 = 483\n",
      "    CAP_PROP_XI_CC_MATRIX_11 = 484\n",
      "    CAP_PROP_XI_CC_MATRIX_12 = 485\n",
      "    CAP_PROP_XI_CC_MATRIX_13 = 486\n",
      "    CAP_PROP_XI_CC_MATRIX_20 = 487\n",
      "    CAP_PROP_XI_CC_MATRIX_21 = 488\n",
      "    CAP_PROP_XI_CC_MATRIX_22 = 489\n",
      "    CAP_PROP_XI_CC_MATRIX_23 = 490\n",
      "    CAP_PROP_XI_CC_MATRIX_30 = 491\n",
      "    CAP_PROP_XI_CC_MATRIX_31 = 492\n",
      "    CAP_PROP_XI_CC_MATRIX_32 = 493\n",
      "    CAP_PROP_XI_CC_MATRIX_33 = 494\n",
      "    CAP_PROP_XI_CHIP_TEMP = 468\n",
      "    CAP_PROP_XI_CMS = 470\n",
      "    CAP_PROP_XI_COLOR_FILTER_ARRAY = 475\n",
      "    CAP_PROP_XI_COLUMN_FPN_CORRECTION = 555\n",
      "    CAP_PROP_XI_COOLING = 466\n",
      "    CAP_PROP_XI_COUNTER_SELECTOR = 536\n",
      "    CAP_PROP_XI_COUNTER_VALUE = 537\n",
      "    CAP_PROP_XI_DATA_FORMAT = 401\n",
      "    CAP_PROP_XI_DEBOUNCE_EN = 507\n",
      "    CAP_PROP_XI_DEBOUNCE_POL = 510\n",
      "    CAP_PROP_XI_DEBOUNCE_T0 = 508\n",
      "    CAP_PROP_XI_DEBOUNCE_T1 = 509\n",
      "    CAP_PROP_XI_DEBUG_LEVEL = 572\n",
      "    CAP_PROP_XI_DECIMATION_HORIZONTAL = 433\n",
      "    CAP_PROP_XI_DECIMATION_PATTERN = 434\n",
      "    CAP_PROP_XI_DECIMATION_SELECTOR = 431\n",
      "    CAP_PROP_XI_DECIMATION_VERTICAL = 432\n",
      "    CAP_PROP_XI_DEFAULT_CC_MATRIX = 495\n",
      "    CAP_PROP_XI_DEVICE_MODEL_ID = 521\n",
      "    CAP_PROP_XI_DEVICE_RESET = 554\n",
      "    CAP_PROP_XI_DEVICE_SN = 522\n",
      "    CAP_PROP_XI_DOWNSAMPLING = 400\n",
      "    CAP_PROP_XI_DOWNSAMPLING_TYPE = 426\n",
      "    CAP_PROP_XI_EXPOSURE = 421\n",
      "    CAP_PROP_XI_EXPOSURE_BURST_COUNT = 422\n",
      "    CAP_PROP_XI_EXP_PRIORITY = 416\n",
      "    CAP_PROP_XI_FFS_ACCESS_KEY = 583\n",
      "    CAP_PROP_XI_FFS_FILE_ID = 594\n",
      "    CAP_PROP_XI_FFS_FILE_SIZE = 580\n",
      "    CAP_PROP_XI_FRAMERATE = 535\n",
      "    CAP_PROP_XI_FREE_FFS_SIZE = 581\n",
      "    CAP_PROP_XI_GAIN = 424\n",
      "    CAP_PROP_XI_GAIN_SELECTOR = 423\n",
      "    CAP_PROP_XI_GAMMAC = 477\n",
      "    CAP_PROP_XI_GAMMAY = 476\n",
      "    CAP_PROP_XI_GPI_LEVEL = 408\n",
      "    CAP_PROP_XI_GPI_MODE = 407\n",
      "    CAP_PROP_XI_GPI_SELECTOR = 406\n",
      "    CAP_PROP_XI_GPO_MODE = 410\n",
      "    CAP_PROP_XI_GPO_SELECTOR = 409\n",
      "    CAP_PROP_XI_HDR = 559\n",
      "    CAP_PROP_XI_HDR_KNEEPOINT_COUNT = 560\n",
      "    CAP_PROP_XI_HDR_T1 = 561\n",
      "    CAP_PROP_XI_HDR_T2 = 562\n",
      "    CAP_PROP_XI_HEIGHT = 452\n",
      "    CAP_PROP_XI_HOUS_BACK_SIDE_TEMP = 590\n",
      "    CAP_PROP_XI_HOUS_TEMP = 469\n",
      "    CAP_PROP_XI_HW_REVISION = 571\n",
      "    CAP_PROP_XI_IMAGE_BLACK_LEVEL = 565\n",
      "    CAP_PROP_XI_IMAGE_DATA_BIT_DEPTH = 462\n",
      "    CAP_PROP_XI_IMAGE_DATA_FORMAT = 435\n",
      "    CAP_PROP_XI_IMAGE_DATA_FORMAT_RGB32_ALPHA = 529\n",
      "    CAP_PROP_XI_IMAGE_IS_COLOR = 474\n",
      "    CAP_PROP_XI_IMAGE_PAYLOAD_SIZE = 530\n",
      "    CAP_PROP_XI_IS_COOLED = 465\n",
      "    CAP_PROP_XI_IS_DEVICE_EXIST = 547\n",
      "    CAP_PROP_XI_KNEEPOINT1 = 563\n",
      "    CAP_PROP_XI_KNEEPOINT2 = 564\n",
      "    CAP_PROP_XI_LED_MODE = 412\n",
      "    CAP_PROP_XI_LED_SELECTOR = 411\n",
      "    CAP_PROP_XI_LENS_APERTURE_VALUE = 512\n",
      "    CAP_PROP_XI_LENS_FEATURE = 518\n",
      "    CAP_PROP_XI_LENS_FEATURE_SELECTOR = 517\n",
      "    CAP_PROP_XI_LENS_FOCAL_LENGTH = 516\n",
      "    CAP_PROP_XI_LENS_FOCUS_DISTANCE = 515\n",
      "    CAP_PROP_XI_LENS_FOCUS_MOVE = 514\n",
      "    CAP_PROP_XI_LENS_FOCUS_MOVEMENT_VALUE = 513\n",
      "    CAP_PROP_XI_LENS_MODE = 511\n",
      "    CAP_PROP_XI_LIMIT_BANDWIDTH = 459\n",
      "    CAP_PROP_XI_LUT_EN = 541\n",
      "    CAP_PROP_XI_LUT_INDEX = 542\n",
      "    CAP_PROP_XI_LUT_VALUE = 543\n",
      "    CAP_PROP_XI_MANUAL_WB = 413\n",
      "    CAP_PROP_XI_OFFSET_X = 402\n",
      "    CAP_PROP_XI_OFFSET_Y = 403\n",
      "    CAP_PROP_XI_OUTPUT_DATA_BIT_DEPTH = 461\n",
      "    CAP_PROP_XI_OUTPUT_DATA_PACKING = 463\n",
      "    CAP_PROP_XI_OUTPUT_DATA_PACKING_TYPE = 464\n",
      "    CAP_PROP_XI_RECENT_FRAME = 553\n",
      "    CAP_PROP_XI_REGION_MODE = 595\n",
      "    CAP_PROP_XI_REGION_SELECTOR = 589\n",
      "    CAP_PROP_XI_ROW_FPN_CORRECTION = 591\n",
      "    CAP_PROP_XI_SENSOR_BOARD_TEMP = 596\n",
      "    CAP_PROP_XI_SENSOR_CLOCK_FREQ_HZ = 532\n",
      "    CAP_PROP_XI_SENSOR_CLOCK_FREQ_INDEX = 533\n",
      "    CAP_PROP_XI_SENSOR_DATA_BIT_DEPTH = 460\n",
      "    CAP_PROP_XI_SENSOR_FEATURE_SELECTOR = 585\n",
      "    CAP_PROP_XI_SENSOR_FEATURE_VALUE = 586\n",
      "    CAP_PROP_XI_SENSOR_MODE = 558\n",
      "    CAP_PROP_XI_SENSOR_OUTPUT_CHANNEL_COUNT = 534\n",
      "    CAP_PROP_XI_SENSOR_TAPS = 437\n",
      "    CAP_PROP_XI_SHARPNESS = 478\n",
      "    CAP_PROP_XI_SHUTTER_TYPE = 436\n",
      "    CAP_PROP_XI_TARGET_TEMP = 467\n",
      "    CAP_PROP_XI_TEST_PATTERN = 588\n",
      "    CAP_PROP_XI_TEST_PATTERN_GENERATOR_SELECTOR = 587\n",
      "    CAP_PROP_XI_TIMEOUT = 420\n",
      "    CAP_PROP_XI_TRANSPORT_PIXEL_FORMAT = 531\n",
      "    CAP_PROP_XI_TRG_DELAY = 544\n",
      "    CAP_PROP_XI_TRG_SELECTOR = 498\n",
      "    CAP_PROP_XI_TRG_SOFTWARE = 405\n",
      "    CAP_PROP_XI_TRG_SOURCE = 404\n",
      "    CAP_PROP_XI_TS_RST_MODE = 545\n",
      "    CAP_PROP_XI_TS_RST_SOURCE = 546\n",
      "    CAP_PROP_XI_USED_FFS_SIZE = 582\n",
      "    CAP_PROP_XI_WB_KB = 450\n",
      "    CAP_PROP_XI_WB_KG = 449\n",
      "    CAP_PROP_XI_WB_KR = 448\n",
      "    CAP_PROP_XI_WIDTH = 451\n",
      "    CAP_PROP_ZOOM = 27\n",
      "    CAP_PVAPI = 800\n",
      "    CAP_PVAPI_DECIMATION_2OUTOF16 = 8\n",
      "    CAP_PVAPI_DECIMATION_2OUTOF4 = 2\n",
      "    CAP_PVAPI_DECIMATION_2OUTOF8 = 4\n",
      "    CAP_PVAPI_DECIMATION_OFF = 1\n",
      "    CAP_PVAPI_FSTRIGMODE_FIXEDRATE = 3\n",
      "    CAP_PVAPI_FSTRIGMODE_FREERUN = 0\n",
      "    CAP_PVAPI_FSTRIGMODE_SOFTWARE = 4\n",
      "    CAP_PVAPI_FSTRIGMODE_SYNCIN1 = 1\n",
      "    CAP_PVAPI_FSTRIGMODE_SYNCIN2 = 2\n",
      "    CAP_PVAPI_PIXELFORMAT_BAYER16 = 4\n",
      "    CAP_PVAPI_PIXELFORMAT_BAYER8 = 3\n",
      "    CAP_PVAPI_PIXELFORMAT_BGR24 = 6\n",
      "    CAP_PVAPI_PIXELFORMAT_BGRA32 = 8\n",
      "    CAP_PVAPI_PIXELFORMAT_MONO16 = 2\n",
      "    CAP_PVAPI_PIXELFORMAT_MONO8 = 1\n",
      "    CAP_PVAPI_PIXELFORMAT_RGB24 = 5\n",
      "    CAP_PVAPI_PIXELFORMAT_RGBA32 = 7\n",
      "    CAP_QT = 500\n",
      "    CAP_REALSENSE = 1500\n",
      "    CAP_UNICAP = 600\n",
      "    CAP_V4L = 200\n",
      "    CAP_V4L2 = 200\n",
      "    CAP_VFW = 200\n",
      "    CAP_WINRT = 1410\n",
      "    CAP_XIAPI = 1100\n",
      "    CAP_XINE = 2400\n",
      "    CASCADE_DO_CANNY_PRUNING = 1\n",
      "    CASCADE_DO_ROUGH_SEARCH = 8\n",
      "    CASCADE_FIND_BIGGEST_OBJECT = 4\n",
      "    CASCADE_SCALE_IMAGE = 2\n",
      "    CCL_DEFAULT = -1\n",
      "    CCL_GRANA = 1\n",
      "    CCL_WU = 0\n",
      "    CC_STAT_AREA = 4\n",
      "    CC_STAT_HEIGHT = 3\n",
      "    CC_STAT_LEFT = 0\n",
      "    CC_STAT_MAX = 5\n",
      "    CC_STAT_TOP = 1\n",
      "    CC_STAT_WIDTH = 2\n",
      "    CHAIN_APPROX_NONE = 1\n",
      "    CHAIN_APPROX_SIMPLE = 2\n",
      "    CHAIN_APPROX_TC89_KCOS = 4\n",
      "    CHAIN_APPROX_TC89_L1 = 3\n",
      "    CIRCLES_GRID_FINDER_PARAMETERS_ASYMMETRIC_GRID = 1\n",
      "    CIRCLES_GRID_FINDER_PARAMETERS_SYMMETRIC_GRID = 0\n",
      "    CMP_EQ = 0\n",
      "    CMP_GE = 2\n",
      "    CMP_GT = 1\n",
      "    CMP_LE = 4\n",
      "    CMP_LT = 3\n",
      "    CMP_NE = 5\n",
      "    COLORMAP_AUTUMN = 0\n",
      "    COLORMAP_BONE = 1\n",
      "    COLORMAP_CIVIDIS = 17\n",
      "    COLORMAP_COOL = 8\n",
      "    COLORMAP_HOT = 11\n",
      "    COLORMAP_HSV = 9\n",
      "    COLORMAP_INFERNO = 14\n",
      "    COLORMAP_JET = 2\n",
      "    COLORMAP_MAGMA = 13\n",
      "    COLORMAP_OCEAN = 5\n",
      "    COLORMAP_PARULA = 12\n",
      "    COLORMAP_PINK = 10\n",
      "    COLORMAP_PLASMA = 15\n",
      "    COLORMAP_RAINBOW = 4\n",
      "    COLORMAP_SPRING = 7\n",
      "    COLORMAP_SUMMER = 6\n",
      "    COLORMAP_TWILIGHT = 18\n",
      "    COLORMAP_TWILIGHT_SHIFTED = 19\n",
      "    COLORMAP_VIRIDIS = 16\n",
      "    COLORMAP_WINTER = 3\n",
      "    COLOR_BAYER_BG2BGR = 46\n",
      "    COLOR_BAYER_BG2BGRA = 139\n",
      "    COLOR_BAYER_BG2BGR_EA = 135\n",
      "    COLOR_BAYER_BG2BGR_VNG = 62\n",
      "    COLOR_BAYER_BG2GRAY = 86\n",
      "    COLOR_BAYER_BG2RGB = 48\n",
      "    COLOR_BAYER_BG2RGBA = 141\n",
      "    COLOR_BAYER_BG2RGB_EA = 137\n",
      "    COLOR_BAYER_BG2RGB_VNG = 64\n",
      "    COLOR_BAYER_GB2BGR = 47\n",
      "    COLOR_BAYER_GB2BGRA = 140\n",
      "    COLOR_BAYER_GB2BGR_EA = 136\n",
      "    COLOR_BAYER_GB2BGR_VNG = 63\n",
      "    COLOR_BAYER_GB2GRAY = 87\n",
      "    COLOR_BAYER_GB2RGB = 49\n",
      "    COLOR_BAYER_GB2RGBA = 142\n",
      "    COLOR_BAYER_GB2RGB_EA = 138\n",
      "    COLOR_BAYER_GB2RGB_VNG = 65\n",
      "    COLOR_BAYER_GR2BGR = 49\n",
      "    COLOR_BAYER_GR2BGRA = 142\n",
      "    COLOR_BAYER_GR2BGR_EA = 138\n",
      "    COLOR_BAYER_GR2BGR_VNG = 65\n",
      "    COLOR_BAYER_GR2GRAY = 89\n",
      "    COLOR_BAYER_GR2RGB = 47\n",
      "    COLOR_BAYER_GR2RGBA = 140\n",
      "    COLOR_BAYER_GR2RGB_EA = 136\n",
      "    COLOR_BAYER_GR2RGB_VNG = 63\n",
      "    COLOR_BAYER_RG2BGR = 48\n",
      "    COLOR_BAYER_RG2BGRA = 141\n",
      "    COLOR_BAYER_RG2BGR_EA = 137\n",
      "    COLOR_BAYER_RG2BGR_VNG = 64\n",
      "    COLOR_BAYER_RG2GRAY = 88\n",
      "    COLOR_BAYER_RG2RGB = 46\n",
      "    COLOR_BAYER_RG2RGBA = 139\n",
      "    COLOR_BAYER_RG2RGB_EA = 135\n",
      "    COLOR_BAYER_RG2RGB_VNG = 62\n",
      "    COLOR_BGR2BGR555 = 22\n",
      "    COLOR_BGR2BGR565 = 12\n",
      "    COLOR_BGR2BGRA = 0\n",
      "    COLOR_BGR2GRAY = 6\n",
      "    COLOR_BGR2HLS = 52\n",
      "    COLOR_BGR2HLS_FULL = 68\n",
      "    COLOR_BGR2HSV = 40\n",
      "    COLOR_BGR2HSV_FULL = 66\n",
      "    COLOR_BGR2LAB = 44\n",
      "    COLOR_BGR2LUV = 50\n",
      "    COLOR_BGR2Lab = 44\n",
      "    COLOR_BGR2Luv = 50\n",
      "    COLOR_BGR2RGB = 4\n",
      "    COLOR_BGR2RGBA = 2\n",
      "    COLOR_BGR2XYZ = 32\n",
      "    COLOR_BGR2YCR_CB = 36\n",
      "    COLOR_BGR2YCrCb = 36\n",
      "    COLOR_BGR2YUV = 82\n",
      "    COLOR_BGR2YUV_I420 = 128\n",
      "    COLOR_BGR2YUV_IYUV = 128\n",
      "    COLOR_BGR2YUV_YV12 = 132\n",
      "    COLOR_BGR5552BGR = 24\n",
      "    COLOR_BGR5552BGRA = 28\n",
      "    COLOR_BGR5552GRAY = 31\n",
      "    COLOR_BGR5552RGB = 25\n",
      "    COLOR_BGR5552RGBA = 29\n",
      "    COLOR_BGR5652BGR = 14\n",
      "    COLOR_BGR5652BGRA = 18\n",
      "    COLOR_BGR5652GRAY = 21\n",
      "    COLOR_BGR5652RGB = 15\n",
      "    COLOR_BGR5652RGBA = 19\n",
      "    COLOR_BGRA2BGR = 1\n",
      "    COLOR_BGRA2BGR555 = 26\n",
      "    COLOR_BGRA2BGR565 = 16\n",
      "    COLOR_BGRA2GRAY = 10\n",
      "    COLOR_BGRA2RGB = 3\n",
      "    COLOR_BGRA2RGBA = 5\n",
      "    COLOR_BGRA2YUV_I420 = 130\n",
      "    COLOR_BGRA2YUV_IYUV = 130\n",
      "    COLOR_BGRA2YUV_YV12 = 134\n",
      "    COLOR_BayerBG2BGR = 46\n",
      "    COLOR_BayerBG2BGRA = 139\n",
      "    COLOR_BayerBG2BGR_EA = 135\n",
      "    COLOR_BayerBG2BGR_VNG = 62\n",
      "    COLOR_BayerBG2GRAY = 86\n",
      "    COLOR_BayerBG2RGB = 48\n",
      "    COLOR_BayerBG2RGBA = 141\n",
      "    COLOR_BayerBG2RGB_EA = 137\n",
      "    COLOR_BayerBG2RGB_VNG = 64\n",
      "    COLOR_BayerGB2BGR = 47\n",
      "    COLOR_BayerGB2BGRA = 140\n",
      "    COLOR_BayerGB2BGR_EA = 136\n",
      "    COLOR_BayerGB2BGR_VNG = 63\n",
      "    COLOR_BayerGB2GRAY = 87\n",
      "    COLOR_BayerGB2RGB = 49\n",
      "    COLOR_BayerGB2RGBA = 142\n",
      "    COLOR_BayerGB2RGB_EA = 138\n",
      "    COLOR_BayerGB2RGB_VNG = 65\n",
      "    COLOR_BayerGR2BGR = 49\n",
      "    COLOR_BayerGR2BGRA = 142\n",
      "    COLOR_BayerGR2BGR_EA = 138\n",
      "    COLOR_BayerGR2BGR_VNG = 65\n",
      "    COLOR_BayerGR2GRAY = 89\n",
      "    COLOR_BayerGR2RGB = 47\n",
      "    COLOR_BayerGR2RGBA = 140\n",
      "    COLOR_BayerGR2RGB_EA = 136\n",
      "    COLOR_BayerGR2RGB_VNG = 63\n",
      "    COLOR_BayerRG2BGR = 48\n",
      "    COLOR_BayerRG2BGRA = 141\n",
      "    COLOR_BayerRG2BGR_EA = 137\n",
      "    COLOR_BayerRG2BGR_VNG = 64\n",
      "    COLOR_BayerRG2GRAY = 88\n",
      "    COLOR_BayerRG2RGB = 46\n",
      "    COLOR_BayerRG2RGBA = 139\n",
      "    COLOR_BayerRG2RGB_EA = 135\n",
      "    COLOR_BayerRG2RGB_VNG = 62\n",
      "    COLOR_COLORCVT_MAX = 143\n",
      "    COLOR_GRAY2BGR = 8\n",
      "    COLOR_GRAY2BGR555 = 30\n",
      "    COLOR_GRAY2BGR565 = 20\n",
      "    COLOR_GRAY2BGRA = 9\n",
      "    COLOR_GRAY2RGB = 8\n",
      "    COLOR_GRAY2RGBA = 9\n",
      "    COLOR_HLS2BGR = 60\n",
      "    COLOR_HLS2BGR_FULL = 72\n",
      "    COLOR_HLS2RGB = 61\n",
      "    COLOR_HLS2RGB_FULL = 73\n",
      "    COLOR_HSV2BGR = 54\n",
      "    COLOR_HSV2BGR_FULL = 70\n",
      "    COLOR_HSV2RGB = 55\n",
      "    COLOR_HSV2RGB_FULL = 71\n",
      "    COLOR_LAB2BGR = 56\n",
      "    COLOR_LAB2LBGR = 78\n",
      "    COLOR_LAB2LRGB = 79\n",
      "    COLOR_LAB2RGB = 57\n",
      "    COLOR_LBGR2LAB = 74\n",
      "    COLOR_LBGR2LUV = 76\n",
      "    COLOR_LBGR2Lab = 74\n",
      "    COLOR_LBGR2Luv = 76\n",
      "    COLOR_LRGB2LAB = 75\n",
      "    COLOR_LRGB2LUV = 77\n",
      "    COLOR_LRGB2Lab = 75\n",
      "    COLOR_LRGB2Luv = 77\n",
      "    COLOR_LUV2BGR = 58\n",
      "    COLOR_LUV2LBGR = 80\n",
      "    COLOR_LUV2LRGB = 81\n",
      "    COLOR_LUV2RGB = 59\n",
      "    COLOR_Lab2BGR = 56\n",
      "    COLOR_Lab2LBGR = 78\n",
      "    COLOR_Lab2LRGB = 79\n",
      "    COLOR_Lab2RGB = 57\n",
      "    COLOR_Luv2BGR = 58\n",
      "    COLOR_Luv2LBGR = 80\n",
      "    COLOR_Luv2LRGB = 81\n",
      "    COLOR_Luv2RGB = 59\n",
      "    COLOR_M_RGBA2RGBA = 126\n",
      "    COLOR_RGB2BGR = 4\n",
      "    COLOR_RGB2BGR555 = 23\n",
      "    COLOR_RGB2BGR565 = 13\n",
      "    COLOR_RGB2BGRA = 2\n",
      "    COLOR_RGB2GRAY = 7\n",
      "    COLOR_RGB2HLS = 53\n",
      "    COLOR_RGB2HLS_FULL = 69\n",
      "    COLOR_RGB2HSV = 41\n",
      "    COLOR_RGB2HSV_FULL = 67\n",
      "    COLOR_RGB2LAB = 45\n",
      "    COLOR_RGB2LUV = 51\n",
      "    COLOR_RGB2Lab = 45\n",
      "    COLOR_RGB2Luv = 51\n",
      "    COLOR_RGB2RGBA = 0\n",
      "    COLOR_RGB2XYZ = 33\n",
      "    COLOR_RGB2YCR_CB = 37\n",
      "    COLOR_RGB2YCrCb = 37\n",
      "    COLOR_RGB2YUV = 83\n",
      "    COLOR_RGB2YUV_I420 = 127\n",
      "    COLOR_RGB2YUV_IYUV = 127\n",
      "    COLOR_RGB2YUV_YV12 = 131\n",
      "    COLOR_RGBA2BGR = 3\n",
      "    COLOR_RGBA2BGR555 = 27\n",
      "    COLOR_RGBA2BGR565 = 17\n",
      "    COLOR_RGBA2BGRA = 5\n",
      "    COLOR_RGBA2GRAY = 11\n",
      "    COLOR_RGBA2M_RGBA = 125\n",
      "    COLOR_RGBA2RGB = 1\n",
      "    COLOR_RGBA2YUV_I420 = 129\n",
      "    COLOR_RGBA2YUV_IYUV = 129\n",
      "    COLOR_RGBA2YUV_YV12 = 133\n",
      "    COLOR_RGBA2mRGBA = 125\n",
      "    COLOR_XYZ2BGR = 34\n",
      "    COLOR_XYZ2RGB = 35\n",
      "    COLOR_YCR_CB2BGR = 38\n",
      "    COLOR_YCR_CB2RGB = 39\n",
      "    COLOR_YCrCb2BGR = 38\n",
      "    COLOR_YCrCb2RGB = 39\n",
      "    COLOR_YUV2BGR = 84\n",
      "    COLOR_YUV2BGRA_I420 = 105\n",
      "    COLOR_YUV2BGRA_IYUV = 105\n",
      "    COLOR_YUV2BGRA_NV12 = 95\n",
      "    COLOR_YUV2BGRA_NV21 = 97\n",
      "    COLOR_YUV2BGRA_UYNV = 112\n",
      "    COLOR_YUV2BGRA_UYVY = 112\n",
      "    COLOR_YUV2BGRA_Y422 = 112\n",
      "    COLOR_YUV2BGRA_YUNV = 120\n",
      "    COLOR_YUV2BGRA_YUY2 = 120\n",
      "    COLOR_YUV2BGRA_YUYV = 120\n",
      "    COLOR_YUV2BGRA_YV12 = 103\n",
      "    COLOR_YUV2BGRA_YVYU = 122\n",
      "    COLOR_YUV2BGR_I420 = 101\n",
      "    COLOR_YUV2BGR_IYUV = 101\n",
      "    COLOR_YUV2BGR_NV12 = 91\n",
      "    COLOR_YUV2BGR_NV21 = 93\n",
      "    COLOR_YUV2BGR_UYNV = 108\n",
      "    COLOR_YUV2BGR_UYVY = 108\n",
      "    COLOR_YUV2BGR_Y422 = 108\n",
      "    COLOR_YUV2BGR_YUNV = 116\n",
      "    COLOR_YUV2BGR_YUY2 = 116\n",
      "    COLOR_YUV2BGR_YUYV = 116\n",
      "    COLOR_YUV2BGR_YV12 = 99\n",
      "    COLOR_YUV2BGR_YVYU = 118\n",
      "    COLOR_YUV2GRAY_420 = 106\n",
      "    COLOR_YUV2GRAY_I420 = 106\n",
      "    COLOR_YUV2GRAY_IYUV = 106\n",
      "    COLOR_YUV2GRAY_NV12 = 106\n",
      "    COLOR_YUV2GRAY_NV21 = 106\n",
      "    COLOR_YUV2GRAY_UYNV = 123\n",
      "    COLOR_YUV2GRAY_UYVY = 123\n",
      "    COLOR_YUV2GRAY_Y422 = 123\n",
      "    COLOR_YUV2GRAY_YUNV = 124\n",
      "    COLOR_YUV2GRAY_YUY2 = 124\n",
      "    COLOR_YUV2GRAY_YUYV = 124\n",
      "    COLOR_YUV2GRAY_YV12 = 106\n",
      "    COLOR_YUV2GRAY_YVYU = 124\n",
      "    COLOR_YUV2RGB = 85\n",
      "    COLOR_YUV2RGBA_I420 = 104\n",
      "    COLOR_YUV2RGBA_IYUV = 104\n",
      "    COLOR_YUV2RGBA_NV12 = 94\n",
      "    COLOR_YUV2RGBA_NV21 = 96\n",
      "    COLOR_YUV2RGBA_UYNV = 111\n",
      "    COLOR_YUV2RGBA_UYVY = 111\n",
      "    COLOR_YUV2RGBA_Y422 = 111\n",
      "    COLOR_YUV2RGBA_YUNV = 119\n",
      "    COLOR_YUV2RGBA_YUY2 = 119\n",
      "    COLOR_YUV2RGBA_YUYV = 119\n",
      "    COLOR_YUV2RGBA_YV12 = 102\n",
      "    COLOR_YUV2RGBA_YVYU = 121\n",
      "    COLOR_YUV2RGB_I420 = 100\n",
      "    COLOR_YUV2RGB_IYUV = 100\n",
      "    COLOR_YUV2RGB_NV12 = 90\n",
      "    COLOR_YUV2RGB_NV21 = 92\n",
      "    COLOR_YUV2RGB_UYNV = 107\n",
      "    COLOR_YUV2RGB_UYVY = 107\n",
      "    COLOR_YUV2RGB_Y422 = 107\n",
      "    COLOR_YUV2RGB_YUNV = 115\n",
      "    COLOR_YUV2RGB_YUY2 = 115\n",
      "    COLOR_YUV2RGB_YUYV = 115\n",
      "    COLOR_YUV2RGB_YV12 = 98\n",
      "    COLOR_YUV2RGB_YVYU = 117\n",
      "    COLOR_YUV420P2BGR = 99\n",
      "    COLOR_YUV420P2BGRA = 103\n",
      "    COLOR_YUV420P2GRAY = 106\n",
      "    COLOR_YUV420P2RGB = 98\n",
      "    COLOR_YUV420P2RGBA = 102\n",
      "    COLOR_YUV420SP2BGR = 93\n",
      "    COLOR_YUV420SP2BGRA = 97\n",
      "    COLOR_YUV420SP2GRAY = 106\n",
      "    COLOR_YUV420SP2RGB = 92\n",
      "    COLOR_YUV420SP2RGBA = 96\n",
      "    COLOR_YUV420p2BGR = 99\n",
      "    COLOR_YUV420p2BGRA = 103\n",
      "    COLOR_YUV420p2GRAY = 106\n",
      "    COLOR_YUV420p2RGB = 98\n",
      "    COLOR_YUV420p2RGBA = 102\n",
      "    COLOR_YUV420sp2BGR = 93\n",
      "    COLOR_YUV420sp2BGRA = 97\n",
      "    COLOR_YUV420sp2GRAY = 106\n",
      "    COLOR_YUV420sp2RGB = 92\n",
      "    COLOR_YUV420sp2RGBA = 96\n",
      "    COLOR_mRGBA2RGBA = 126\n",
      "    CONTOURS_MATCH_I1 = 1\n",
      "    CONTOURS_MATCH_I2 = 2\n",
      "    CONTOURS_MATCH_I3 = 3\n",
      "    COVAR_COLS = 16\n",
      "    COVAR_NORMAL = 1\n",
      "    COVAR_ROWS = 8\n",
      "    COVAR_SCALE = 4\n",
      "    COVAR_SCRAMBLED = 0\n",
      "    COVAR_USE_AVG = 2\n",
      "    CV_16S = 3\n",
      "    CV_16SC1 = 3\n",
      "    CV_16SC2 = 11\n",
      "    CV_16SC3 = 19\n",
      "    CV_16SC4 = 27\n",
      "    CV_16U = 2\n",
      "    CV_16UC1 = 2\n",
      "    CV_16UC2 = 10\n",
      "    CV_16UC3 = 18\n",
      "    CV_16UC4 = 26\n",
      "    CV_32F = 5\n",
      "    CV_32FC1 = 5\n",
      "    CV_32FC2 = 13\n",
      "    CV_32FC3 = 21\n",
      "    CV_32FC4 = 29\n",
      "    CV_32S = 4\n",
      "    CV_32SC1 = 4\n",
      "    CV_32SC2 = 12\n",
      "    CV_32SC3 = 20\n",
      "    CV_32SC4 = 28\n",
      "    CV_64F = 6\n",
      "    CV_64FC1 = 6\n",
      "    CV_64FC2 = 14\n",
      "    CV_64FC3 = 22\n",
      "    CV_64FC4 = 30\n",
      "    CV_8S = 1\n",
      "    CV_8SC1 = 1\n",
      "    CV_8SC2 = 9\n",
      "    CV_8SC3 = 17\n",
      "    CV_8SC4 = 25\n",
      "    CV_8U = 0\n",
      "    CV_8UC1 = 0\n",
      "    CV_8UC2 = 8\n",
      "    CV_8UC3 = 16\n",
      "    CV_8UC4 = 24\n",
      "    CirclesGridFinderParameters_ASYMMETRIC_GRID = 1\n",
      "    CirclesGridFinderParameters_SYMMETRIC_GRID = 0\n",
      "    DCT_INVERSE = 1\n",
      "    DCT_ROWS = 4\n",
      "    DECOMP_CHOLESKY = 3\n",
      "    DECOMP_EIG = 2\n",
      "    DECOMP_LU = 0\n",
      "    DECOMP_NORMAL = 16\n",
      "    DECOMP_QR = 4\n",
      "    DECOMP_SVD = 1\n",
      "    DESCRIPTOR_MATCHER_BRUTEFORCE = 2\n",
      "    DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING = 4\n",
      "    DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMINGLUT = 5\n",
      "    DESCRIPTOR_MATCHER_BRUTEFORCE_L1 = 3\n",
      "    DESCRIPTOR_MATCHER_BRUTEFORCE_SL2 = 6\n",
      "    DESCRIPTOR_MATCHER_FLANNBASED = 1\n",
      "    DFT_COMPLEX_INPUT = 64\n",
      "    DFT_COMPLEX_OUTPUT = 16\n",
      "    DFT_INVERSE = 1\n",
      "    DFT_REAL_OUTPUT = 32\n",
      "    DFT_ROWS = 4\n",
      "    DFT_SCALE = 2\n",
      "    DISOPTICAL_FLOW_PRESET_FAST = 1\n",
      "    DISOPTICAL_FLOW_PRESET_MEDIUM = 2\n",
      "    DISOPTICAL_FLOW_PRESET_ULTRAFAST = 0\n",
      "    DISOpticalFlow_PRESET_FAST = 1\n",
      "    DISOpticalFlow_PRESET_MEDIUM = 2\n",
      "    DISOpticalFlow_PRESET_ULTRAFAST = 0\n",
      "    DIST_C = 3\n",
      "    DIST_FAIR = 5\n",
      "    DIST_HUBER = 7\n",
      "    DIST_L1 = 1\n",
      "    DIST_L12 = 4\n",
      "    DIST_L2 = 2\n",
      "    DIST_LABEL_CCOMP = 0\n",
      "    DIST_LABEL_PIXEL = 1\n",
      "    DIST_MASK_3 = 3\n",
      "    DIST_MASK_5 = 5\n",
      "    DIST_MASK_PRECISE = 0\n",
      "    DIST_USER = -1\n",
      "    DIST_WELSCH = 6\n",
      "    DRAW_MATCHES_FLAGS_DEFAULT = 0\n",
      "    DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG = 1\n",
      "    DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS = 4\n",
      "    DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS = 2\n",
      "    DescriptorMatcher_BRUTEFORCE = 2\n",
      "    DescriptorMatcher_BRUTEFORCE_HAMMING = 4\n",
      "    DescriptorMatcher_BRUTEFORCE_HAMMINGLUT = 5\n",
      "    DescriptorMatcher_BRUTEFORCE_L1 = 3\n",
      "    DescriptorMatcher_BRUTEFORCE_SL2 = 6\n",
      "    DescriptorMatcher_FLANNBASED = 1\n",
      "    DrawMatchesFlags_DEFAULT = 0\n",
      "    DrawMatchesFlags_DRAW_OVER_OUTIMG = 1\n",
      "    DrawMatchesFlags_DRAW_RICH_KEYPOINTS = 4\n",
      "    DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS = 2\n",
      "    EVENT_FLAG_ALTKEY = 32\n",
      "    EVENT_FLAG_CTRLKEY = 8\n",
      "    EVENT_FLAG_LBUTTON = 1\n",
      "    EVENT_FLAG_MBUTTON = 4\n",
      "    EVENT_FLAG_RBUTTON = 2\n",
      "    EVENT_FLAG_SHIFTKEY = 16\n",
      "    EVENT_LBUTTONDBLCLK = 7\n",
      "    EVENT_LBUTTONDOWN = 1\n",
      "    EVENT_LBUTTONUP = 4\n",
      "    EVENT_MBUTTONDBLCLK = 9\n",
      "    EVENT_MBUTTONDOWN = 3\n",
      "    EVENT_MBUTTONUP = 6\n",
      "    EVENT_MOUSEHWHEEL = 11\n",
      "    EVENT_MOUSEMOVE = 0\n",
      "    EVENT_MOUSEWHEEL = 10\n",
      "    EVENT_RBUTTONDBLCLK = 8\n",
      "    EVENT_RBUTTONDOWN = 2\n",
      "    EVENT_RBUTTONUP = 5\n",
      "    FAST_FEATURE_DETECTOR_FAST_N = 10002\n",
      "    FAST_FEATURE_DETECTOR_NONMAX_SUPPRESSION = 10001\n",
      "    FAST_FEATURE_DETECTOR_THRESHOLD = 10000\n",
      "    FAST_FEATURE_DETECTOR_TYPE_5_8 = 0\n",
      "    FAST_FEATURE_DETECTOR_TYPE_7_12 = 1\n",
      "    FAST_FEATURE_DETECTOR_TYPE_9_16 = 2\n",
      "    FILE_NODE_EMPTY = 16\n",
      "    FILE_NODE_FLOAT = 2\n",
      "    FILE_NODE_FLOW = 8\n",
      "    FILE_NODE_INT = 1\n",
      "    FILE_NODE_MAP = 5\n",
      "    FILE_NODE_NAMED = 32\n",
      "    FILE_NODE_NONE = 0\n",
      "    FILE_NODE_REAL = 2\n",
      "    FILE_NODE_SEQ = 4\n",
      "    FILE_NODE_STR = 3\n",
      "    FILE_NODE_STRING = 3\n",
      "    FILE_NODE_TYPE_MASK = 7\n",
      "    FILE_NODE_UNIFORM = 8\n",
      "    FILE_STORAGE_APPEND = 2\n",
      "    FILE_STORAGE_BASE64 = 64\n",
      "    FILE_STORAGE_FORMAT_AUTO = 0\n",
      "    FILE_STORAGE_FORMAT_JSON = 24\n",
      "    FILE_STORAGE_FORMAT_MASK = 56\n",
      "    FILE_STORAGE_FORMAT_XML = 8\n",
      "    FILE_STORAGE_FORMAT_YAML = 16\n",
      "    FILE_STORAGE_INSIDE_MAP = 4\n",
      "    FILE_STORAGE_MEMORY = 4\n",
      "    FILE_STORAGE_NAME_EXPECTED = 2\n",
      "    FILE_STORAGE_READ = 0\n",
      "    FILE_STORAGE_UNDEFINED = 0\n",
      "    FILE_STORAGE_VALUE_EXPECTED = 1\n",
      "    FILE_STORAGE_WRITE = 1\n",
      "    FILE_STORAGE_WRITE_BASE64 = 65\n",
      "    FILLED = -1\n",
      "    FILTER_SCHARR = -1\n",
      "    FLOODFILL_FIXED_RANGE = 65536\n",
      "    FLOODFILL_MASK_ONLY = 131072\n",
      "    FM_7POINT = 1\n",
      "    FM_8POINT = 2\n",
      "    FM_LMEDS = 4\n",
      "    FM_RANSAC = 8\n",
      "    FONT_HERSHEY_COMPLEX = 3\n",
      "    FONT_HERSHEY_COMPLEX_SMALL = 5\n",
      "    FONT_HERSHEY_DUPLEX = 2\n",
      "    FONT_HERSHEY_PLAIN = 1\n",
      "    FONT_HERSHEY_SCRIPT_COMPLEX = 7\n",
      "    FONT_HERSHEY_SCRIPT_SIMPLEX = 6\n",
      "    FONT_HERSHEY_SIMPLEX = 0\n",
      "    FONT_HERSHEY_TRIPLEX = 4\n",
      "    FONT_ITALIC = 16\n",
      "    FORMATTER_FMT_C = 5\n",
      "    FORMATTER_FMT_CSV = 2\n",
      "    FORMATTER_FMT_DEFAULT = 0\n",
      "    FORMATTER_FMT_MATLAB = 1\n",
      "    FORMATTER_FMT_NUMPY = 4\n",
      "    FORMATTER_FMT_PYTHON = 3\n",
      "    FastFeatureDetector_FAST_N = 10002\n",
      "    FastFeatureDetector_NONMAX_SUPPRESSION = 10001\n",
      "    FastFeatureDetector_THRESHOLD = 10000\n",
      "    FastFeatureDetector_TYPE_5_8 = 0\n",
      "    FastFeatureDetector_TYPE_7_12 = 1\n",
      "    FastFeatureDetector_TYPE_9_16 = 2\n",
      "    FileNode_EMPTY = 16\n",
      "    FileNode_FLOAT = 2\n",
      "    FileNode_FLOW = 8\n",
      "    FileNode_INT = 1\n",
      "    FileNode_MAP = 5\n",
      "    FileNode_NAMED = 32\n",
      "    FileNode_NONE = 0\n",
      "    FileNode_REAL = 2\n",
      "    FileNode_SEQ = 4\n",
      "    FileNode_STR = 3\n",
      "    FileNode_STRING = 3\n",
      "    FileNode_TYPE_MASK = 7\n",
      "    FileNode_UNIFORM = 8\n",
      "    FileStorage_APPEND = 2\n",
      "    FileStorage_BASE64 = 64\n",
      "    FileStorage_FORMAT_AUTO = 0\n",
      "    FileStorage_FORMAT_JSON = 24\n",
      "    FileStorage_FORMAT_MASK = 56\n",
      "    FileStorage_FORMAT_XML = 8\n",
      "    FileStorage_FORMAT_YAML = 16\n",
      "    FileStorage_INSIDE_MAP = 4\n",
      "    FileStorage_MEMORY = 4\n",
      "    FileStorage_NAME_EXPECTED = 2\n",
      "    FileStorage_READ = 0\n",
      "    FileStorage_UNDEFINED = 0\n",
      "    FileStorage_VALUE_EXPECTED = 1\n",
      "    FileStorage_WRITE = 1\n",
      "    FileStorage_WRITE_BASE64 = 65\n",
      "    Formatter_FMT_C = 5\n",
      "    Formatter_FMT_CSV = 2\n",
      "    Formatter_FMT_DEFAULT = 0\n",
      "    Formatter_FMT_MATLAB = 1\n",
      "    Formatter_FMT_NUMPY = 4\n",
      "    Formatter_FMT_PYTHON = 3\n",
      "    GC_BGD = 0\n",
      "    GC_EVAL = 2\n",
      "    GC_EVAL_FREEZE_MODEL = 3\n",
      "    GC_FGD = 1\n",
      "    GC_INIT_WITH_MASK = 1\n",
      "    GC_INIT_WITH_RECT = 0\n",
      "    GC_PR_BGD = 2\n",
      "    GC_PR_FGD = 3\n",
      "    GEMM_1_T = 1\n",
      "    GEMM_2_T = 2\n",
      "    GEMM_3_T = 4\n",
      "    HISTCMP_BHATTACHARYYA = 3\n",
      "    HISTCMP_CHISQR = 1\n",
      "    HISTCMP_CHISQR_ALT = 4\n",
      "    HISTCMP_CORREL = 0\n",
      "    HISTCMP_HELLINGER = 3\n",
      "    HISTCMP_INTERSECT = 2\n",
      "    HISTCMP_KL_DIV = 5\n",
      "    HOGDESCRIPTOR_DEFAULT_NLEVELS = 64\n",
      "    HOGDESCRIPTOR_DESCR_FORMAT_COL_BY_COL = 0\n",
      "    HOGDESCRIPTOR_DESCR_FORMAT_ROW_BY_ROW = 1\n",
      "    HOGDESCRIPTOR_L2HYS = 0\n",
      "    HOGDescriptor_DEFAULT_NLEVELS = 64\n",
      "    HOGDescriptor_DESCR_FORMAT_COL_BY_COL = 0\n",
      "    HOGDescriptor_DESCR_FORMAT_ROW_BY_ROW = 1\n",
      "    HOGDescriptor_L2Hys = 0\n",
      "    HOUGH_GRADIENT = 3\n",
      "    HOUGH_MULTI_SCALE = 2\n",
      "    HOUGH_PROBABILISTIC = 1\n",
      "    HOUGH_STANDARD = 0\n",
      "    IMREAD_ANYCOLOR = 4\n",
      "    IMREAD_ANYDEPTH = 2\n",
      "    IMREAD_COLOR = 1\n",
      "    IMREAD_GRAYSCALE = 0\n",
      "    IMREAD_IGNORE_ORIENTATION = 128\n",
      "    IMREAD_LOAD_GDAL = 8\n",
      "    IMREAD_REDUCED_COLOR_2 = 17\n",
      "    IMREAD_REDUCED_COLOR_4 = 33\n",
      "    IMREAD_REDUCED_COLOR_8 = 65\n",
      "    IMREAD_REDUCED_GRAYSCALE_2 = 16\n",
      "    IMREAD_REDUCED_GRAYSCALE_4 = 32\n",
      "    IMREAD_REDUCED_GRAYSCALE_8 = 64\n",
      "    IMREAD_UNCHANGED = -1\n",
      "    IMWRITE_EXR_TYPE = 48\n",
      "    IMWRITE_EXR_TYPE_FLOAT = 2\n",
      "    IMWRITE_EXR_TYPE_HALF = 1\n",
      "    IMWRITE_JPEG2000_COMPRESSION_X1000 = 272\n",
      "    IMWRITE_JPEG_CHROMA_QUALITY = 6\n",
      "    IMWRITE_JPEG_LUMA_QUALITY = 5\n",
      "    IMWRITE_JPEG_OPTIMIZE = 3\n",
      "    IMWRITE_JPEG_PROGRESSIVE = 2\n",
      "    IMWRITE_JPEG_QUALITY = 1\n",
      "    IMWRITE_JPEG_RST_INTERVAL = 4\n",
      "    IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1\n",
      "    IMWRITE_PAM_FORMAT_GRAYSCALE = 2\n",
      "    IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3\n",
      "    IMWRITE_PAM_FORMAT_NULL = 0\n",
      "    IMWRITE_PAM_FORMAT_RGB = 4\n",
      "    IMWRITE_PAM_FORMAT_RGB_ALPHA = 5\n",
      "    IMWRITE_PAM_TUPLETYPE = 128\n",
      "    IMWRITE_PNG_BILEVEL = 18\n",
      "    IMWRITE_PNG_COMPRESSION = 16\n",
      "    IMWRITE_PNG_STRATEGY = 17\n",
      "    IMWRITE_PNG_STRATEGY_DEFAULT = 0\n",
      "    IMWRITE_PNG_STRATEGY_FILTERED = 1\n",
      "    IMWRITE_PNG_STRATEGY_FIXED = 4\n",
      "    IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2\n",
      "    IMWRITE_PNG_STRATEGY_RLE = 3\n",
      "    IMWRITE_PXM_BINARY = 32\n",
      "    IMWRITE_TIFF_COMPRESSION = 259\n",
      "    IMWRITE_TIFF_RESUNIT = 256\n",
      "    IMWRITE_TIFF_XDPI = 257\n",
      "    IMWRITE_TIFF_YDPI = 258\n",
      "    IMWRITE_WEBP_QUALITY = 64\n",
      "    INPAINT_NS = 0\n",
      "    INPAINT_TELEA = 1\n",
      "    INTERSECT_FULL = 2\n",
      "    INTERSECT_NONE = 0\n",
      "    INTERSECT_PARTIAL = 1\n",
      "    INTER_AREA = 3\n",
      "    INTER_BITS = 5\n",
      "    INTER_BITS2 = 10\n",
      "    INTER_CUBIC = 2\n",
      "    INTER_LANCZOS4 = 4\n",
      "    INTER_LINEAR = 1\n",
      "    INTER_LINEAR_EXACT = 5\n",
      "    INTER_MAX = 7\n",
      "    INTER_NEAREST = 0\n",
      "    INTER_TAB_SIZE = 32\n",
      "    INTER_TAB_SIZE2 = 1024\n",
      "    KAZE_DIFF_CHARBONNIER = 3\n",
      "    KAZE_DIFF_PM_G1 = 0\n",
      "    KAZE_DIFF_PM_G2 = 1\n",
      "    KAZE_DIFF_WEICKERT = 2\n",
      "    KMEANS_PP_CENTERS = 2\n",
      "    KMEANS_RANDOM_CENTERS = 0\n",
      "    KMEANS_USE_INITIAL_LABELS = 1\n",
      "    LDR_SIZE = 256\n",
      "    LINE_4 = 4\n",
      "    LINE_8 = 8\n",
      "    LINE_AA = 16\n",
      "    LMEDS = 4\n",
      "    LSD_REFINE_ADV = 2\n",
      "    LSD_REFINE_NONE = 0\n",
      "    LSD_REFINE_STD = 1\n",
      "    MARKER_CROSS = 0\n",
      "    MARKER_DIAMOND = 3\n",
      "    MARKER_SQUARE = 4\n",
      "    MARKER_STAR = 2\n",
      "    MARKER_TILTED_CROSS = 1\n",
      "    MARKER_TRIANGLE_DOWN = 6\n",
      "    MARKER_TRIANGLE_UP = 5\n",
      "    MAT_AUTO_STEP = 0\n",
      "    MAT_CONTINUOUS_FLAG = 16384\n",
      "    MAT_DEPTH_MASK = 7\n",
      "    MAT_MAGIC_MASK = 4294901760\n",
      "    MAT_MAGIC_VAL = 1124007936\n",
      "    MAT_SUBMATRIX_FLAG = 32768\n",
      "    MAT_TYPE_MASK = 4095\n",
      "    MIXED_CLONE = 2\n",
      "    MONOCHROME_TRANSFER = 3\n",
      "    MORPH_BLACKHAT = 6\n",
      "    MORPH_CLOSE = 3\n",
      "    MORPH_CROSS = 1\n",
      "    MORPH_DILATE = 1\n",
      "    MORPH_ELLIPSE = 2\n",
      "    MORPH_ERODE = 0\n",
      "    MORPH_GRADIENT = 4\n",
      "    MORPH_HITMISS = 7\n",
      "    MORPH_OPEN = 2\n",
      "    MORPH_RECT = 0\n",
      "    MORPH_TOPHAT = 5\n",
      "    MOTION_AFFINE = 2\n",
      "    MOTION_EUCLIDEAN = 1\n",
      "    MOTION_HOMOGRAPHY = 3\n",
      "    MOTION_TRANSLATION = 0\n",
      "    Mat_AUTO_STEP = 0\n",
      "    Mat_CONTINUOUS_FLAG = 16384\n",
      "    Mat_DEPTH_MASK = 7\n",
      "    Mat_MAGIC_MASK = 4294901760\n",
      "    Mat_MAGIC_VAL = 1124007936\n",
      "    Mat_SUBMATRIX_FLAG = 32768\n",
      "    Mat_TYPE_MASK = 4095\n",
      "    NORMAL_CLONE = 1\n",
      "    NORMCONV_FILTER = 2\n",
      "    NORM_HAMMING = 6\n",
      "    NORM_HAMMING2 = 7\n",
      "    NORM_INF = 1\n",
      "    NORM_L1 = 2\n",
      "    NORM_L2 = 4\n",
      "    NORM_L2SQR = 5\n",
      "    NORM_MINMAX = 32\n",
      "    NORM_RELATIVE = 8\n",
      "    NORM_TYPE_MASK = 7\n",
      "    OPTFLOW_FARNEBACK_GAUSSIAN = 256\n",
      "    OPTFLOW_LK_GET_MIN_EIGENVALS = 8\n",
      "    OPTFLOW_USE_INITIAL_FLOW = 4\n",
      "    ORB_FAST_SCORE = 1\n",
      "    ORB_HARRIS_SCORE = 0\n",
      "    PARAM_ALGORITHM = 6\n",
      "    PARAM_BOOLEAN = 1\n",
      "    PARAM_FLOAT = 7\n",
      "    PARAM_INT = 0\n",
      "    PARAM_MAT = 4\n",
      "    PARAM_MAT_VECTOR = 5\n",
      "    PARAM_REAL = 2\n",
      "    PARAM_SCALAR = 12\n",
      "    PARAM_STRING = 3\n",
      "    PARAM_UCHAR = 11\n",
      "    PARAM_UINT64 = 9\n",
      "    PARAM_UNSIGNED_INT = 8\n",
      "    PCA_DATA_AS_COL = 1\n",
      "    PCA_DATA_AS_ROW = 0\n",
      "    PCA_USE_AVG = 2\n",
      "    PROJ_SPHERICAL_EQRECT = 1\n",
      "    PROJ_SPHERICAL_ORTHO = 0\n",
      "    Param_ALGORITHM = 6\n",
      "    Param_BOOLEAN = 1\n",
      "    Param_FLOAT = 7\n",
      "    Param_INT = 0\n",
      "    Param_MAT = 4\n",
      "    Param_MAT_VECTOR = 5\n",
      "    Param_REAL = 2\n",
      "    Param_SCALAR = 12\n",
      "    Param_STRING = 3\n",
      "    Param_UCHAR = 11\n",
      "    Param_UINT64 = 9\n",
      "    Param_UNSIGNED_INT = 8\n",
      "    QT_CHECKBOX = 1\n",
      "    QT_FONT_BLACK = 87\n",
      "    QT_FONT_BOLD = 75\n",
      "    QT_FONT_DEMIBOLD = 63\n",
      "    QT_FONT_LIGHT = 25\n",
      "    QT_FONT_NORMAL = 50\n",
      "    QT_NEW_BUTTONBAR = 1024\n",
      "    QT_PUSH_BUTTON = 0\n",
      "    QT_RADIOBOX = 2\n",
      "    QT_STYLE_ITALIC = 1\n",
      "    QT_STYLE_NORMAL = 0\n",
      "    QT_STYLE_OBLIQUE = 2\n",
      "    RANSAC = 8\n",
      "    RECURS_FILTER = 1\n",
      "    REDUCE_AVG = 1\n",
      "    REDUCE_MAX = 2\n",
      "    REDUCE_MIN = 3\n",
      "    REDUCE_SUM = 0\n",
      "    RETR_CCOMP = 2\n",
      "    RETR_EXTERNAL = 0\n",
      "    RETR_FLOODFILL = 4\n",
      "    RETR_LIST = 1\n",
      "    RETR_TREE = 3\n",
      "    RHO = 16\n",
      "    RNG_NORMAL = 1\n",
      "    RNG_UNIFORM = 0\n",
      "    ROTATE_180 = 1\n",
      "    ROTATE_90_CLOCKWISE = 0\n",
      "    ROTATE_90_COUNTERCLOCKWISE = 2\n",
      "    SOLVELP_MULTI = 1\n",
      "    SOLVELP_SINGLE = 0\n",
      "    SOLVELP_UNBOUNDED = -2\n",
      "    SOLVELP_UNFEASIBLE = -1\n",
      "    SOLVEPNP_AP3P = 5\n",
      "    SOLVEPNP_DLS = 3\n",
      "    SOLVEPNP_EPNP = 1\n",
      "    SOLVEPNP_IPPE = 6\n",
      "    SOLVEPNP_IPPE_SQUARE = 7\n",
      "    SOLVEPNP_ITERATIVE = 0\n",
      "    SOLVEPNP_MAX_COUNT = 8\n",
      "    SOLVEPNP_P3P = 2\n",
      "    SOLVEPNP_UPNP = 4\n",
      "    SORT_ASCENDING = 0\n",
      "    SORT_DESCENDING = 16\n",
      "    SORT_EVERY_COLUMN = 1\n",
      "    SORT_EVERY_ROW = 0\n",
      "    SPARSE_MAT_HASH_BIT = 2147483648\n",
      "    SPARSE_MAT_HASH_SCALE = 1540483477\n",
      "    SPARSE_MAT_MAGIC_VAL = 1123876864\n",
      "    SPARSE_MAT_MAX_DIM = 32\n",
      "    STEREO_BM_PREFILTER_NORMALIZED_RESPONSE = 0\n",
      "    STEREO_BM_PREFILTER_XSOBEL = 1\n",
      "    STEREO_MATCHER_DISP_SCALE = 16\n",
      "    STEREO_MATCHER_DISP_SHIFT = 4\n",
      "    STEREO_SGBM_MODE_HH = 1\n",
      "    STEREO_SGBM_MODE_HH4 = 3\n",
      "    STEREO_SGBM_MODE_SGBM = 0\n",
      "    STEREO_SGBM_MODE_SGBM_3WAY = 2\n",
      "    STITCHER_ERR_CAMERA_PARAMS_ADJUST_FAIL = 3\n",
      "    STITCHER_ERR_HOMOGRAPHY_EST_FAIL = 2\n",
      "    STITCHER_ERR_NEED_MORE_IMGS = 1\n",
      "    STITCHER_OK = 0\n",
      "    STITCHER_PANORAMA = 0\n",
      "    STITCHER_SCANS = 1\n",
      "    SUBDIV2D_NEXT_AROUND_DST = 34\n",
      "    SUBDIV2D_NEXT_AROUND_LEFT = 19\n",
      "    SUBDIV2D_NEXT_AROUND_ORG = 0\n",
      "    SUBDIV2D_NEXT_AROUND_RIGHT = 49\n",
      "    SUBDIV2D_PREV_AROUND_DST = 51\n",
      "    SUBDIV2D_PREV_AROUND_LEFT = 32\n",
      "    SUBDIV2D_PREV_AROUND_ORG = 17\n",
      "    SUBDIV2D_PREV_AROUND_RIGHT = 2\n",
      "    SUBDIV2D_PTLOC_ERROR = -2\n",
      "    SUBDIV2D_PTLOC_INSIDE = 0\n",
      "    SUBDIV2D_PTLOC_ON_EDGE = 2\n",
      "    SUBDIV2D_PTLOC_OUTSIDE_RECT = -1\n",
      "    SUBDIV2D_PTLOC_VERTEX = 1\n",
      "    SVD_FULL_UV = 4\n",
      "    SVD_MODIFY_A = 1\n",
      "    SVD_NO_UV = 2\n",
      "    SparseMat_HASH_BIT = 2147483648\n",
      "    SparseMat_HASH_SCALE = 1540483477\n",
      "    SparseMat_MAGIC_VAL = 1123876864\n",
      "    SparseMat_MAX_DIM = 32\n",
      "    StereoBM_PREFILTER_NORMALIZED_RESPONSE = 0\n",
      "    StereoBM_PREFILTER_XSOBEL = 1\n",
      "    StereoMatcher_DISP_SCALE = 16\n",
      "    StereoMatcher_DISP_SHIFT = 4\n",
      "    StereoSGBM_MODE_HH = 1\n",
      "    StereoSGBM_MODE_HH4 = 3\n",
      "    StereoSGBM_MODE_SGBM = 0\n",
      "    StereoSGBM_MODE_SGBM_3WAY = 2\n",
      "    Stitcher_ERR_CAMERA_PARAMS_ADJUST_FAIL = 3\n",
      "    Stitcher_ERR_HOMOGRAPHY_EST_FAIL = 2\n",
      "    Stitcher_ERR_NEED_MORE_IMGS = 1\n",
      "    Stitcher_OK = 0\n",
      "    Stitcher_PANORAMA = 0\n",
      "    Stitcher_SCANS = 1\n",
      "    Subdiv2D_NEXT_AROUND_DST = 34\n",
      "    Subdiv2D_NEXT_AROUND_LEFT = 19\n",
      "    Subdiv2D_NEXT_AROUND_ORG = 0\n",
      "    Subdiv2D_NEXT_AROUND_RIGHT = 49\n",
      "    Subdiv2D_PREV_AROUND_DST = 51\n",
      "    Subdiv2D_PREV_AROUND_LEFT = 32\n",
      "    Subdiv2D_PREV_AROUND_ORG = 17\n",
      "    Subdiv2D_PREV_AROUND_RIGHT = 2\n",
      "    Subdiv2D_PTLOC_ERROR = -2\n",
      "    Subdiv2D_PTLOC_INSIDE = 0\n",
      "    Subdiv2D_PTLOC_ON_EDGE = 2\n",
      "    Subdiv2D_PTLOC_OUTSIDE_RECT = -1\n",
      "    Subdiv2D_PTLOC_VERTEX = 1\n",
      "    TERM_CRITERIA_COUNT = 1\n",
      "    TERM_CRITERIA_EPS = 2\n",
      "    TERM_CRITERIA_MAX_ITER = 1\n",
      "    THRESH_BINARY = 0\n",
      "    THRESH_BINARY_INV = 1\n",
      "    THRESH_MASK = 7\n",
      "    THRESH_OTSU = 8\n",
      "    THRESH_TOZERO = 3\n",
      "    THRESH_TOZERO_INV = 4\n",
      "    THRESH_TRIANGLE = 16\n",
      "    THRESH_TRUNC = 2\n",
      "    TM_CCOEFF = 4\n",
      "    TM_CCOEFF_NORMED = 5\n",
      "    TM_CCORR = 2\n",
      "    TM_CCORR_NORMED = 3\n",
      "    TM_SQDIFF = 0\n",
      "    TM_SQDIFF_NORMED = 1\n",
      "    TermCriteria_COUNT = 1\n",
      "    TermCriteria_EPS = 2\n",
      "    TermCriteria_MAX_ITER = 1\n",
      "    UMAT_AUTO_STEP = 0\n",
      "    UMAT_CONTINUOUS_FLAG = 16384\n",
      "    UMAT_DATA_ASYNC_CLEANUP = 128\n",
      "    UMAT_DATA_COPY_ON_MAP = 1\n",
      "    UMAT_DATA_DEVICE_COPY_OBSOLETE = 4\n",
      "    UMAT_DATA_DEVICE_MEM_MAPPED = 64\n",
      "    UMAT_DATA_HOST_COPY_OBSOLETE = 2\n",
      "    UMAT_DATA_TEMP_COPIED_UMAT = 24\n",
      "    UMAT_DATA_TEMP_UMAT = 8\n",
      "    UMAT_DATA_USER_ALLOCATED = 32\n",
      "    UMAT_DEPTH_MASK = 7\n",
      "    UMAT_MAGIC_MASK = 4294901760\n",
      "    UMAT_MAGIC_VAL = 1124007936\n",
      "    UMAT_SUBMATRIX_FLAG = 32768\n",
      "    UMAT_TYPE_MASK = 4095\n",
      "    UMatData_ASYNC_CLEANUP = 128\n",
      "    UMatData_COPY_ON_MAP = 1\n",
      "    UMatData_DEVICE_COPY_OBSOLETE = 4\n",
      "    UMatData_DEVICE_MEM_MAPPED = 64\n",
      "    UMatData_HOST_COPY_OBSOLETE = 2\n",
      "    UMatData_TEMP_COPIED_UMAT = 24\n",
      "    UMatData_TEMP_UMAT = 8\n",
      "    UMatData_USER_ALLOCATED = 32\n",
      "    UMat_AUTO_STEP = 0\n",
      "    UMat_CONTINUOUS_FLAG = 16384\n",
      "    UMat_DEPTH_MASK = 7\n",
      "    UMat_MAGIC_MASK = 4294901760\n",
      "    UMat_MAGIC_VAL = 1124007936\n",
      "    UMat_SUBMATRIX_FLAG = 32768\n",
      "    UMat_TYPE_MASK = 4095\n",
      "    USAGE_ALLOCATE_DEVICE_MEMORY = 2\n",
      "    USAGE_ALLOCATE_HOST_MEMORY = 1\n",
      "    USAGE_ALLOCATE_SHARED_MEMORY = 4\n",
      "    USAGE_DEFAULT = 0\n",
      "    VIDEOWRITER_PROP_FRAMEBYTES = 2\n",
      "    VIDEOWRITER_PROP_NSTRIPES = 3\n",
      "    VIDEOWRITER_PROP_QUALITY = 1\n",
      "    WARP_FILL_OUTLIERS = 8\n",
      "    WARP_INVERSE_MAP = 16\n",
      "    WARP_POLAR_LINEAR = 0\n",
      "    WARP_POLAR_LOG = 256\n",
      "    WINDOW_AUTOSIZE = 1\n",
      "    WINDOW_FREERATIO = 256\n",
      "    WINDOW_FULLSCREEN = 1\n",
      "    WINDOW_GUI_EXPANDED = 0\n",
      "    WINDOW_GUI_NORMAL = 16\n",
      "    WINDOW_KEEPRATIO = 0\n",
      "    WINDOW_NORMAL = 0\n",
      "    WINDOW_OPENGL = 4096\n",
      "    WND_PROP_ASPECT_RATIO = 2\n",
      "    WND_PROP_AUTOSIZE = 1\n",
      "    WND_PROP_FULLSCREEN = 0\n",
      "    WND_PROP_OPENGL = 3\n",
      "    WND_PROP_VISIBLE = 4\n",
      "    haarcascades = '/home/ubuntu/anaconda3/lib/python3.7/site-packages/cv2...\n",
      "\n",
      "VERSION\n",
      "    4.1.1\n",
      "\n",
      "FILE\n",
      "    /home/ubuntu/anaconda3/lib/python3.7/site-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# face detection with mtcnn on a photograph\n",
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    " \n",
    "# draw an image with detected objects\n",
    "def draw_image_with_boxes(filename, result_list):\n",
    "    # load the image\n",
    "#     data = pyplot.imread(filename)\n",
    "    # plot the image\n",
    "#     pyplot.imshow(data)\n",
    "\n",
    "    image = Image.open(filename)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # get the context for drawing boxes\n",
    "    face_array = np.asarray(image)\n",
    "    \n",
    "    height, width, depth = face_array.shape\n",
    "\n",
    "    What size does the figure need to be in inches to fit the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "    # Create a figure of the right size with one axes that takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    # plot each box\n",
    "    for result in result_list:\n",
    "        # get coordinates\n",
    "        x, y, width, height = result['box']\n",
    "        # create the shape\n",
    "        rect = Rectangle((x, y), width, height, fill=False, color='red',edgecolor=\"red\", linewidth=7.5)\n",
    "        # draw the box\n",
    "        ax.add_patch(rect)\n",
    "                # draw the dots on eyes nose ..\n",
    "        for key, value in result['keypoints'].items():\n",
    "            # create and draw dot\n",
    "            dot = Circle(value, radius=2, color='red')\n",
    "            ax.add_patch(dot)\n",
    "            # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display faces on the original image\n",
    "draw_image_with_boxes(filename, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(filename)\n",
    "# # convert to RGB, if needed\n",
    "# image = image.convert('RGB')\n",
    "\n",
    "# # 211 240 715 738\n",
    "# im = Image.fromarray(pixels[211-20:240+20, 715-20:738+20])\n",
    "# im.save('test.png')\n",
    "\n",
    "# # face_array = np.asarray(im)\n",
    "\n",
    "# # plt.imshow(face_array)\n",
    "\n",
    "# im = Image.open(\"test.png\")\n",
    "# im.rotate(45).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing all necessary libraries \n",
    "# import cv2 \n",
    "# import os \n",
    "  \n",
    "# # Read the video from specified path \n",
    "# cam = cv2.VideoCapture(\"aagfhgtpmv.mp4\") \n",
    "  \n",
    "# try: \n",
    "      \n",
    "#     # creating a folder named data \n",
    "#     if not os.path.exists('image-data'): \n",
    "#         os.makedirs('image-data') \n",
    "  \n",
    "# # if not created then raise error \n",
    "# except OSError: \n",
    "#     print ('Error: Creating directory of data') \n",
    "  \n",
    "# # frame \n",
    "# currentframe = 0\n",
    "  \n",
    "# while(True): \n",
    "      \n",
    "#     # reading from frame \n",
    "#     ret,frame = cam.read() \n",
    "  \n",
    "#     if ret: \n",
    "#         # if video is still left continue creating images \n",
    "#         name = './data/frame' + str(currentframe) + '.jpg'\n",
    "#         print ('Creating...' + name) \n",
    "  \n",
    "#         # writing the extracted images \n",
    "#         cv2.imwrite(name, frame) \n",
    "  \n",
    "#         # increasing counter so that it will \n",
    "#         # show how many frames are created \n",
    "#         currentframe += 1\n",
    "#     else: \n",
    "#         break\n",
    "  \n",
    "# # Release all space and windows once done \n",
    "# cam.release() \n",
    "# cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path ='./image-data'\n",
    "# files = np.array(os.listdir(path))\n",
    "# index = np.random.randint(0, len(files), size=1)\n",
    "# print(files[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_faces(filename):\n",
    "    \n",
    "#     # load image from file\n",
    "#     image = Image.open(filename)\n",
    "#     # convert to RGB, if needed\n",
    "#     image = image.convert('RGB')\n",
    "#     # convert to array\n",
    "#     pixels = np.asarray(image)\n",
    "\n",
    "#     # create the detector, using default weights\n",
    "#     detector = mtcnn.MTCNN()\n",
    "#     # detect faces in the image\n",
    "#     results = detector.detect_faces(pixels)\n",
    "#     draw_image_with_boxes(filename, results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,f in enumerate(files[index]):\n",
    "#     imagepath = \"./image-data/\"+f\n",
    "#     print(imagepath)\n",
    "#     plt.subplot(len(files), 1, i+1)\n",
    "#     plt.axis('off')\n",
    "#     get_faces(imagepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_faces(imagepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dlib\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnnFaceDetector = dlib.cnn_face_detection_model_v1(\"./models/mmod_human_face_detector.dat\")\n",
    "# face_detect = dlib.get_frontal_face_detector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(imagepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = dlib.load_rgb_image(\"4people.png\")\n",
    "\n",
    "# rects = dnnFaceDetector(img, 3)\n",
    "# \n",
    "# rects = face_detect(img,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i, rect) in enumerate(rects):\n",
    "#     x1 = rect.rect.left()\n",
    "#     y1 = rect.rect.top()\n",
    "#     x2 = rect.rect.right()\n",
    "#     y2 = rect.rect.bottom()\n",
    "#     # Rectangle around the face\n",
    "#     cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 255), 3)\n",
    "# plt.figure(figsize=(12,8))\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_detect = dlib.get_frontal_face_detector()\n",
    "# rects = face_detect(img, 3)\n",
    "# for (i, rect) in enumerate(rects):\n",
    "# (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "#     cv2.rectangle(gray, (x, y), (x + w, y + h), (255, 255, 255), 3)\n",
    "    \n",
    "# plt.figure(figsize=(12,8))\n",
    "# plt.imshow(gray, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
